---
layout: single
title:  'LSTMCell'
toc: true
categories: [TensorFlow]
tags: [tensorflow, timeseries]

---

ë³¸ ê²Œì‹œë¬¼ì€ tensorflowì˜ [í¬ìŠ¤íŠ¸](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell)ë¥¼ ë³´ê³  ì •ë¦¬í•˜ëŠ” ê¸€ì´ë‹¤.
{: .notice}

```python
tf.keras.layers.LSTMCell(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
```

### ğŸ“Œ LSTM vs LSTMCell

- LSTM: recurrent layerë¡œ ì „ì²´ sequenceë¥¼ ì²˜ë¦¬ í•  ìˆ˜ ìˆìŒ.
- LSTMCell: ì „ì²´ sequence ì¤‘ í•œ ë‹¨ê³„ë§Œì„ ì²˜ë¦¬í•˜ëŠ” object

recurrent layerì—ëŠ” cell ê°ì²´ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, cellì—ëŠ” ê° ë‹¨ê³„ì˜ ê³„ì‚°ì„ ìœ„í•œ ì½”ë“œê°€ í¬í•¨ë˜ì–´ ìˆë‹¤. ì¦‰, recurrent layerëŠ” ê° cellì—ê²Œ ê³„ì‚°ì„ ìœ„í•œ ëª…ë ¹ì„ ë‚´ë¦¬ê³  cellì€ ì´ë¥¼ ìˆ˜í–‰í•œë‹¤. LSTMCellì€ RNN layerì˜ cell ë¶€ë¶„ìœ¼ë¡œ ì…ë ¥ë˜ì–´ LSTMê³¼ ë™ì¼í•œ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. LSTM ë˜í•œ ë‚´ë¶€ ì½”ë“œëŠ” RNNì— LSTMCellì„ í¬í•¨í•œ ê²ƒì´ë‹¤.

### ğŸ“Œ Args

- units: outputì˜ demension
- activation: cell state ì—°ì‚°ì‹œ ì‚¬ìš©ë˜ëŠ” activation functionìœ¼ë¡œ defaultëŠ” tanhì´ë‹¤. Noneì¼ ê²½ìš° activationì´ ì ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤.
- recurrent_activation: input/ forget/ output gate ì—°ì‚°ì‹œ ì‚¬ìš©ë˜ëŠ” activation functionìœ¼ë¡œ defaultëŠ” sigmoidì´ë‹¤. Noneì¼ ê²½ìš° activationì´ ì ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤.
- use_bias: layerê°€ biasë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ì— ëŒ€í•œ ì—¬ë¶€ë¡œ defaultëŠ” Trueì´ë‹¤.
- kernel_initializer: inputì˜ ì„ í˜•ë³€í™˜ì— ì‚¬ìš©ë˜ëŠ” kernel weight matrixì˜ initializerì´ë‹¤. (inputì´ cell ì—°ì‚°ë˜ê¸° ì „ kernel weightì™€ ì—°ì‚°ë¨)
- recurrent_initializer: hidden stateì˜ ì„ í˜•ë³€í™˜ì— ì‚¬ìš©ë˜ëŠ” recurrent kernel weight matrixì˜ initializerì´ë‹¤. (hidden stateë“¤ì´ ì—°ì‚°ë˜ê¸° ì „ recurrent kernel weightì™€ ì—°ì‚°ë¨)
- bias_initializer: biasì˜ initializerì´ë‹¤.
- unit_forget_bias: forget gateì˜ biasì— 1ì„ ë”í•˜ë©°, default ê°’ì€ Trueì´ë‹¤. Trueì‹œ bias_initializer ë˜í•œ Trueë¡œ ì„¸íŒ…ëœë‹¤. (ì¶”ì²œ)
- kernel_regularizer: kernel weight matrixì— ì •ê·œí™”ë¥¼ ì ìš©í•˜ë©°, defaultëŠ” Noneì´ë‹¤.
- recurrent_regularizer: recurrent kernel weight matrixì— ì •ê·œí™”ë¥¼ ì ìš©í•˜ë©°, defaultëŠ” Noneì´ë‹¤.
- bias_regularizer: biasì— ì •ê·œí™”ë¥¼ ì ìš©í•˜ë©°, defaultëŠ” Noneì´ë‹¤.
- kernel_constraint: kernel weight matrixì— ì ìš©í•˜ëŠ” ì œì•½ì‹ì´ë©°, defaultëŠ” Noneì´ë‹¤.
- recurrent_constraint: recurrent kernel weight matrixì— ì ìš©í•˜ëŠ” ì œì•½ì‹ì´ë©°, defaultëŠ” Noneì´ë‹¤.
- bias_constraint: biasì— ì ìš©í•˜ëŠ” ì œì•½ì‹ì´ë©°, defaultëŠ” Noneì´ë‹¤.
- dropout: inputì— ì ìš©í•˜ëŠ” dropoutì´ë©°, defaultëŠ” 0ì´ë‹¤.
- recurrent_dropout: hidden stateì— ì ìš©í•˜ëŠ” dropoutì´ë©°, defaultëŠ” 0ì´ë‹¤.

### ğŸ“Œ Example

````python
inputs = tf.random.normal([32, 10, 8])
rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))
output = rnn(inputs)
print(output.shape)
````

````
(32, 4)
````

````python
rnn = tf.keras.layers.RNN(
   tf.keras.layers.LSTMCell(4),
   return_sequences=True,
   return_state=True)
whole_seq_output, final_memory_state, final_carry_state = rnn(inputs)
print(whole_seq_output.shape)
print(final_memory_state.shape)
print(final_carry_state.shape)
````

````
(32, 10, 4)
(32, 4)
(32, 4)
````
