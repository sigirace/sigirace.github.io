---
layout: single
title:  'Sampling (4) Markov Chanin Monte Carloê°€ ë­ì•¼?'
toc: true
categories: [Sampling]
tags: [MCMC, Markov Chain, Monte Carlo]

---

ë³¸ ê²Œì‹œë¬¼ì€ ê³µëŒì´ì˜ ìˆ˜í•™ì •ë¦¬ë…¸íŠ¸ì˜ [Markov Chain Monte Carlo](https://angeloyeo.github.io/2020/09/17/MCMC.html)ì„ ì°¸ê³ í•˜ì—¬ ì‘ì„±í•˜ì˜€ë‹¤.
{: .notice}

## 1. Introduce

### 1.1 Markov Chain

ğŸ‘€ **Definition**

> ê³¼ê±°ì™€ í˜„ì¬ ìƒíƒœê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë¯¸ë˜ ìƒíƒœì˜ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ê°€ ê³¼ê±° ìƒíƒœì™€ëŠ” ë…ë¦½ì ìœ¼ë¡œ í˜„ì¬ ìƒíƒœì— ì˜í•´ì„œë§Œ ê²°ì •ëœë‹¤.

- Markov Chainì€ ì´ì‚° ì‹œê°„ í™•ë¥  ê³¼ì •

- ìƒíƒœì˜ ë³€í™”ë¥¼ ì „ì´ë¼ê³  í•¨

ğŸ“ **ìˆ˜í•™ì  í‘œí˜„**

$$
Pr(X_n = x_n | X_{n-1} = x_{n-1}, ...,X_1=x_1) \\
= Pr(X_n=x_n | X_{n-1}=x_{n-1})
$$

- ì¡°ê±´ë¶€ í™•ë¥  í™•ì¸

- ë©”ëª¨ë¦¬ê°€ 1ì¸ Markov chain í‘œí˜„

- ë©”ëª¨ë¦¬ê°€ 1ì´ë©´ ìƒíƒœëŠ” ì´ì „ì˜ ìƒíƒœì—ë§Œ ì˜ì¡´í•¨

ğŸ“ **Example**

```python
import numpy as np

# ìƒíƒœ ì •ì˜
states = ['Sunny', 'Rainy']

# ì „ì´ í™•ë¥  í–‰ë ¬: P[i, j]ëŠ” ìƒíƒœ iì—ì„œ jë¡œ ì „ì´í•  í™•ë¥ 
# P = [
#  [Sunny -> Sunny, Sunny -> Rainy],
#  [Rainy -> Sunny, Rainy -> Rainy]
# ]
P = np.array([
    [0.9, 0.1],  # Sunnyì—ì„œ Sunnyë¡œ, Sunnyì—ì„œ Rainyë¡œ
    [0.5, 0.5]   # Rainyì—ì„œ Sunnyë¡œ, Rainyì—ì„œ Rainyë¡œ
])

# ì´ˆê¸° ìƒíƒœ (Sunnyë¡œ ì‹œì‘)
current_state = 0  # 'Sunny' ìƒíƒœì˜ ì¸ë±ìŠ¤

# ë§ˆë¥´ì½”í”„ ì²´ì¸ ì‹œë®¬ë ˆì´ì…˜
num_steps = 10  # ì‹œë®¬ë ˆì´ì…˜í•  ë‹¨ê³„ ìˆ˜
for step in range(num_steps):
    print(f"Step {step}: {states[current_state]}")
    current_state = np.random.choice([0, 1], p=P[current_state])

print(f"Final State: {states[current_state]}")
```

```
Step 0: Sunny
Step 1: Rainy
Step 2: Rainy
Step 3: Sunny
Step 4: Sunny
Step 5: Rainy
Step 6: Sunny
Step 7: Sunny
Step 8: Sunny
Step 9: Rainy
Final State: Rainy
```

### 1.2 Monte Carlo

ğŸ‘€ **Definition**

> Monte Carlo methodëŠ” ë°˜ë³µëœ ë¬´ì‘ìœ„ ì¶”ì¶œì„ ì´ìš©í•˜ì—¬ ê°’ì„ ìˆ˜ë¦¬ì ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ë¶€ë¥´ëŠ” ìš©ì–´ì´ë‹¤.

- ë¬´ìˆ˜í•œ ìˆ˜í–‰ì„ í†µí•´ ìˆ˜ì¹˜ì ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ì–´ë ¤ìš´ ê°’ì„ ê·¼ì‚¬í•˜ë ¤ëŠ” ë°©ë²•ë¡ 

ğŸ“ **ê·¼ì‚¬í™” ì´ìœ **

- ê³„ì‚°í•˜ë ¤ëŠ” ê°’ì´ ë‹«íŒ í˜•ì‹(=closed form)ìœ¼ë¡œ í‘œí˜„ë˜ì§€ ì•Šì„ ê²½ìš°

- ìˆ˜ë¦¬ì ìœ¼ë¡œ ë³µì¡í•  ê²½ìš°

ğŸ“**Note: closed form**

- ì£¼ì–´ì§„ ë¬¸ì œê°€ ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„ í•¨ìˆ˜ë‚˜ ìˆ˜í•™ ì—°ì‚°ìœ¼ë¡œ í•´ë¥¼ êµ¬í•  ìˆ˜ ìˆëŠ” ì‹

- í˜¹ì€ í•´ì„ì´ ê°€ëŠ¥í•œ ì‹ (ex ë¬´í•œê°œì˜ ë”í•˜ê¸°ë¼ë„ $\sum$ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ í•´ì„)

ğŸ“**Example**

ì›ì˜ ë„“ì´ë¥¼ êµ¬í•˜ëŠ” ì˜ˆì‹œë¡œ ëª¬í…Œ ì¹´ë¥¼ë¡œ ê·¼ì‚¬ ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.

```python
import random

def estimate_pi(num_samples):
    inside_circle = 0
    for _ in range(num_samples):
        x, y = random.random(), random.random()  # 0ê³¼ 1 ì‚¬ì´ì˜ ë¬´ì‘ìœ„ ì  ìƒì„±
        distance = x**2 + y**2  # ì›ì ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬ ê³„ì‚°
        if distance <= 1:  # ë‹¨ìœ„ ì› ë‚´ë¶€ì— ìˆëŠ”ì§€ íŒë‹¨
            inside_circle += 1
    pi_estimate = 4 * inside_circle / num_samples  # Ï€ ê°’ ì¶”ì •
    return pi_estimate

# 10000ë²ˆì˜ ì‹œí–‰ìœ¼ë¡œ Ï€ ì¶”ì •
pi_estimate = estimate_pi(10000)
print(f"Estimated Ï€: {pi_estimate}")
```

```
Estimated Ï€: 3.138
```

ğŸ“ **Visualization**

```python
import matplotlib.pyplot as plt
import random

def estimate_pi_and_points(num_samples):
    inside_circle_x = []
    inside_circle_y = []
    outside_circle_x = []
    outside_circle_y = []

    inside_circle = 0
    for _ in range(num_samples):
        x, y = random.random(), random.random()  
        distance = x**2 + y**2  
        if distance <= 1:  
            inside_circle += 1
            inside_circle_x.append(x)
            inside_circle_y.append(y)
        else:
            outside_circle_x.append(x)
            outside_circle_y.append(y)

    pi_estimate = 4 * inside_circle / num_samples  
    return pi_estimate, inside_circle_x, inside_circle_y, outside_circle_x, outside_circle_y

num_samples = 10000
pi_estimate, inside_x, inside_y, outside_x, outside_y = estimate_pi_and_points(num_samples)

# Plotting
plt.figure(figsize=(6, 6))
plt.scatter(inside_x, inside_y, color='blue', s=1, label='Inside circle')
plt.scatter(outside_x, outside_y, color='red', s=1, label='Outside circle')
plt.xlabel('X')
plt.ylabel('Y')
plt.title(f"Monte Carlo Estimation of Ï€ (Estimate: {pi_estimate})")
plt.legend()
plt.axis('equal') 
plt.show()
```

![](https://github.com/sigirace/page-images/blob/main/statistics/sampling/mcmc/mc1.png?raw=true)

### 1.3 Markov Chain Monte Carlo (MCMC)

ğŸ‘€**Definition**

> Markov chainì— ê¸°ë°˜í•˜ì—¬ í™•ë¥  ë¶„í¬ë¡œë¶€í„° ì›í•˜ëŠ” ë¶„í¬ì˜ ì •ì  ë¶„í¬ë¥¼ ê°–ëŠ” í‘œë³¸ì„ ì¶”ì¶œí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜

- MCMCëŠ” í‘œë³¸ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²• â˜ Sampling

- Sampling: í™•ë¥  ë¶„í¬ë¡œë¶€í„° í‘œë³¸ì„ ì¶”ì¶œí•˜ëŠ” ê²ƒ

ğŸ“**Requirements**

- Samplingì„ ìœ„í•œ target ë¶„í¬(=í™•ë¥ ë¶„í¬)ê°€ ìˆì–´ì•¼ í•¨

- Target distirubtionì€ ì•„ë˜ì™€ ê°™ë‹¤ê³  ê°€ì •

$$
f(x) = 0.3 \cdot e^{-0.2 x^2} + 0.7 \cdot e^{-0.2(x - 10)^2}
$$

## 2. Sampling Process

### 2.1 Random Initialization

- Sample spaceì—ì„œ random pointë¥¼ ì„ íƒ

ğŸ“ **Example**

```python
import matplotlib.pyplot as plt
import numpy as np

def target_distribution(x):
    return 0.3 * np.exp(-0.2 * x**2) + 0.7 * np.exp(-0.2 * (x - 10)**2)

x = np.linspace(-10, 20, 1000)
target_pdf = target_distribution(x)
initial_point = 7

plt.figure(figsize=(10, 6))
plt.plot(x, target_pdf, label='Target Distribution $f(x)$', linewidth=2)
plt.scatter(initial_point, 0, color='red', label='Initial Point')
plt.xlabel('x')
plt.ylabel('Density')
plt.legend()
plt.show()
```

![](https://github.com/sigirace/page-images/blob/main/statistics/sampling/mcmc/mc2.png?raw=true)

- Sample space ì¤‘ initial pointë¡œ 7 ì„ íƒ

ğŸ“**Note: Sample space**

- ì‹¤í—˜ ê²°ê³¼ì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  ê²°ê³¼ì˜ ì§‘í•©

- ex) ë™ì „ ë˜ì§€ê¸°ì˜ ìƒ˜í”Œê³µê°„ â˜ {ì•, ë’¤}

- ex) ì£¼ì‚¬ìœ„ ë˜ì§€ê¸°ì˜ ìƒ˜í”Œê³µê°„ â˜ {1, 2, 3, 4, 5, 6}

- ex) ì •ê·œë¶„í¬ì˜ ìƒ˜í”Œê³µê°„ â˜ {$-\infty, \infty$}

### 2.2 Recommend

- Initial pointë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ëŠ” ì œì•ˆë¶„í¬ ìƒì„±
  
  - Metropolisì¼ ê²½ìš° symmetricí•œ í™•ë¥ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ë©° ëŒ€ë¶€ë¶„ ì •ê·œë¶„í¬ ì‚¬ìš©
  
  - ì •ê·œë¶„í¬ì˜ ë„ˆë¹„ëŠ” ì—°êµ¬ìì˜ ì„ íƒì— ë”°ë¼ ì„ì˜ë¡œ ì„¤ì •

- ì œì•ˆë¶„í¬ì—ì„œ ìƒˆë¡œìš´ ìƒ˜í”Œ í¬ì¸íŠ¸ë¥¼ ì¶”ì¶œ

- ì œì•ˆëœ ìƒ˜í”Œ í¬ì¸íŠ¸ì™€ ê·¸ í¬ì¸íŠ¸ì—ì„œì˜ íƒ€ê²Ÿ ë¶„í¬ì˜ ê°’ ë¹„êµ
  
  - ${f(x_1) \over f(x_0)}>1$ ì¼ ê²½ìš° accept

- Acceptì¼ ê²½ìš° ìƒ˜í”Œ í¬ì¸íŠ¸ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê³¼ì • ë°˜ë³µ

- Rejectì¼ ê²½ìš° ë‹¤ìŒ stepìœ¼ë¡œ ì´ë™

ğŸ“**Note: MCMC ë°©ë²•ì— ë”°ë¥¸ ë¶„í¬ì˜ í˜•íƒœ**

- Metropolis: symmetricí•œ í™•ë¥  ë¶„í¬ ì‚¬ìš© â˜ ex) ì •ê·œë¶„í¬

- Hastings: ì¼ë°˜ì ì¸ í™•ë¥  ë¶„í¬

ğŸ“**Note: Hasting ì•Œê³ ë¦¬ì¦˜**

- ì œì•ˆë¶„í¬ë¥¼ g(x)ë¼ê³  í•  ë•Œ ì•„ë˜ì™€ ê°™ì´ ì •ê·œí™”
  
  - $\frac{\frac{f(x_1)}{g(x_1 \mid x_0)}}{\frac{f(x_0)}{g(x_0 \mid x_1)}}$



ğŸ“**Example: ì œì•ˆë¶„í¬ ìƒì„±**

```python
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

def target_distribution(x):
    return 0.3 * np.exp(-0.2 * x**2) + 0.7 * np.exp(-0.2 * (x - 10)**2)

x = np.linspace(-10, 20, 1000)
target_pdf = target_distribution(x)
initial_point = 7

normal_dist = norm.pdf(x, initial_point, 2)

plt.figure(figsize=(10, 6))
plt.plot(x, target_pdf, label='Target Distribution $f(x)$', linewidth=2)
plt.plot(x, normal_dist, label='Normal Distribution $N(initial\_point, 2)$', linewidth=2, linestyle='--')
plt.scatter(initial_point, 0, color='red', label='Initial Point')
plt.axvline(x=initial_point, color='gray', linestyle='--', label='Center of Normal Distribution')
plt.xlabel('x')
plt.ylabel('Density')
plt.legend()
plt.show()
```

![](https://github.com/sigirace/page-images/blob/main/statistics/sampling/mcmc/mc3.png?raw=true)

ğŸ“**Example: ì‹ ê·œ í¬ì¸íŠ¸ ì¶”ì²œ**

```python
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm

def target_distribution(x):
    return 0.3 * np.exp(-0.2 * x**2) + 0.7 * np.exp(-0.2 * (x - 10)**2)

x = np.linspace(-10, 20, 1000)
target_pdf = target_distribution(x)
initial_point = 7

normal_dist = norm.pdf(x, initial_point, 2)

def recommend(x_0, target_distribution):
    x_1 = np.random.normal(x_0, 2)

    f_x_0 = target_distribution(x_0)
    f_x_1 = target_distribution(x_1)

    if f_x_1 / f_x_0 > 1:
        return (x_1, f_x_1), (x_0, f_x_0), True
    else:
        return (x_1, f_x_1), (x_0, f_x_0), False

new_points, initial_points, is_accepted = recommend(initial_point, target_distribution)

plt.figure(figsize=(10, 6))
plt.plot(x, target_pdf, label='Target Distribution $f(x)$', linewidth=2)
plt.plot(x, normal_dist, label='Normal Distribution $N(initial\_point, 2)$', linewidth=2, linestyle='--')
plt.scatter(initial_points[0], 0, color='red')
plt.scatter(initial_points[0], initial_points[1], color='red', label='Initial Point')
plt.scatter(new_points[0], 0, color='green')
plt.scatter(new_points[0], new_points[1], color='green', label='new Point')
plt.xlabel('x')
plt.ylabel('Density')
plt.legend()
plt.show()
```

![](https://github.com/sigirace/page-images/blob/main/statistics/sampling/mcmc/mc4.png?raw=true)

- Acceptì¸ ê²½ìš°

![](https://github.com/sigirace/page-images/blob/main/statistics/sampling/mcmc/mc5.png?raw=true)

- Rejectì¸ ê²½ìš°

### 2.3 Resurrection

- ì¼ëª… **íŒ¨ìë¶€í™œì „**ìœ¼ë¡œ rejectëœ ìƒ˜í”Œë“¤ì„ ë¬´ì¡°ê±´ ì´ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í†µê³„ì ìœ¼ë¡œ ìˆ˜ìš©í•  ìˆ˜ ìˆë„ë¡ í•¨

- ì›ë˜ ìƒ˜í”Œì„ x_1, ì œì•ˆë¶„í¬ë¥¼ í†µí•´ ìƒˆë¡œ ì œì•ˆë°›ì€ ìƒ˜í”Œì„ x_2ë¼ í•  ë•Œ ë‹¤ìŒì„ ë¹„êµ
  
  - ${f(x_1) \over f(x_0)}>u$
  
  - ì´ë•Œ uëŠ” uniform distribution $U_{(0,1)}$ì—ì„œ ì¶”ì¶œí•œ ì„ì˜ì˜ ìƒ˜í”Œ

ğŸ“**Note: Why doing resurrection?**

- ì•ì„œ recommend ê³¼ì •ì—ì„œ accpetë˜ëŠ” ì¡°ê±´ì€ ìƒˆë¡œìš´ ìƒ˜í”Œì´ target distributionì—ì„œ ë” í° ê°’ì„ ê°€ì§€ê³  ìˆì–´ì•¼ í•¨

- ìƒ˜í”Œì€ ê³„ì†í•´ì„œ target distribution ì˜ ë†’ì€ ë¶€ë¶„ìœ¼ë¡œ ìˆ˜ë ´í•¨

- íŒ¨ìë¶€í™œì „ ì—†ì´ëŠ” ë‚®ì€ í™•ë¥ ì„ ê°€ì§„ ìƒ˜í”Œë“¤ì´ ì¶”ì¶œë˜ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ë°œìƒí•¨

## 3. Visualization

```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¬ì„¤ì •
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm
from scipy.integrate import quad


# ëª©í‘œ ë¶„í¬ ì •ì˜
def target_distribution(x):
    return 0.3 * np.exp(-0.2 * x**2) + 0.7 * np.exp(-0.2 * (x - 10)**2)

x_domain = [-7, 17]

# MCMC Metropolis ì•Œê³ ë¦¬ì¦˜ ì •ì˜
def metropolis_algorithm(target_distribution, initial_point, num_samples, step_size=2):
    samples = [initial_point]
    current_point = initial_point

    for i in range(num_samples - 1):
        new_point = np.random.normal(current_point, step_size)
        if x_domain[0] <= new_point <= x_domain[1]:
            f_current = target_distribution(current_point)
            f_new = target_distribution(new_point)
            acceptance_ratio = f_new / f_current

            u = np.random.uniform(0, 1)
            if acceptance_ratio > u:
                samples.append(new_point)
                current_point = new_point
            else:
                samples.append(current_point)
        else:
            samples.append(current_point)

    return samples

# ì‹¤í–‰ ë§¤ê°œë³€ìˆ˜ ì„¤ì •
num_samples = 5000
initial_point = 7

# ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
samples = metropolis_algorithm(target_distribution, initial_point, num_samples)

# -7ë¶€í„° 17ê¹Œì§€ ëª©í‘œ ë¶„í¬ì˜ ì ë¶„ ê³„ì‚°
integral_result, _ = quad(target_distribution, x_domain[0], x_domain[1])

integral_result

# ìƒ˜í”Œ íˆìŠ¤í† ê·¸ë¨ ìƒì„± (ì •ê·œí™” ì—†ì´)
counts, bin_edges = np.histogram(samples, bins=int(30 / 0.5), density=False)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

# ì ë¶„ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ íˆìŠ¤í† ê·¸ë¨ ì •ê·œí™”
total_samples = len(samples)
normalized_counts = (counts / total_samples) * (integral_result / np.diff(bin_edges))

plt.figure(figsize=(10, 6))
plt.plot(x, target_pdf, label='Target Distribution', linewidth=2)
plt.bar(bin_centers, normalized_counts, width=np.diff(bin_edges)[0], alpha=0.5, color='orange', label='MCMC Samples (Normalized by Integral)')
plt.scatter(initial_point, target_distribution(initial_point), color='red', zorder=5, label='Initial Point')
plt.xlabel('x')
plt.ylabel('Density')
plt.legend()
plt.title('MCMC Sampling with Histogram Normalized by Integral Result')
plt.show()
```

![](https://github.com/sigirace/page-images/blob/main/statistics/sampling/mcmc/mc6.png?raw=true)

- MCMCì˜ metropolis ë°©ì‹ì— ë”°ë¥¸ sampling ìˆ˜í–‰ ê²°ê³¼

ğŸƒ**Next Step**

- MCMCëŠ” Sampling ë¿ ì•„ë‹ˆë¼ í™•ë¥ ë¶„í¬ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŒ

- MCMCë¥¼ ì´ìš©í•œ Bayesian estimationì— ëŒ€í•´ ì•Œì•„ë´„
