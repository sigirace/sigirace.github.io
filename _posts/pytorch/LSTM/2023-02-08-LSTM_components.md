---
layout: single
title:  'LSTMì˜ ëª¨ë“ ê²ƒ (2) PyTorch ê³µì‹ ë¬¸ì„œë¡œ ë³´ëŠ” êµ¬ì„±ìš”ì†Œ'
toc: true
categories: [Deep Learning]
tags: [lstm, pytorch]
---

ë³¸ ê²Œì‹œë¬¼ì€ [Pytorch ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)ë¥¼ ì°¸ê³ í•˜ì—¬ LSTMì˜ êµ¬ì„±ìš”ì†Œì— ëŒ€í•´ ì •ë¦¬í•˜ëŠ” ê¸€ì´ë‹¤.
{: .notice}

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm/lstm_c_7.png?raw=true" width="600" height="500"></p>

<center>ì¶œì²˜: https://discuss.pytorch.org/t/rnn-output-vs-hidden-state-dont-match-up-my-misunderstanding/43280</center><br>

## 1. Parameters

```python
LSTM(input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, proj_size)
```

### 1.1 input_size / hidden_size

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm/lstm_c_2.png?raw=true" width="600" height="250"></p>

```
input_size â€“ The number of expected features in the input x
hidden_size â€“ The number of features in the hidden state h
```

- input_sizeëŠ” ì…ë ¥ë˜ëŠ” **ë°ì´í„°ì˜ ì°¨ì›**ì´ë‹¤. ë‹¨ì¼ë¡œ ì…ë ¥ë˜ëŠ” ë°ì´í„°ëŠ” (sequence_length, input_size)ë¡œ êµ¬ì„±ë˜ë©°, sequence_lengthëŠ” ì‹œê³„ì—´ í˜¹ì€ ë¬¸ì¥ì˜ ê¸¸ì´, input_sizeëŠ” ì‹œê³„ì—´ì— í¬í•¨ëœ feature ìˆ˜ í˜¹ì€ ë‹¨ì–´ì˜ ì„ë² ë”©ì˜ ì°¨ì›ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

- hidden_sizeëŠ” **hidden stateì˜ ì°¨ì›**ì´ë‹¤. ì…ë ¥ëœ ë°ì´í„°ê°€ ì—°ì‚°ì„ í†µí•´ ê°€ì§€ê²Œ ë  ì°¨ì›ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

### 1.2 num_layers

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm/lstm_c_3.png?raw=true" width="350" height="350"></p>

```
num_layers â€“ Number of recurrent layers.
E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM,<br>
with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1
```

- num_layersëŠ” **LSTM cellì„ stackí•˜ëŠ” ìˆ˜**ì´ë‹¤. ìœ ì˜í•  ì ì€ ì²«ë²ˆì§¸ layerì—ì„œëŠ” inputìœ¼ë¡œ ì…ë ¥ ë°ì´í„°ê°€ ë“¤ì–´ê°€ë‚˜ ë‘ë²ˆì§¸ layerëŠ” inputìœ¼ë¡œ ì²«ë²ˆì§¸ layerì˜ outputì¸ hidden stateê°€ ë“¤ì–´ê°„ë‹¤ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ ë‘ë²ˆì§¸ lstmì˜ inputê³¼ ì—°ì‚°ë˜ëŠ” weightëŠ” ì°¨ì›ì´ ë°”ë€Œê²Œ ëœë‹¤. (âœ¤ ì°¸ê³ : variablesì˜ weight_ih_l[k], weight_hh_l[k])

### 1.3 bias

- biasëŠ” hidden stateì™€ inputì´ ê° gateì—ì„œ matrixì™€ vectorì˜ ê³± ì—°ì‚°(ì„ í˜•ë³€í™˜)ì‹œ ë”°ë¼ì˜¤ëŠ” ê²ƒìœ¼ë¡œ **hidden sateì˜ ì°¨ì›ê³¼ ë™ì¼**í•˜ë‹¤.

### 1.4 batch_first

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm/lstm_c_5.png?raw=true" width="850" height="500"></p>

- batch_firstëŠ” mini batch í•™ìŠµì‹œ batch sizeê°€ ë°ì´í„° shapeì˜ ê°€ì¥ ì²«ë²ˆì§¸ ìš”ì†Œì¸ì§€ í™•ì¸í•˜ëŠ” ê²ƒì´ë‹¤. ì•„ë˜ëŠ” ë‹¨ë°©í–¥(bidirectional=False)ì¼ë•Œì˜ batch_firstì— ë”°ë¥¸ í˜• ë³€í™” ì˜ˆì‹œì´ë‹¤.

ğŸ“**ì˜ˆì‹œ**

> True â˜ Data : [batch_size, sequence length, input_size] / Output : [batch_size, sequence length, hidden_size]
>
> False â˜ Data : [sequence length, batch_size, input_size] / Output : [sequence length, batch_size, hidden_size]

### 1.5 dropout

- ë§ˆì§€ë§‰ layerë¥¼ ì œì™¸í•œ ê° LSTMì˜ ê° layerì˜ ì¶œë ¥ì— dropout layerë¥¼ ì¶”ê°€í•˜ë©° ì…ë ¥í•œ ì¸ìê°€ **dropout í™•ë¥ **ì´ ëœë‹¤.

### 1.6 bidirectional

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm/lstm_c_6.png?raw=true" width="850" height="500"></p>

- bidirectionalì€ ê° layerë¥¼ ì–‘ë°©í–¥ LSTM cellë¡œ êµ¬ì„±í• ì§€ ê²°ì •í•˜ëŠ” ê²ƒì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì–‘ë°©í–¥ LSTMì„ ì‚¬ìš©í•  ê²½ìš° sequence ë°ì´í„°ì—ì„œ ë§ì€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì´ ì¢‹ê²Œ ë‚˜íƒ€ë‚œë‹¤. ì–´ëŠ í•œ time stepì˜ hidden stateëŠ” ë³´í†µ ìˆœë°©í–¥ê³¼ ì—­ë°©í–¥ì˜ hidden stateë¥¼ concatenationí•˜ì—¬ ë‹¤ìŒ layerë¡œ ì „ë‹¬í•˜ëŠ”ë°, ì´ë•Œ hidden stateëŠ” 2*hidden_sizeì˜ í¬ê¸°ë¥¼ ê°€ì§„ë‹¤.

### 1.7 proj_size

## 2. CLASS

### 2.1 Componenets

- $i_t, f_t, o_t$ : input / forget/ output gateì˜ ì¶œë ¥
- $c_t, g_t$ : cell state/ cell stateë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ì¤‘ê°„ ì—°ì‚°(i_tì™€ element-wise ê³±ì„ ìˆ˜í–‰í•¨)
- $W_i, W_h$ : inputê³¼ hiddenì˜ ì„ í˜•ê²°í•©ì‹œì— ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜
- $b_i, h_i$ : bias

### 2.2 Calculate

ğŸ“ **ì˜ˆì‹œ**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm/lstm_c_4.png?raw=true" width="900" height="500"></p>

> Hidden size: 2, Input size 3 ì¼ë•Œ (ì²«ë²ˆì§¸ layerì˜) ê° cell ë‚´ë¶€ì—ì„œ ì—°ì‚°ë˜ëŠ” ê³¼ì •

### 1.3 Component Shape

- $x_t$ : [input_size, 1]
- $h_t$ : [hidden_size, 1]
- $W_{i}$
  - at first layer : [hidden_size, input_size]  
  - otherwise : [hidden_size, num_directions * hidden_size] 
  - proj_size > 0 : [hidden_size, proj_size] not at first layer
- $W_{h}$
  - at first layer : [hidden_size, hidden_size]
  - proj_size > 0 : [hidden_size, proj_size]
- $b_i$ : [hidden_size]
- $b_h$ : [hidden_size]

## 3. Inputs

## 4. Outputs

## 5. Variables



