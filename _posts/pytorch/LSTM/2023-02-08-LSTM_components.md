---
layout: single
title:  'LSTMì˜ ëª¨ë“ ê²ƒ (2) PyTorch ê³µì‹ ë¬¸ì„œë¡œ ë³´ëŠ” êµ¬ì„±ìš”ì†Œ'
toc: true
categories: [Deep Learning]
tags: [lstm, pytorch]
---

ë³¸ ê²Œì‹œë¬¼ì€ [Pytorch ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)ë¥¼ ì°¸ê³ í•˜ì—¬ LSTMì˜ êµ¬ì„±ìš”ì†Œì— ëŒ€í•´ ì •ë¦¬í•˜ëŠ” ê¸€ì´ë‹¤.
{: .notice}

<div class="notice">
<li><a href="https://sigirace.github.io/deep%20learning/LSTM_method///">LSTMì˜ ëª¨ë“ ê²ƒ (1) LSTM ë° ë‚´ë¶€ Gateì— ëŒ€í•œ ì´í•´</a></li>
</div>


## 1. Parameters

```python
LSTM(input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, proj_size)
```

### 1.1 input_size / hidden_size

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm_c/lstm_c_2.png?raw=true" width="800" height="300"></p>

```
input_size â€“ The number of expected features in the input x
hidden_size â€“ The number of features in the hidden state h
```

- <mark style='background-color: #f6f8fa'>&nbsp;input_sizeÂ </mark>ëŠ” ì…ë ¥ë˜ëŠ” **ë°ì´í„°ì˜ ì°¨ì›**ì´ë‹¤. ë‹¨ì¼ë¡œ ì…ë ¥ë˜ëŠ” ë°ì´í„°ëŠ” (sequence_length, input_size)ë¡œ êµ¬ì„±ë˜ë©°, sequence_lengthëŠ” ì‹œê³„ì—´ í˜¹ì€ ë¬¸ì¥ì˜ ê¸¸ì´, input_sizeëŠ” ì‹œê³„ì—´ì— í¬í•¨ëœ feature ìˆ˜ í˜¹ì€ ë‹¨ì–´ì˜ ì„ë² ë”©ì˜ ì°¨ì›ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

- <mark style='background-color: #f6f8fa'>&nbsp;hidden_sizeÂ </mark>ëŠ” **hidden stateì˜ ì°¨ì›**ì´ë‹¤. ì…ë ¥ëœ ë°ì´í„°ê°€ ì—°ì‚°ì„ í†µí•´ ê°€ì§€ê²Œ ë  ì°¨ì›ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

### 1.2 num_layers

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm_c/lstm_c_3.png?raw=true" width="350" height="350"></p>

```
num_layers â€“ Number of recurrent layers.
E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM,
with the second LSTM taking in outputs of the first LSTM and computing the final results.
[Default: 1]
```

- <mark style='background-color: #f6f8fa'>&nbsp;num_layersÂ </mark>ëŠ” **LSTM cellì„ stackí•˜ëŠ” ìˆ˜**ì´ë‹¤. ìœ ì˜í•  ì ì€ ì²«ë²ˆì§¸ layerì—ì„œëŠ” inputìœ¼ë¡œ ì…ë ¥ ë°ì´í„°ê°€ ë“¤ì–´ê°€ë‚˜ ë‘ë²ˆì§¸ layerëŠ” inputìœ¼ë¡œ ì²«ë²ˆì§¸ layerì˜ outputì¸ hidden stateê°€ ë“¤ì–´ê°„ë‹¤ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ ë‘ë²ˆì§¸ lstmì˜ inputê³¼ ì—°ì‚°ë˜ëŠ” weightëŠ” ì°¨ì›ì´ ë°”ë€Œê²Œ ëœë‹¤. (âœ¤ ì°¸ê³ : variablesì˜ weight_ih_l[k], weight_hh_l[k])

### 1.3 bias

```
bias â€“ If False, then the layer does not use bias weights b_ih and b_hh. 
[Default: True]
```

- <mark style='background-color: #f6f8fa'>&nbsp;biasÂ </mark>ëŠ” hidden stateì™€ inputì´ ê° gateì—ì„œ matrixì™€ vectorì˜ ê³± ì—°ì‚°(ì„ í˜•ë³€í™˜)ì‹œ ë”°ë¼ì˜¤ëŠ” ê²ƒìœ¼ë¡œ **hidden sateì˜ ì°¨ì›ê³¼ ë™ì¼**í•˜ë‹¤.

### 1.4 batch_first

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm_c/lstm_c_5.png?raw=true" width="850" height="500"></p>

```
batch_first â€“ If True, then the input and output tensors are provided as (batch, seq, feature) 
instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. 
See the Inputs/Outputs sections below for details. 
[Default: False]
```

- <mark style='background-color: #f6f8fa'>&nbsp;batch_firstÂ </mark>ëŠ” mini batch í•™ìŠµì‹œ batch sizeê°€ ë°ì´í„° shapeì˜ ê°€ì¥ ì²«ë²ˆì§¸ ìš”ì†Œì¸ì§€ í™•ì¸í•˜ëŠ” ê²ƒì´ë‹¤. ì•„ë˜ëŠ” ë‹¨ë°©í–¥(bidirectional=False)ì¼ë•Œì˜ batch_firstì— ë”°ë¥¸ data setê³¼ outputì˜ í˜• ë³€í™” ì˜ˆì‹œì´ë‹¤.

ğŸ“**ì˜ˆì‹œ**

> True â˜ [batch_size, sequence length, input_size] / [batch_size, sequence length, hidden_size]
>
> False â˜ [sequence length, batch_size, input_size] /  [sequence length, batch_size, hidden_size]

### 1.5 dropout

```
dropout â€“ If non-zero, introduces a Dropout layer on the outputs of each LSTM layer 
except the last layer, with dropout probability equal to dropout.
[Default: 0]
```

- <mark style='background-color: #f6f8fa'>&nbsp;dropoutÂ </mark>ì€ 0ì´ ì•„ë‹ì‹œ ë§ˆì§€ë§‰ layerë¥¼ ì œì™¸í•œ ê° LSTMì˜ ê° layerì˜ ì¶œë ¥ì— ì…ë ¥í•œ ì¸ìì˜ í™•ë¥ ì„ ê°€ì§„ dropout layerë¥¼ ì¶”ê°€ í•œë‹¤.

### 1.6 bidirectional

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm_c/lstm_c_6.png?raw=true" width="850" height="500"></p>

```
bidirectional â€“ If True, becomes a bidirectional LSTM. 
[Default: False]
```

- <mark style='background-color: #f6f8fa'>&nbsp;bidirectionalÂ </mark>ì€ ê° layerë¥¼ ì–‘ë°©í–¥ LSTM cellë¡œ êµ¬ì„±í• ì§€ ê²°ì •í•˜ëŠ” ê²ƒì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì–‘ë°©í–¥ LSTMì„ ì‚¬ìš©í•  ê²½ìš° sequence ë°ì´í„°ì—ì„œ ë§ì€ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì´ ì¢‹ê²Œ ë‚˜íƒ€ë‚œë‹¤. ì–´ëŠ í•œ time stepì˜ hidden stateëŠ” ë³´í†µ ìˆœë°©í–¥ê³¼ ì—­ë°©í–¥ì˜ hidden stateë¥¼ concatenationí•˜ì—¬ ë‹¤ìŒ layerë¡œ ì „ë‹¬í•˜ëŠ”ë°, ì´ë•Œ hidden stateëŠ” 2*hidden_sizeì˜ í¬ê¸°ë¥¼ ê°€ì§„ë‹¤.

### 1.7 proj_size

- <mark style='background-color: #f6f8fa'>&nbsp;proj_sizeÂ </mark>ëŠ” LSTM cellì˜ output ì¤‘ í•˜ë‚˜ì¸ $h_t$ì„ learnable projection matrixë¥¼ í†µí•´ ì„ í˜•ë³€í™˜ì‹œí‚¤ëŠ” ì¸ìì´ë‹¤. (ê°œì¸ì ìœ¼ë¡œ ì™œ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ì§€ ì•„ì§ ì˜ë¬¸ì´ê¸° ë•Œë¬¸ì— ê³µì‹ë¬¸ì„œì—ì„œ ì²¨ë¶€í•œ ë…¼ë¬¸ì„ ë³´ê³  ì¶”í›„ ì—…ë°ì´íŠ¸ í•˜ê¸°ë¡œ í•œë‹¤.)

## 2. Inputs

```
input, (h_0, c_0)
```

### 2.1 Shape

- input : $(N, L, H_{in})$ when batch_first=True
- $h_0$ : $(D*layers, N, H_{out})$
- $c_0$ : $(D*layers, N, H_{cell})$

### 2.2 Components

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm_c/lstm_c_7.png?raw=true" width="450" height="200"></p>

## 3. Outputs

```
output, (h_n, c_n)
```

### 3.1 Shape

- output : $(N, L, D*H_{out})$ when batch_first=True
- $h_n$ : $(D*layers, N, H_{out})$
- $c_n$ : $(D*layers, N, H_{cell})$

## 4. Class

### 4.1 Componenets

- $i_t, f_t, o_t$ : input / forget/ output gateì˜ ì¶œë ¥
- $c_t, g_t$ : cell state/ cell stateë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ì¤‘ê°„ ì—°ì‚°(i_tì™€ element-wise ê³±ì„ ìˆ˜í–‰í•¨)
- $W_i, W_h$ : inputê³¼ hiddenì˜ ì„ í˜•ê²°í•©ì‹œì— ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜
- $b_i, h_i$ : bias

### 4.2 Calculate

ğŸ“ **ì˜ˆì‹œ**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm_c/lstm_c_4.png?raw=true" width="900" height="500"></p>

> Hidden size: 2, Input size 3 ì¼ë•Œ (ì²«ë²ˆì§¸ layerì˜) ê° cell ë‚´ë¶€ì—ì„œ ì—°ì‚°ë˜ëŠ” ê³¼ì •

### 4.3 Component Shape at Calculate

- $x_t$ : [batch_size, input_size]
- $h_t$ : [batch_size, hidden_size]
- $W_{i}$
  - at first layer : [hidden_size, input_size] â˜ transpose at calculate
  - otherwise : [hidden_size, num_directions * hidden_size] 
  - proj_size > 0 : [hidden_size, proj_size] not at first layer
- $W_{h}$
  - [hidden_size, hidden_size]
  - proj_size > 0 : [hidden_size, proj_size]
- $b_i$ : [hidden_size]
- $b_h$ : [hidden_size]

## 5. Variables

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/pytorch/lstm_c/lstm_c_8.png?raw=true" width="650" height="550"></p>

