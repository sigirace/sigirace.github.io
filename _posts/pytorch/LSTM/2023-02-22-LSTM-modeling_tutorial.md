---
layout: single
title:  'LSTMì˜ ëª¨ë“ ê²ƒ (5) Timeseries forecasting - Modeling'
toc: true
categories: [Deep Learning]
tags: [timeseries, lstm]
---

ë³¸ ê²Œì‹œë¬¼ì€ Tensorflowì˜ LSTMì„ ì‚¬ìš©í•œ [ì‹œê³„ì—´ ì˜ˆì¸¡ ì˜ˆì œ](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb#scrollTo=6GmSTHXw6lI1) ë‚´ìš© ëª¨ë¸ë§ì„ ì ìš©í•˜ëŠ” ë¶€ë¶„ì„ ì •ë¦¬í•˜ëŠ” ê¸€ì´ë‹¤.
{: .notice}

<div class="notice">
<li><a href="https://sigirace.github.io/deep%20learning/LSTM_method///">LSTMì˜ ëª¨ë“ ê²ƒ (1) LSTM ë° ë‚´ë¶€ Gateì— ëŒ€í•œ ì´í•´</a></li>
<li><a href="https://sigirace.github.io/deep%20learning/LSTM_components//">LSTMì˜ ëª¨ë“ ê²ƒ (2) PyTorch ê³µì‹ ë¬¸ì„œë¡œ ë³´ëŠ” êµ¬ì„±ìš”ì†Œ</a></li>
<li><a href="https://sigirace.github.io/deep%20learning/LSTM-time-tutorial//">LSTMì˜ ëª¨ë“ ê²ƒ (4) Timeseries forecasting - Make Dataset</a></li>  
</div>
## 1. Import dataset

ì´ì „ í¬ìŠ¤íŠ¸ì—ì„œ ë°ì´í„°ì— ëŒ€í•´ ì „ì²˜ë¦¬ì™€ ë”ë¶ˆì–´ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì—ˆë‹¤. ì´ë¥¼ í•˜ë‚˜ì˜ py íŒŒì¼ë¡œ ìƒì„±í•˜ì—¬ ì—…ë¡œë“œí•˜ì˜€ìœ¼ë‹ˆ ë§Œì•½ ìƒˆë¡­ê²Œ íŠœí† ë¦¬ì–¼ì„ ì§„í–‰í•˜ê¸¸ ì›í•˜ë©´ [ì—¬ê¸°](https://github.com/sigirace/sigirace.github.io/blob/master/_posts/pytorch/LSTM/dataset.py)ì—ì„œ ë‹¤ìš´ë°›ì•„ ì„í¬íŠ¸ ì‹œí‚¨ë‹¤.

```python
import dataset as ds
```

## 2. Single step model

ê°€ì¥ ê°„ë‹¨í•œ ëª¨ë¸ì€ í˜„ì¬ ì¡°ê±´ë§Œì„ ì‚¬ìš©í•˜ì—¬ 1 time step(ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” 1ì‹œê°„) í›„ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ 1 time step í›„ì˜ T (degC)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤.

ğŸ¤ª[image11]

ë¨¼ì € ìœ„ì™€ ê°™ì€ single time stepì˜ (input, label) ìŒì„ ìƒì„±í•˜ë„ë¡ ê°ì²´ë¥¼ êµ¬ì„±í•œë‹¤.

```python
target = 'T (degC)'
single_step_window = ds.WindowGenerator(
    input_width=1, label_width=1, shift=1, batch_size = 32, target=target,
    label_columns=[target])
single_step_window
```

```
Total window size: 2
Input indices: [0]
Label indices: [1]
Label column name(s): ['T (degC)']
```

ìƒì„±í•œ ê°ì²´ëŠ” train, validation, test ë°ì´í„° ì„¸íŠ¸ë¡œë¶€í„° tf.data.Datasetsë¥¼ ìƒì„±í•˜ë¯€ë¡œ ì‰½ê²Œ ë°°ì¹˜ë¥¼ ì–»ì„ ìˆ˜ ìˆìœ¼ë©° ë°˜ë³µ ë˜í•œ ê°€ëŠ¥í•˜ë‹¤. ë°ì´í„° ì…‹ì˜ ì˜ˆì‹œëŠ” (batch size, time step, features)ì˜ í˜•íƒœë¥¼ ë„ê³  ìˆë‹¤.

```python
for example_inputs, example_labels in single_step_window.train.take(1):
  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')
  print(f'Labels shape (batch, time, features): {example_labels.shape}')
```

````
Inputs shape (batch, time, features): (32, 1, 19)
Labels shape (batch, time, features): (32, 1, 1)
````

### 2.1 Linear model

í•˜ë‚˜ì˜ Linear layerë¥¼ í†µí•´ ê°€ì¥ ê°„ë‹¨í•œ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤. ì´ë•Œ unitì˜ ìˆ˜ëŠ” ì˜ˆì¸¡í•  time stepì´ë‹¤. í˜„ì¬ëŠ” single stepì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•˜ê³  ìˆê¸° ë•Œë¬¸ì— 1ë¡œ ì„¤ì •í•œë‹¤.

```python
linear = tf.keras.Sequential([
    tf.keras.layers.Dense(units=1)
])

# ìƒ˜í”Œ ë°ì´í„° ì˜ˆì‹œ
print('Input shape:', single_step_window.example[0].shape)
print('Output shape:', linear(single_step_window.example[0]).shape)
```

í›ˆë ¨ì€ ëª¨ë¸ ë° step sizeë§Œ ë°”ê¾¸ì–´ ì—¬ëŸ¬ ì¡°ê±´ìœ¼ë¡œ ì§„í–‰í•  ê²ƒì´ê¸° ë•Œë¬¸ì— í›ˆë ¨ ì ˆì°¨ë¥¼ í•˜ë‚˜ì˜ í•¨ìˆ˜ íŒ¨í‚¤ì§€ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.

````python
MAX_EPOCHS = 20
val_performance = {}
performance = {}

def compile_and_fit(model, window, patience=2):
  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                    patience=patience,
                                                    mode='min')

  model.compile(loss=tf.keras.losses.MeanSquaredError(),
                optimizer=tf.keras.optimizers.Adam(),
                metrics=[tf.keras.metrics.MeanAbsoluteError()])

  history = model.fit(window.train, epochs=MAX_EPOCHS,
                      validation_data=window.val,
                      callbacks=[early_stopping])
  return history
````

êµ¬ì¶•í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ì„ ì§„í–‰í•œë‹¤.

```python
history = compile_and_fit(linear, single_step_window)

val_performance['Linear'] = linear.evaluate(single_step_window.val)
performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)
```

```
Epoch 1/20
1534/1534 [==============================] - 8s 5ms/step - loss: 0.0096 - mean_absolute_error: 0.0721 - val_loss: 82249.0547 - val_mean_absolute_error: 286.7191
Epoch 2/20
1534/1534 [==============================] - 6s 4ms/step - loss: 0.0093 - mean_absolute_error: 0.0707 - val_loss: 85541.4922 - val_mean_absolute_error: 292.3961
Epoch 3/20
1534/1534 [==============================] - 7s 4ms/step - loss: 0.0092 - mean_absolute_error: 0.0702 - val_loss: 88454.2031 - val_mean_absolute_error: 297.3310
439/439 [==============================] - 2s 4ms/step - loss: 88454.1797 - mean_absolute_error: 297.3310
```

ê²°ê³¼ê°€ ì© ì¢‹ì§€ ëª»í•˜ë‚˜ ì˜ˆì¸¡í•œ ë‚´ìš©ì„ ìƒ˜í”Œì„ í™•ì¸í•´ë³¸ë‹¤.

```python
single_step_window.plot(linear)
```

ğŸ¤ª[image12]

Input ë°ì´í„° í¬ì¸íŠ¸ì™€ output ë°ì´í„° í¬ì¸íŠ¸ ê·¸ë¦¬ê³  ì˜ˆì¸¡í•œ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. í•™ìŠµ ê²°ê³¼ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì¢‹ì§€ ëª»í•œ ëª¨ìŠµì´ë‹¤.

ì´ë²ˆì—” step sizeë¥¼ ì¡°ì •í•˜ì—¬ 24ì‹œê°„ì˜ Input ë°ì´í„°ê°€ ë“¤ì–´ê°”ì„ ë•Œ 24ì‹œê°„ í›„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°ì´í„° ì…‹ì„ êµ¬ì„± ë° linear modelì— ëŒ€í•œ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤.

```python
wide_window = WindowGenerator(
    input_width=24, label_width=24, shift=1, batch_size = 32, target=target,
    label_columns=['T (degC)'])

wide_window
```

```
Total window size: 25
Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]
Label indices: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
Label column name(s): ['T (degC)']
```



