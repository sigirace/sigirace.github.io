---
layout: single
title:  'LSTMì˜ ëª¨ë“ ê²ƒ (4) Timeseries forecasting - Single step model'
toc: true
categories: [Time Series]
tags: [LSTM]
---

ë³¸ ê²Œì‹œë¬¼ì€ Tensorflowì˜ LSTMì„ ì‚¬ìš©í•œ [ì‹œê³„ì—´ ì˜ˆì¸¡ ì˜ˆì œ](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb#scrollTo=6GmSTHXw6lI1) ë‚´ìš© ì¤‘ single step modelingì„ ì ìš©í•˜ëŠ” ë¶€ë¶„ì„ ì •ë¦¬í•˜ëŠ” ê¸€ì´ë‹¤.
{: .notice}

<div class="notice">
<li><a href="https://sigirace.github.io/deep%20learning/LSTM_method///">LSTMì˜ ëª¨ë“ ê²ƒ (1) LSTM ë° ë‚´ë¶€ Gateì— ëŒ€í•œ ì´í•´</a></li>
<li><a href="https://sigirace.github.io/deep%20learning/LSTM_components//">LSTMì˜ ëª¨ë“ ê²ƒ (2) PyTorch ê³µì‹ ë¬¸ì„œë¡œ ë³´ëŠ” êµ¬ì„±ìš”ì†Œ</a></li>
<li><a href="https://sigirace.github.io/deep%20learning/LSTM-time-tutorial//">LSTMì˜ ëª¨ë“ ê²ƒ (3) Timeseries forecasting - Make Dataset</a></li>  
</div>

## 1. Import dataset

ì´ì „ í¬ìŠ¤íŠ¸ì—ì„œ ë°ì´í„°ì— ëŒ€í•´ ì „ì²˜ë¦¬ì™€ ë”ë¶ˆì–´ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì—ˆë‹¤. ì´ë¥¼ í•˜ë‚˜ì˜ py íŒŒì¼ë¡œ ìƒì„±í•˜ì—¬ ì—…ë¡œë“œí•˜ì˜€ìœ¼ë‹ˆ ë§Œì•½ ìƒˆë¡­ê²Œ íŠœí† ë¦¬ì–¼ì„ ì§„í–‰í•˜ê¸¸ ì›í•˜ë©´ [ì—¬ê¸°](https://github.com/sigirace/sigirace.github.io/blob/master/_posts/pytorch/LSTM/dataset.py)ì—ì„œ ë‹¤ìš´ë°›ì•„ ì„í¬íŠ¸ ì‹œí‚¨ë‹¤.

```python
import dataset as ds
import tensorflow as tf
```

## 2. Single step model

ê°€ì¥ ê°„ë‹¨í•œ ëª¨ë¸ì€ í˜„ì¬ ì¡°ê±´ë§Œì„ ì‚¬ìš©í•˜ì—¬ 1 time step(ë³¸ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” 1ì‹œê°„) í›„ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ 1 time step í›„ì˜ T (degC)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤.

ğŸ¤ª[image11]

ë¨¼ì € ìœ„ì™€ ê°™ì€ single time stepì˜ (input, label) ìŒì„ ìƒì„±í•˜ë„ë¡ ê°ì²´ë¥¼ êµ¬ì„±í•œë‹¤.

```python
target = 'T (degC)'
single_step_window = ds.WindowGenerator(
    input_width=1, label_width=1, shift=1, batch_size = 32, target=target,
    label_columns=[target])
single_step_window
```

```
Total window size: 2
Input indices: [0]
Label indices: [1]
Label column name(s): ['T (degC)']
```

ìƒì„±í•œ ê°ì²´ëŠ” train, validation, test ë°ì´í„° ì„¸íŠ¸ë¡œë¶€í„° tf.data.Datasetsë¥¼ ìƒì„±í•˜ë¯€ë¡œ ì‰½ê²Œ ë°°ì¹˜ë¥¼ ì–»ì„ ìˆ˜ ìˆìœ¼ë©° ë°˜ë³µ ë˜í•œ ê°€ëŠ¥í•˜ë‹¤. ë°ì´í„° ì…‹ì˜ ì˜ˆì‹œëŠ” (batch size, time step, features)ì˜ í˜•íƒœë¥¼ ë„ê³  ìˆë‹¤.

```python
for example_inputs, example_labels in single_step_window.train.take(1):
  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')
  print(f'Labels shape (batch, time, features): {example_labels.shape}')
```

````
Inputs shape (batch, time, features): (32, 1, 19)
Labels shape (batch, time, features): (32, 1, 1)
````

### 2.1 Linear model

í•˜ë‚˜ì˜ Linear layerë¥¼ í†µí•´ ê°€ì¥ ê°„ë‹¨í•œ ëª¨ë¸ì„ êµ¬ì¶•í•œë‹¤. ì´ë•Œ unitì˜ ìˆ˜ëŠ” ì˜ˆì¸¡í•  time stepì´ë‹¤. í˜„ì¬ëŠ” single stepì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•˜ê³  ìˆê¸° ë•Œë¬¸ì— 1ë¡œ ì„¤ì •í•œë‹¤.

```python
linear = tf.keras.Sequential([
    tf.keras.layers.Dense(units=1)
])

# ìƒ˜í”Œ ë°ì´í„° ì˜ˆì‹œ
print('Input shape:', single_step_window.example[0].shape)
print('Output shape:', linear(single_step_window.example[0]).shape)
```

í›ˆë ¨ì€ ëª¨ë¸ ë° step sizeë§Œ ë°”ê¾¸ì–´ ì—¬ëŸ¬ ì¡°ê±´ìœ¼ë¡œ ì§„í–‰í•  ê²ƒì´ê¸° ë•Œë¬¸ì— í›ˆë ¨ ì ˆì°¨ë¥¼ í•˜ë‚˜ì˜ í•¨ìˆ˜ íŒ¨í‚¤ì§€ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.

````python
MAX_EPOCHS = 20
val_performance = {}
performance = {}

def compile_and_fit(model, window, patience=2):
  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                    patience=patience,
                                                    mode='min')

  model.compile(loss=tf.keras.losses.MeanSquaredError(),
                optimizer=tf.keras.optimizers.Adam(),
                metrics=[tf.keras.metrics.MeanAbsoluteError()])

  history = model.fit(window.train, epochs=MAX_EPOCHS,
                      validation_data=window.val,
                      callbacks=[early_stopping])
  return history
````

êµ¬ì¶•í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ì„ ì§„í–‰í•œë‹¤.

```python
history = compile_and_fit(linear, single_step_window)

val_performance['Linear'] = linear.evaluate(single_step_window.val)
performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)
```

```
Epoch 1/20
1534/1534 [==============================] - 9s 6ms/step - loss: 0.6883 - mean_absolute_error: 0.5809 - val_loss: 0.0796 - val_mean_absolute_error: 0.2212
Epoch 2/20
1534/1534 [==============================] - 11s 7ms/step - loss: 0.0204 - mean_absolute_error: 0.0966 - val_loss: 0.0039 - val_mean_absolute_error: 0.0471
Epoch 3/20
1534/1534 [==============================] - 11s 7ms/step - loss: 0.0054 - mean_absolute_error: 0.0424 - val_loss: 0.0030 - val_mean_absolute_error: 0.0403
Epoch 4/20
1534/1534 [==============================] - 9s 6ms/step - loss: 0.0051 - mean_absolute_error: 0.0399 - val_loss: 0.0029 - val_mean_absolute_error: 0.0399
Epoch 5/20
1534/1534 [==============================] - 9s 6ms/step - loss: 0.0050 - mean_absolute_error: 0.0398 - val_loss: 0.0029 - val_mean_absolute_error: 0.0400
Epoch 6/20
1534/1534 [==============================] - 9s 6ms/step - loss: 0.0051 - mean_absolute_error: 0.0398 - val_loss: 0.0029 - val_mean_absolute_error: 0.0401
439/439 [==============================] - 2s 4ms/step - loss: 0.0029 - mean_absolute_error: 0.0401
```

ê²°ê³¼ê°€ ì© ì¢‹ì§€ ëª»í•˜ë‚˜ ì˜ˆì¸¡í•œ ë‚´ìš©ì„ ìƒ˜í”Œì„ í™•ì¸í•´ë³¸ë‹¤.

```python
single_step_window.plot(linear)
```

ğŸ¤ª[image12]

Input ë°ì´í„° í¬ì¸íŠ¸ì™€ output ë°ì´í„° í¬ì¸íŠ¸ ê·¸ë¦¬ê³  ì˜ˆì¸¡í•œ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. í•™ìŠµ ê²°ê³¼ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì¢‹ì§€ ëª»í•œ ëª¨ìŠµì´ë‹¤.

ì´ë²ˆì—” Input lengthë¥¼ ì¡°ì •í•˜ì—¬ 24ì‹œê°„ì˜ Input ë°ì´í„°ê°€ ë“¤ì–´ê°”ì„ ë•Œ ê° ì‹œê°„ì˜ 1step ë’¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°ì´í„° ì…‹ì„ ì•„ë˜ì™€ ê°™ì´ êµ¬ì„±í•œë‹¤. ì—¬ëŸ¬ Inputì´ ë“¤ì–´ê°”ì„ ë¿ ì˜ˆì¸¡í•˜ëŠ” stepì€ í•˜ë‚˜(single)ì´ë‹¤.

ğŸ¤ª[image13]

ì´í›„ linear modelì„ ì ìš©í•˜ì—¬ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤.

```python
wide_window = WindowGenerator(
    input_width=24, label_width=24, shift=1, batch_size = 32, target=target,
    label_columns=['T (degC)'])

wide_window
```

```
Total window size: 25
Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]
Label indices: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]
Label column name(s): ['T (degC)']
```

```python
history = compile_and_fit(linear, wide_window)

val_performance_wide['Linear'] = linear.evaluate(wide_window.val)
performance_wide['Linear'] = linear.evaluate(wide_window.test, verbose=0)
```

````
Epoch 1/20
1533/1533 [==============================] - 16s 10ms/step - loss: 0.0049 - mean_absolute_error: 0.0390 - val_loss: 0.0029 - val_mean_absolute_error: 0.0399
Epoch 2/20
1533/1533 [==============================] - 12s 8ms/step - loss: 0.0049 - mean_absolute_error: 0.0390 - val_loss: 0.0028 - val_mean_absolute_error: 0.0396
Epoch 3/20
1533/1533 [==============================] - 8s 5ms/step - loss: 0.0049 - mean_absolute_error: 0.0389 - val_loss: 0.0028 - val_mean_absolute_error: 0.0394
Epoch 4/20
1533/1533 [==============================] - 10s 6ms/step - loss: 0.0049 - mean_absolute_error: 0.0389 - val_loss: 0.0028 - val_mean_absolute_error: 0.0394
Epoch 5/20
1533/1533 [==============================] - 9s 6ms/step - loss: 0.0049 - mean_absolute_error: 0.0389 - val_loss: 0.0028 - val_mean_absolute_error: 0.0395
438/438 [==============================] - 2s 4ms/step - loss: 0.0028 - mean_absolute_error: 0.0395
````

````python
wide_window.plot(linear)
````

ğŸ¤ª[image14]

ìˆ˜í–‰ ê²°ê³¼ ëŒ€ì²´ë¡œ ì˜ ë§ëŠ” ê²ƒ ì²˜ëŸ¼ ë³´ì´ë‚˜ ì–´ëŠ ì‹œì ì—ëŠ” ë§ì§€ ì•ŠëŠ” ê·¸ë˜í”„ë„ ë³´ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ë˜í•œ validation datasetì— ëŒ€í•´ maeê°€ ì¤„ì–´ë“  ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” í•™ìŠµì´ ì˜ ë˜ì—ˆë‹¤ëŠ” ì˜ë¯¸ì¼ ìˆ˜ë„ ìˆìœ¼ë‚˜ metricì˜ í‰ê· ì„ ë‚´ëŠ” íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ê°’ì´ ë‚®ì•„ì§„ ê²ƒì¼ ìˆ˜ ìˆë‹¤.

Linear Modelì˜ ì¥ì ì€ í•´ì„í•˜ê¸°ê°€ ìƒëŒ€ì ìœ¼ë¡œ ê°„ë‹¨í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. êµ¬ì„±í•œ Dense Layerì— ìˆëŠ” weightë¥¼ ê°€ì ¸ì™€ ê° Input featureì— í• ë‹¹ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‹œê°í™” í•  ìˆ˜ ìˆë‹¤.

````python
plt.bar(x = range(len(wide_window.train_df.columns)),
        height=linear.layers[0].kernel[:,0].numpy())
axis = plt.gca()
axis.set_xticks(range(len(wide_window.train_df.columns)))
_ = axis.set_xticklabels(wide_window.train_df.columns, rotation=90)
````

ğŸ¤ª[image15]

ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” T (degC)ì— ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ ë‘” ëª¨ë¸ì´ ìƒì„±ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” ì ˆëŒ€ì ì¸ ê²ƒì€ ì•„ë‹ˆê³ , Layerì˜ weightë¥¼ ì–´ë–»ê²Œ ì´ˆê¸°í™” í•˜ëŠëƒì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.

### 2.2 Multi layer

ì´ë²ˆì—ëŠ” ëª¨ë¸ êµ¬ì„± ì‹œ í•œ ì¸µì´ ì•„ë‹Œ ë‹¤ì¸µì˜ layerë¥¼ ìŒ“ì€ ê²°ê³¼ë¥¼ í™•ì¸í•´ ë³¸ë‹¤.

````python
dense = tf.keras.Sequential([
    tf.keras.layers.Dense(units=64, activation='relu'),
    tf.keras.layers.Dense(units=64, activation='relu'),
    tf.keras.layers.Dense(units=1)
])

history = compile_and_fit(dense, wide_window)

val_performance_wide['Dense'] = dense.evaluate(wide_window.val)
performance_wide['Dense'] = dense.evaluate(wide_window.test, verbose=0)
````

````
Epoch 1/20
1533/1533 [==============================] - 20s 12ms/step - loss: 0.0205 - mean_absolute_error: 0.0701 - val_loss: 0.0046 - val_mean_absolute_error: 0.0520
Epoch 2/20
1533/1533 [==============================] - 13s 8ms/step - loss: 0.0048 - mean_absolute_error: 0.0432 - val_loss: 0.0037 - val_mean_absolute_error: 0.0458
Epoch 3/20
1533/1533 [==============================] - 14s 9ms/step - loss: 0.0043 - mean_absolute_error: 0.0405 - val_loss: 0.0035 - val_mean_absolute_error: 0.0447
Epoch 4/20
1533/1533 [==============================] - 13s 9ms/step - loss: 0.0040 - mean_absolute_error: 0.0392 - val_loss: 0.0035 - val_mean_absolute_error: 0.0444
Epoch 5/20
1533/1533 [==============================] - 13s 9ms/step - loss: 0.0038 - mean_absolute_error: 0.0382 - val_loss: 0.0032 - val_mean_absolute_error: 0.0424
Epoch 6/20
1533/1533 [==============================] - 13s 8ms/step - loss: 0.0037 - mean_absolute_error: 0.0382 - val_loss: 0.0033 - val_mean_absolute_error: 0.0426
Epoch 7/20
1533/1533 [==============================] - 13s 9ms/step - loss: 0.0035 - mean_absolute_error: 0.0376 - val_loss: 0.0037 - val_mean_absolute_error: 0.0460
438/438 [==============================] - 2s 5ms/step - loss: 0.0037 - mean_absolute_error: 0.0460
````

ìˆ˜í–‰ ê²°ê³¼ ì˜¤íˆë ¤ maeê°€ ë†’ì•„ì§„ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ëª¨ë¸ì´ ë³µì¡í•œ ê²ƒ(=ê¹Šì€ ê²ƒ)ì´ ëŠ¥ì‚¬ê°€ ì•„ë‹˜ì„ í™•ì¸í•˜ì˜€ë‹¤.

### 2.3 Multi step layer

ğŸ¤ª[image15]



