---
layout: single
title:  'Autoencoderì˜ ëª¨ë“ ê²ƒì˜ ëª¨ë“ ê²ƒ (1) ì˜¤í† ì¸ì½”ë”ë€?'
toc: true
categories: Autoencoder
tags: [Autoencoder]
---

ë³¸ ê²Œì‹œë¬¼ì€ ì´í™œì„ë‹˜ì˜ [ê°•ì˜](https://www.youtube.com/watch?v=o_peo6U7IRM&t=4)ë¥¼ ë³´ê³  ì •ë¦¬í•˜ëŠ” ê¸€ì´ë‹¤.
{: .notice}

<div class="notice">
<li><a href="https://sigirace.github.io/autoencoder/autoencoder_2/">Autoencoderì˜ ëª¨ë“ ê²ƒì˜ ëª¨ë“ ê²ƒ (2) Maximum Likelihood ê´€ì ì˜ Deep Learning</a></li>
<li><a href="https://sigirace.github.io/autoencoder/autoencoder_3/">Autoencoderì˜ ëª¨ë“ ê²ƒì˜ ëª¨ë“ ê²ƒ (3) Manifold Learning</a></li>
</div>

## 1. Introduction

### 1.1 Autoencoder

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/autoencoder/autoencoder1_0.png?raw=true" width="300"></p>

> An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). The encoding is validated and refined by attempting to regenerate the input from the encoding. The autoencoder learns a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore insignificant data (â€œnoiseâ€). 

<center>ì¶œì²˜: https://en.wikipedia.org/wiki/Autoencoder</center><br>

autoencoderëŠ” ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ì˜ íš¨ìœ¨ì ì¸ ì½”ë”©ì„ í•™ìŠµ(=unsupervised learning)í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì¸ê³µ ì‹ ê²½ë§ì˜ í•œ ìœ í˜•ì´ë‹¤. ì¸ì½”ë”©ìœ¼ë¡œë¶€í„° Inputì„ ì¬ìƒì„±í•˜ë ¤ëŠ” ì‹œë„ë¥¼ í†µí•´ ì¸ì½”ë”©ì€ ê²€ì¦ë˜ê³  ì •ì œëœë‹¤. ì¼ë°˜ì ìœ¼ë¡œ autoencoderëŠ” ì°¨ì›ì¶•ì†Œë¥¼ ìœ„í•´ ì¤‘ìš”í•˜ì§€ ì•Šì€ ë°ì´í„°(noise)ë¥¼ ë¬´ì‹œí•˜ë„ë¡ í›ˆë ¨ì‹œí‚´ìœ¼ë¡œì¨ ë°ì´í„° ì…‹ì— ëŒ€í•œ <mark style='background-color: #f6f8fa'> representation(ì¸ì½”ë”©)</mark>ì„ í•™ìŠµí•œë‹¤.

ğŸ‘€ **keywords**

```markdown
1. Unsupervised learning
2. Representation learning
   = Efficient coding learning
3. Dimensionality reduction
4. Generative model learning
```

autoencoderì˜ ì¤‘ìš”í•œ taskëŠ” <mark style='background-color: #f6f8fa'> Dimensionality reduction </mark>ì´ë©° ìµœê·¼ì—ëŠ” <mark style='background-color: #f6f8fa'> Generative model learning </mark>ìœ¼ë¡œë„ ì‚¬ìš©í•˜ê³  ìˆë‹¤. 

### 1.2 Nonlinear dimensionality reduction(NLDR)

> Nonlinear dimensionality reduction, also known as manifold learning, refers to various related techniques that aim to project high-dimensional data onto lower-dimensional latent manifolds, with the goal of either visualizing the data in the low-dimensional space, or learning the mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa) itself. The techniques described below can be understood as generalizations of linear decomposition methods used for dimensionality reduction, such as singular value decomposition and principal component analysis.

<center>ì¶œì²˜: https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction</center><br>

Manifold learningìœ¼ë¡œ ì•Œë ¤ì§„ ë¹„ì„ í˜• ì°¨ì› ì¶•ì†ŒëŠ” ê³ ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ë‚®ì€ ì°¨ì›ì˜ ì ì¬ì ì¸ mainfoldë“¤ë¡œ íˆ¬ì˜, ì‹œê°í™” ë° ë§¤í•‘ì„ í•™ìŠµí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ì´ëŸ¬í•œ ê¸°ë²•ì€ svd ê·¸ë¦¬ê³  pcaì™€ ê°™ì´ ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•œ linear decompositionì˜ ì¼ë°˜í™”ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤. (manifoldëŠ” ë’· chapterì—ì„œ ì„¤ëª…)

ğŸ‘€ **keywords**

```markdown
1. Unsupervised learning
2. Nonlinear Dimensionality reduction
   = Representation learning
   = Efficient coding learning
   = Feature extraction
   = Manifold learning
3. Generative model learning
```

### 1.3 Representation learning

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/autoencoder/autoencoder1_1.png?raw=true" width="400" height="270"></p>

<center>ì¶œì²˜: http://videolectures.net/kdd2014_bengio_deep_learning/</center><br>

ë²¤ì§€ì˜¤ êµìˆ˜ì˜ ê¸°ìˆ  ë¶„ë¥˜í‘œ ìƒì—ì„œ autoencoderëŠ” representation learningì— ì†í•œë‹¤ê³  í•œë‹¤. 

### 1.4 ML density estimation

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/autoencoder/autoencoder1_2.png?raw=true" width="400" height="270"></p>

<center>ì¶œì²˜: http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf</center><br>

ì´ì•ˆ êµ³í ë¡œìš°ê°€ ë§Œë“  ë¶„ë¥˜í‘œë¥¼ ë³´ë©´ Variational(variational autoencoder)ì´ Maximum Likelihoodì— ì†í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.

### 1.6 Summary

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/autoencoder/autoencoder1_3.png?raw=true" width="650" height="400"></p>

ì…ë ¥ê³¼ ì¶œë ¥ì´ ë™ì¼í•œ ê°’ì„ ë§Œë“œëŠ” ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©´ autoencoderë¼ ì¹­í•˜ëŠ”ë°, ì´ë•Œ ê´€ê³„ë˜ëŠ” í‚¤ì›Œë“œë¥¼ ì•„ë˜ì˜ 4ê°œë¡œ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤.

ğŸ‘€ **keywords**

```markdown
1. Unsupervised learning
2. Manifold learning
3. Generative model learning
4. ML density estimation
```

ìœ„ ë„¤ê°€ì§€ í‚¤ì›Œë“œê°€ ì–´ë–»ê²Œ ê´€ê³„ê°€ ìˆëŠ”ì§€ ì‚´í´ë³´ë©´, ë¨¼ì € autoencoderëŠ” <mark style='background-color: #f6f8fa'> 1. unsupervised learning </mark>ìœ¼ë¡œ í•™ìŠµì´ ì´ë£¨ì–´ì§€ê³  ì´ë•Œ lossë¥¼ ìµœì†Œí™” í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì„ ì‹œí‚¤ëŠ” loss functionì— ëŒ€í•œ í•´ì„ì´ <mark style='background-color: #f6f8fa'> 4. Maximum Likelihood density estimation </mark>ê³¼ ê´€ê³„ìˆê²Œ ëœë‹¤. ì´ë ‡ê²Œ í•™ìŠµí•œ autoencoderì—ì„œ ì…ë ¥ ë¶€ë¶„ì„ ë³´ë©´ ì°¨ì› ì¶•ì†Œì´ê¸° ë•Œë¬¸ì— <mark style='background-color: #f6f8fa'> 2. Manifold learning </mark>, ì¶œë ¥ ë¶€ë¶„ì„ ë³´ë©´ ìƒì„± ëª¨ë¸ì´ê¸° ë•Œë¬¸ì— <mark style='background-color: #f6f8fa'> 3. Generative model learning </mark>ì˜ ê°œë…ê³¼ ê´€ë ¨ì´ ìˆë‹¤.
