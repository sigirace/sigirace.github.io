

wnadb 사용법: https://minimin2.tistory.com/186

early stopping: https://velog.io/@jg31109/Early-stopping

reducelronplateau: https://bo-10000.tistory.com/95

그래디언트 클리핑: https://kh-kim.gitbook.io/natural-language-processing-with-pytorch/00-cover-6/05-gradient-clipping

wandb lstm 예시: https://wandb.ai/wandb_fc/korean/reports/PyTorch-LSTM---VmlldzoyNDc4NjY4#%EC%83%98%ED%94%8C-%EB%AA%A8%EB%8D%B8-%EC%BD%94%EB%93%9C

lstm 코드 구현: https://ok-lab.tistory.com/209

lstm 코드 구현: https://notebook.community/jhjungCode/pytorch-tutorial/16_MNIST-RNN(LSTM)

initialization PyTorch: https://gist.github.com/SauravMaheshkar/5704edf87c33ab09033dc9c0a10adaa1

gradient histogram: https://stackoverflow.com/questions/42315202/understanding-tensorboard-weight-histograms

gradient: https://sdolnote.tistory.com/entry/Gradient

many to one: https://discuss.pytorch.org/t/many-to-one-lstm-input-shape/142468

one to many: https://discuss.pytorch.org/t/how-to-create-a-lstm-with-one-to-many/108659/4

keras: https://stackoverflow.com/questions/52138290/how-can-we-define-one-to-one-one-to-many-many-to-one-and-many-to-many-lstm-ne/52139618#52139618

return_sequence: https://stackoverflow.com/questions/62204109/return-sequences-false-equivalent-in-pytorch-lstm

keras -ml : https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/

TimeDistributed: https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=chunjein&logNo=221589624838

LSTM AE : https://data-newbie.tistory.com/136

https://data-newbie.tistory.com/567

LSTM-KERAS (KR): https://yjjo.tistory.com/32
