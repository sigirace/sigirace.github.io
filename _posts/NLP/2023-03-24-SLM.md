---
layout: single
title:  'LM(2) 통계적 언어모델'
toc: true
categories: [Language Model]
tags: [LM]


---

언어모델의 통계적인 접근 방법에 대해 알아본다.
{: .notice}

##  1. 통계적 언어모델

전통적 언어 모델은 통계적으로 접근하는 방식을 사용하였다. 통계적 언어모델(Statistical Language Model)은 줄여서 SLM이라고도 한다.

### 1.1 문장에 대한 확률

문장은 단어들의 연결로 이루어진다. 단어들의 연결은 문맥을 이루며, 뜬금없는 경우가 아니라면 어떤 단어들의 연결 뒤에는 특정 단어가 나올 확률이 높다. 따라서 문장의 확률은 어떤 단어의 나열이 주어졌을 때, 다음 단어가 나올 확률인 조건부 확률의 곱으로 구성된다.


$$
P(w_1, w_2, w_3, ... ,w_n) = \prod_{n=1}^{n}P(w_n|w1,...,w_{n-1})
$$


📍**Example**

> P(My dog is very cute) = P(My) * P(dog/My) * P(is/My dog) * P(very/My dog is) * P(cute/ My dog is very)



### 1.2 카운트 기반의 접근

문장의 확률을 구하기 위해 다음 단어에 대한 예측 확률을 모두 곱하는 방식을 사용한다. 이때, 다음 단어에 대한 예측 확률은 카운트 기반 확률을 사용한다. 



📍**Example**

> 학습시키는 전체 데이터에서 My dog is very가 100번 등장했고, 그 다음 cute가 30번 등장했다면 My dog is very가 나왔을때, cute가 나올 확률 P(cute/ My dog is very)은 30%이다.



### 1.3 희소문제

모델은 학습을 통해 전체 데이터의 확률 분포를 근사하는 특징을 가지고 있다. 동일하게 언어 모델은 코퍼스(=학습 데이터)를 통해 실생활에서 사용되는 언어의 확률 분포를 근사하려고 한다. 하지만 언어 모델을 만들기 위해 카운트 기반의 확률을 사용한다면, 언어 모델을 현실에 근사시키기 어려운 문제에 마주치게 된다. 만약 My dog is very very cute라는 문장을 예측한다면 수식은 아래와 같을 것이다.


$$
P(cute|\ My\ dog\ is\ very\ very) = \frac{P(My\ dog\ is\ very\ very\ cute)}{P(My\ dog\ is\ very\ very)}
$$


그런데 만약 구성된 코퍼스에 My dog is very very cute라는 단어 나열이 한번도 등장하지 않았다면, P(My dog is very very cute)의 확률은 0일 것이다. 또한 My dog is very very라는 단어의 나열이 한번도 등장하지 않았다면, 해당 문장의 확률은 정의 될 수 없다, 하지만 현실에서 해당 문장은 충분히 나올 수 있으며 심지어 빈번히 사용될 수 도 있다. 이처럼 코퍼스에 위와 같은 문장이 없는 이유로 언어 모델이 문장을 전혀 예측할 수 없는 문제를 희소문제라고 한다.

위 문제를 완화하는 방법으로 바로 이어서 배우게 되는 n-gram 언어 모델이나, 스무딩, 백오프 같은 일반화 기법이 존재한다. 하지만 어떤 기법을 사용하여도 희소문제라는 한계에 부딪혔으며 결국 통계적 언어 모델에서 인공 신경망 언어 모델로 트렌드가 바뀌게 된다.

## 2. N-gram 언어모델

N-gram 언어모델 또한 카운트에 기반한 통계적 접근 방식을 사용하기에 SLM의 일종이다. 다만, 모든 단어를 고려하여 확률을 계산하는 방식이 아닌 일부 단어만을 고려하는 접근 방법을 사용한다. 이때 몇개의 단어를 고려하는지가 N에 해당하게된다.

### 2.1 카운트 기반 언어모델과의 차이

앞서 모든 코퍼스를 사용해 언어 모델을 구성한 경우, 예측할 문장이나 단어가 코퍼스 내에 없을 경우 확률을 계산할 수 없다는 문제가 있었다. 더하여 문장이 길어질수록 해당 문제가 발생할 확률이 커진다. 따라서, 주변 단어들을 통해 확률을 계산하는 N-gram의 경우는 이를 조금 완화시켰다고 볼 수 있다.

📍 **Example**

> My dog is very very 다음에 cute가 나오는 확률을 예측하고자 할때, My dog is very very라는 단어 시퀀스가 코퍼스내에 존재할 가능성은 낮지만, very라는 짧은 단어 시퀀스(한 단어)는 높은 등장 가능성을 가지고 있다. 따라서 P(cute | My dog is very very)는 계산할 수 없으나, P(cute | very)은 계산할 수 있게 된다.

### 2.2 N-gram

👀 **Definition**

````
코퍼스에서 n개의 단어 뭉치 단위로 끊어 생성한 연속적인 시퀀스의 형태이며, N-gram 언어 모델은 다음에 나올 단어의 예측을 오직 n-1개의 단어에만 의존한다. 
````

📍**Example**

> uni-gram : my, dog, is, very, very, cute<br>bi-gram : my dog, dog is, is very, very very, very cute<br>tri-gram : my dog is, dog is very, is very very, very very cute
>
> n=2인 bi-gram을 이용한 언어모델을 사용할 경우 다음 단어를 예측하기 위해 앞의 1개 단어만을 고려한다.<br>만약 가지고 있는 코퍼스에서 very가 1000번, very good이 500번, very cute가 200번 나왔다면, P(good | very) = 0.5, P(cute | very) = 0.2이기 때문에 확률적 선택에 따라 good이 더 맞다고 판단하게 된다.

### 2.3 N-gram의 한계

N-gram은 결국 확률 계산을 위해 한정적인 단어들 만을 보게된다. 이는 코퍼스의 구성에 따라 달라지겠지만, 우리가 보기에 말이 안되는 문맥의 문장을 형성할 수 있다. 즉, 전체 코퍼스를 보는 언어 모델에 비해 확률 계산에 대한 개선은 이루어졌지만, 정확도는 상대적으로 떨어졌다고 볼 수 있다. N-gram의 또 다른 한계점들은 아래와 같다.

**(1) 희소문제**

전체 코퍼스에 비해 희소 문제를 줄이긴 하였지만, 여전히 존재한다.

**(2) n을 선택하는 것에 대한 문제(trade-off)**

n을 크게 선택할 수록 언어 모델의 유연성(perplexity)는 좋아지나, 반대로 희소 문제가 커지게 된다.

**(3) Domain에 맞는 코퍼스 수집**

현재 사용되고 있는 chatGPT(open domain)와 같은 언어 모델과 다르게 코퍼스에 한정적이게 된다.

앞서 이야기 하였듯, N-gram 또한 희소문제를 해결하기 위한 일반화 기법들이 등장하였다. 하지만 본질적으로 n-gram의 취약점을 완전히 해결하지 못하였고, 결국 신경망 모델로 트렌드가 넘어가게 된다.



### 참조

https://wikidocs.net/21687
