---
layout: single
title:  'Llama 3.1 한국어 Finetuning'
toc: true
categories: [Large Language Model]
tags: [Llama, Fine-Tuning]

---

이번 포스트를 통해 unsloth 라이브러리를 사용하여 llama 3.1 모델 finetuning을 실습한다.
{: .notice}

## 📌 들어가기 전에..

👀 **Unsloth**

> **Unsloth**는 대형 언어 모델(LLM)을 효율적으로 미세 조정(fine-tuning)할 수 있도록 설계된 Python 라이브러리입니다. 이 라이브러리는 특히 **Llama, Mistral, Phi, Gemma**와 같은 모델들을 다루며, **PEFT**을 통해 **최대 5배** 빠르게 학습을 진행할 수 있고, 메모리 사용량도 **최대 80%**까지 절감할 수 있습니다. 

**❤️‍🔥 PEFT ❤️‍🔥**

- [이전 포스트 글 확인](https://sigirace.github.io/large%20language%20model/LLM%EC%9D%98-%ED%99%9C%EC%9A%A9%EB%B0%A9%EB%B2%95/#25-%EF%B8%8F-peft-%EF%B8%8F)

📜 **Github**

- [unsloth finetuning tutorial](https://github.com/unslothai/unsloth?tab=readme-ov-file)

🌈 **Environment**

- **모델**: Llama3.1 8B
- **실습환경**: GPU - T4 (colab, kaggle)

## 1. Install

```python
%%capture
# Installs Unsloth, Xformers (Flash Attention) and all other packages!
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes
```

- **xformers**: PyTorch에서 Transformer 기반 모델의 학습과 추론 성능을 최적화
- **trl(transformers reinforcement learning)**: 강화 학습을 Transformers 모델과 통합하는 라이브러리, 언어 모델을 강화 학습으로 훈련하거나 조정하는데 유용함
- **peft(parameter efficient fine-tuning)**: 언어 모델의 일부 파라미터만 미세 조정함으로써, 더 효율적으로 모델을 튜닝할 수 있게 해줌
- **accelerate**: Hugging Face에서 제공하는 라이브러리로, PyTorch 모델을 여러 GPU와 TPU에서 손쉽게 병렬 처리할 수 있게 함
- **bitsandbytes**: 8bit 수학 연산을 지원하여, 대형 모델의 메모리 사용을 줄이고 속도를 높일 수 있게 해주는 라이브러리로, 주로 메모리 제약이 있는 환경에서 사용함

✏️  **bit를 줄이는 것이 왜 필요할까?**

- 컴퓨터에서 데이터는 비트 단위로 저장되며, AI/ML 영역의 대표적인 데이터인 모델의 가중치는 32 bit 부동소수점으로 표현
- 이를 8bit 정수로 표현하면, 데이터의 크기가 4분의 1로 줄어들며 이는 parameter가 수백만에서 수십억 개 존재할 경우 매우 효과적
- 저장하는 bit를 줄이는 과정을 양자화라고 하며, 이로인해 모델 크기가 작아져 작은 규모의 인프라 환경(*메모리*)에서도 동작할 수 있게 됨
- 단, 연산의 정밀도가 떨어져 성능이 저하될 수 있으며, 구현이 복잡한 단점이 있음





```html
<div>
hello
<div>
	<div><!-- 적색 버튼 --></div>
	<div><!-- 황색 버튼 --></div>
	<div><!-- 녹색 버튼 --></div>
</div>

<pre>
print("hi")
</pre>
</div>
```



## 예제 코드

```html
<!-- HTML 코드 블럭 -->
<div class="code-container">
    <pre id="code-block">
        <code>
def hello_world():
    print("Hello, world!")
        </code>
    </pre>
    <button class="copy-button" onclick="copyToClipboard()">Copy</button>
</div>

<script>
function copyToClipboard() {
    var code = document.getElementById("code-block").innerText;
    navigator.clipboard.writeText(code).then(function() {
        alert("Copied to clipboard!");
    }, function(err) {
        alert("Failed to copy text: ", err);
    });
}
</script>
```

