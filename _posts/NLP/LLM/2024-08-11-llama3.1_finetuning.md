---
layout: single
title:  'Llama 3.1 í•œêµ­ì–´ Finetuning'
toc: true
categories: [Large Language Model]
tags: [Llama, Fine-Tuning]

---

ì´ë²ˆ í¬ìŠ¤íŠ¸ë¥¼ í†µí•´ unsloth ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ llama 3.1 ëª¨ë¸ finetuningì„ ì‹¤ìŠµí•œë‹¤.
{: .notice}

## ğŸ’¬ ë“¤ì–´ê°€ê¸° ì „ì—..

ğŸ‘€ **Unsloth**

> **Unsloth**ëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •(fine-tuning)í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” íŠ¹íˆ **Llama, Mistral, Phi, Gemma**ì™€ ê°™ì€ ëª¨ë¸ë“¤ì„ ë‹¤ë£¨ë©°, **PEFT**ì„ í†µí•´ **ìµœëŒ€ 5ë°°** ë¹ ë¥´ê²Œ í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆê³ , ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ë„ **ìµœëŒ€ 80%**ê¹Œì§€ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

**â¤ï¸â€ğŸ”¥ PEFT â¤ï¸â€ğŸ”¥**

- [ì´ì „ í¬ìŠ¤íŠ¸ ê¸€ í™•ì¸](https://sigirace.github.io/large%20language%20model/LLM%EC%9D%98-%ED%99%9C%EC%9A%A9%EB%B0%A9%EB%B2%95/#25-%EF%B8%8F-peft-%EF%B8%8F)

ğŸ“œ **Github**

- [unsloth finetuning tutorial](https://github.com/unslothai/unsloth?tab=readme-ov-file)

ğŸŒˆ **Environment**

- **ëª¨ë¸**: Llama3.1 8B
- **ì‹¤ìŠµí™˜ê²½**: GPU - T4 (colab, kaggle)

## 1. Install

```python
%%capture
# Installs Unsloth, Xformers (Flash Attention) and all other packages!
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes
```

ğŸ“ **Install list**

- **xformers**: PyTorchì—ì„œ Transformer ê¸°ë°˜ ëª¨ë¸ì˜ í•™ìŠµê³¼ ì¶”ë¡  ì„±ëŠ¥ì„ ìµœì í™”
- **trl(transformers reinforcement learning)**: ê°•í™” í•™ìŠµì„ Transformers ëª¨ë¸ê³¼ í†µí•©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬, ì–¸ì–´ ëª¨ë¸ì„ ê°•í™” í•™ìŠµìœ¼ë¡œ í›ˆë ¨í•˜ê±°ë‚˜ ì¡°ì •í•˜ëŠ”ë° ìœ ìš©í•¨
- **peft(parameter efficient fine-tuning)**: ì–¸ì–´ ëª¨ë¸ì˜ ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ ë¯¸ì„¸ ì¡°ì •í•¨ìœ¼ë¡œì¨, ë” íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ì„ íŠœë‹í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ
- **accelerate**: Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, PyTorch ëª¨ë¸ì„ ì—¬ëŸ¬ GPUì™€ TPUì—ì„œ ì†ì‰½ê²Œ ë³‘ë ¬ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•¨
- **bitsandbytes**: 4bit, 8bit ìˆ˜í•™ ì—°ì‚°ì„ ì§€ì›í•˜ì—¬ ëŒ€í˜• ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì´ê³  ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ì£¼ë¡œ ë©”ëª¨ë¦¬ ì œì•½ì´ ìˆëŠ” í™˜ê²½ì—ì„œ ì‚¬ìš©í•¨

âœï¸  **bitë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ì™œ í•„ìš”í• ê¹Œ?**

- ì»´í“¨í„°ì—ì„œ ë°ì´í„°ëŠ” ë¹„íŠ¸ ë‹¨ìœ„ë¡œ ì €ì¥ë˜ë©°, AI/ML ì˜ì—­ì˜ ëŒ€í‘œì ì¸ ë°ì´í„°ì¸ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ëŠ” 32 bit ë¶€ë™ì†Œìˆ˜ì ìœ¼ë¡œ í‘œí˜„
- ì´ë¥¼ 8bit ì •ìˆ˜ë¡œ í‘œí˜„í•˜ë©´, ë°ì´í„°ì˜ í¬ê¸°ê°€ 4ë¶„ì˜ 1ë¡œ ì¤„ì–´ë“¤ë©° ì´ëŠ” parameterê°€ ìˆ˜ë°±ë§Œì—ì„œ ìˆ˜ì‹­ì–µ ê°œ ì¡´ì¬í•  ê²½ìš° ë§¤ìš° íš¨ê³¼ì 
- ì €ì¥í•˜ëŠ” bitë¥¼ ì¤„ì´ëŠ” ê³¼ì •ì„ ì–‘ìí™”ë¼ê³  í•˜ë©°, ì´ë¡œì¸í•´ ëª¨ë¸ í¬ê¸°ê°€ ì‘ì•„ì ¸ ì‘ì€ ê·œëª¨ì˜ ì¸í”„ë¼ í™˜ê²½(*ë©”ëª¨ë¦¬*)ì—ì„œë„ ë™ì‘í•  ìˆ˜ ìˆê²Œ ë¨
- ë‹¨, ì—°ì‚°ì˜ ì •ë°€ë„ê°€ ë–¨ì–´ì ¸ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìœ¼ë©°, êµ¬í˜„ì´ ë³µì¡í•œ ë‹¨ì ì´ ìˆìŒ

## 2. Model Load

### 2.1 Pretrained model load

```python
from unsloth import FastLanguageModel
import torch

max_seq_length = 2048  # ì„ì˜ë¡œ ì„ íƒ ê°€ëŠ¥! ë‚´ë¶€ì ìœ¼ë¡œ RoPE Scalingì„ ìë™ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤.
dtype = None  # Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ ìë™ìœ¼ë¡œ ê°ì§€ë©ë‹ˆë‹¤. Tesla T4, V100ì—ëŠ” Float16, Ampere ì´ìƒì—ì„œëŠ” Bfloat16ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
load_in_4bit = True  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ 4ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Falseë¡œ ì„¤ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)
```

ğŸ“ **from_pretrained**

- unsloth ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë¯¸ë¦¬ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ê³  ì„¤ì •í•˜ëŠ” í•¨ìˆ˜
- ì–¸ì–´ ëª¨ë¸(LLM)ì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œí•˜ê³ , í•„ìš”ì— ë”°ë¼ ë¯¸ì„¸ ì¡°ì •(PEFT) ë˜ëŠ” ì–‘ìí™”(Quantization) ê¸°ëŠ¥ì„ ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ í•¨

ğŸ“Œ **Parameters**

- *max_seq_length*
  - ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ì§€ì •
  - RoPE(ìƒëŒ€ì  ìœ„ì¹˜ ì¸ì½”ë”©, Rotary Position Embedding): ëª¨ë¸ ë‚´ë¶€ì— ì ìš©ë˜ì–´ ë” ê¸´ ì‹œí€€ìŠ¤ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ í•¨
  - ì˜ˆì œì—ì„œëŠ” 2048ê°œì˜ í† í° ì²˜ë¦¬í•˜ì˜€ìœ¼ë‚˜ RoPEë¡œ ì¸í•´ 4096ê¹Œì§€ë„ ê°€ëŠ¥
- *dtype*
  - ë°ì´í„° íƒ€ì…ì„ ì§€ì •
  - None ì¼ì‹œ ìë™ìœ¼ë¡œ ìµœì ì˜ íƒ€ì… ê°ì§€
  - ex) T4, V100ì—ì„œëŠ” Float16, Ampere ì´ìƒì€ Bfloat16
- *load_in_4bit*
  - ëª¨ë¸ì´ 4bit ì–‘ìí™” ëœ ìƒíƒœë¡œ ë¡œë“œë  ìˆ˜ ìˆê²Œ í•¨



### 2.2 peft model load

```python
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # 0ë³´ë‹¤ í° ìˆ«ìë¥¼ ì„ íƒí•˜ì„¸ìš”! ê¶Œì¥ê°’: 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # ì–´ë–¤ ê°’ì´ë“  ì§€ì›í•˜ì§€ë§Œ, 0ì´ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤
    bias = "none",    # ì–´ë–¤ ê°’ì´ë“  ì§€ì›í•˜ì§€ë§Œ, "none"ì´ ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤
    # [ìƒˆë¡œìš´ ê¸°ëŠ¥] "unsloth"ëŠ” 30% ë” ì ì€ VRAMì„ ì‚¬ìš©í•˜ë©°, 2ë°° ë” í° ë°°ì¹˜ í¬ê¸°ë¥¼ ì§€ì›í•©ë‹ˆë‹¤!
    use_gradient_checkpointing = "unsloth", # ë§¤ìš° ê¸´ ì»¨í…ìŠ¤íŠ¸ì˜ ê²½ìš° True ë˜ëŠ” "unsloth" ì‚¬ìš©
    random_state = 3407,
    use_rslora = False,  # ìˆœìœ„ ì•ˆì •í™” LoRAë¥¼ ì§€ì›í•©ë‹ˆë‹¤
    loftq_config = None, # ê·¸ë¦¬ê³  LoftQë„ ì§€ì›í•©ë‹ˆë‹¤
)
```

ğŸ“ **get_peft_model**

- FastLanguageModel í´ë˜ìŠ¤ì—ì„œ PEFT (Parameter-Efficient Fine-Tuning) ê¸°ë²•ì„ ì ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë©”ì„œë“œ
- ëª©ì : RoRA(Residual on Residual Adapter)ë‚˜ LoRA(Low-Rank Adaptation)ì™€ ê°™ì€ íš¨ìœ¨ì ì¸ ë¯¸ì„¸ ì¡°ì • ê¸°ë²•ì„ í†µí•´ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê³ , ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒ

ğŸ“Œ **Parameters**

- *r*
  - LoRA ê¸°ë²•ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë­í¬(rank)
  - ì´ ê°’ì´ í´ìˆ˜ë¡ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì€ ì¦ê°€í•˜ì§€ë§Œ, ë©”ëª¨ë¦¬ì™€ ê³„ì‚° ë¹„ìš©ë„ ëŠ˜ì–´ë‚¨
  - ê¶Œì¥ê°’: 8, 16, 32, 64, 128 ë“±
- *target_modules*
  - LoRAë¡œ fine tuningì‹œ transformer ëª¨ë¸ ì•„í‚¤í…ì²˜ì—ì„œ íƒ€ê²ŸíŒ…í•  ëª¨ë“ˆ ë¦¬ìŠ¤íŠ¸
- *lora_alpha*
  - LoRAì—ì„œì˜ ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°
  - í•™ìŠµ ì¤‘ Low Rank Adapaterì˜ ì¶œë ¥ì„ ì¡°ì ˆí•˜ëŠ” ë° ì‚¬ìš©ë¨
  - ê¸°ë³¸ê°’: 16, ëª¨ë¸ì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ í•  ìˆ˜ ìˆë„ë¡ ìµœì í™” ëœ ê°’
- *lora_dropout*
  - í•™ìŠµ ì¤‘ Dropoutì„ ì ìš©í•  í™•ë¥ ë¡œ ëª¨ë¸ì˜ ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë¨
  - ê¶Œì¥ê°’: 0 (dropout ë¹„í™œì„±í™”)
- *bias*
  - í¸í–¥(bias)ì„ ì¶”ê°€í• ì§€ ì—¬ë¶€ë¥¼ ì„¤ì •
  - ì˜µì…˜: â€œnoneâ€, â€œallâ€, â€œlora_onlyâ€ 
  - ê¶Œì¥ê°’: "none" (í¸í–¥ ë¹„í™œì„±í™”)
- *use_gradient_checkpointing*
  - ê·¸ë¼ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´ì„œ, ì—­ì „íŒŒ ê³„ì‚° ì‹œ ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ ë‹¤ì‹œ ê³„ì‚°í•˜ëŠ” ë°©ë²•
  - ì˜µì…˜: True, False, "unsloth" (ìµœì í™”ëœ ì˜µì…˜).
- *use_rslora*
  - ìˆœìœ„ ì•ˆì •í™” LoRAë¥¼ ì‚¬ìš©í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •
- *loftq_config*
  - LoftQ(Quantization) ì„¤ì •ì„ ì •ì˜í•©ë‹ˆë‹¤. ì–‘ìí™”ë¥¼ í†µí•´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ë°©ë²•ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - ê¸°ë³¸ê°’: None(ì–‘ì ë¹„í™œì„±)

ğŸ·ï¸ **ì°¸ì¡°**

- ğŸ“œ [[ì´ë¡ \] LoRA (Low-Rank Adaptation)](https://x2bee.tistory.com/335)

âœï¸ **ìˆœìœ„ ì•ˆì •í™” LoRA (Rank-Stabilized Low-Rank Adaptation, RsLoRA)**

1. *LoRA ìˆœìœ„ ë¶ˆì•ˆì •ì„±*
   - LoRAëŠ” ì €ë­í¬ ê·¼ì‚¬(low-rank approximation)ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ê·¼ì‚¬ ê³¼ì •ì—ì„œ ì›ë³¸ í–‰ë ¬ì˜ ì •ë³´ê°€ ì¼ë¶€ ì†ì‹¤ë  ìˆ˜ ìˆìŒ
   - íŠ¹íˆ, ì €ë­í¬ í–‰ë ¬ë¡œ ì¸í•´ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì´ ì œí•œë  ìˆ˜ ìˆê³ , ì´ë¡œ ì¸í•´ ì„±ëŠ¥ì´ ì €í•˜ë˜ê±°ë‚˜ ë¶ˆì•ˆì •í•´ì§ˆ ê°€ëŠ¥ì„±ì„ ìˆœìœ„ ë¶ˆì•ˆì •ì„±ì´ë¼ í•¨
2. *í•´ê²°ë°©ë²•*
   - ì •ê·œí™”: ì €ë­í¬ í–‰ë ¬ì˜ ë…¸ë¦„(norm)ì„ ì •ê·œí™”í•˜ì—¬, í–‰ë ¬ì˜ í¬ê¸°ê°€ ë„ˆë¬´ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ê²ƒì„ ë°©ì§€
   - ì•ˆì •í™” ì œì•½: í–‰ë ¬ ë¶„í•´ ê³¼ì •ì—ì„œ ì¼ì •í•œ ì¡°ê±´ì„ ì¶”ê°€í•˜ì—¬, ì›ë³¸ í–‰ë ¬ì˜ ì •ë³´ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ê³ , ë¶„í•´ëœ ì €ë­í¬ í–‰ë ¬ì´ ë³´ë‹¤ ì•ˆì •ì ì¸ í‘œí˜„ì„ í•  ìˆ˜ ìˆë„ë¡ í•¨

âœï¸ **LoftQ(Quantization)**

- íŠ¹ì • í™˜ê²½ì—ì„œ 4ë¹„íŠ¸ ì–‘ìí™”ë¥¼ ì ìš©í•˜ì—¬, ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ë©´ì„œë„ ì„±ëŠ¥ ì €í•˜ë¥¼ ìµœì†Œí™”í•˜ë ¤ëŠ” ê¸°ìˆ 



## 3. Data

### 3.1 Data Prepare

- ğŸ’¾ [Huggingface-korea_safe_conversation](https://huggingface.co/datasets/jojo0217/korean_safe_conversation)

### 3.2 Data Loader

```python
from datasets import load_dataset
# í•œêµ­ì–´ ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("jojo0217/korean_safe_conversation", split = "train")
# ë°ì´í„°ì…‹ì— í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… í•¨ìˆ˜ ì ìš©
dataset = dataset.map(formatting_prompts_func, batched = True,)
dataset
```

```json
Dataset({
    features: ['instruction', 'output', 'input'],
    num_rows: 26979
})
```

- column: instruction, output, input
- row: 26979

```python
dataset[0]
```

```json
{'instruction': 'ì¹œêµ¬ë“¤ê³¼ ì—¬í–‰ì„ ê°€ë³´ì‹  ì ì´ ìˆë‚˜ìš”? ê°€ë´¤ë‹¤ë©´ ì–´ë””ë¡œ ê°€ë³´ì…¨ë‚˜ìš”?',
 'output': 'ì œê°€ ì—¬í–‰ì„ ê°€ê±°ë‚˜ ê²½í—˜ì„ í•  ìˆ˜ëŠ” ì—†ëŠ” ì¸ê³µ ì§€ëŠ¥ ëª¨ë¸ì´ë¯€ë¡œ, ì‹¤ì œë¡œ ì—¬í–‰ì„ ê°„ ì ì´ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬í–‰ì€ ìƒˆë¡œìš´ ì¥ì†Œë¥¼ íƒí—˜í•˜ê³  ë‹¤ì–‘í•œ ë¬¸í™”ë¥¼ ê²½í—˜í•˜ëŠ” ë©‹ì§„ í™œë™ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì—¬í–‰ì€ ë§ì€ ì‚¬ëŒë“¤ì—ê²Œ í¥ë¯¸ë¡œìš´ ê²½í—˜ê³¼ ì¶”ì–µì„ ì œê³µí•©ë‹ˆë‹¤. ì—¬í–‰ì— ëŒ€í•´ ì–´ë–¤ ì •ë³´ë‚˜ ì¶”ì²œì´ í•„ìš”í•˜ì‹œë‹¤ë©´, ì–¸ì œë“  ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
 'input': ''}
```

- datasetì˜ í˜•ì‹

ğŸ“ **Inputì´ ì—†ëŠ” ì´ìœ **

- ì›ë˜ Alpaca ë°ì´í„°ì…‹ì€ Instruction, input, outputìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒ
- ìƒí˜¸ì‘ìš©(chat, QA) ëª¨ë¸ì—ì„œ ëŒ€ë‹µì„ ì§§ê²Œ í•˜ëŠ” ê²½í–¥ê³¼ ë§¥ë½ì„ ì´í•´í•˜ì§€ ëª»í•˜ëŠ” ê²½í–¥ì„ ê°œì„ í•˜ê¸° ìœ„í•´ instruction-output ì…‹ì„ ì‚¬ìš©
- ë²ˆì—­, í…ìŠ¤íŠ¸ ë³€í™˜ ë“±ì˜ ì‘ì—…ì—ì„œëŠ” input-outputì´ ì‚¬ìš©ë¨

ğŸ·ï¸ **ì°¸ì¡°**

- ğŸ“œ [Instruction tuning : LLMì´ ì‚¬ëŒ ë§ì„ ì•Œì•„ ë“£ëŠ” ë°©ë²•](https://devocean.sk.com/blog/techBoardDetail.do?ID=165806&boardType=techBlog)

ğŸŒˆ **Example: ê¸°ì¡´ì˜ Input í˜•ì‹**

```json
{
  "instruction":"ë‹¤ìŒ ë¬¸ì¥ì— ê°€ì¥ ì í•©í•œ ë¶€ì‚¬ë¥¼ ì°¾ìœ¼ì„¸ìš”.",
  "input":"ê·¸ë…€ëŠ” ì¼í•©ë‹ˆë‹¤",
  "output":"ê·¸ë…€ëŠ” ì—´ì‹¬íˆ ì¼í•©ë‹ˆë‹¤."
},
```

### 3.3 Prompt ìƒì„±

```python
EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
```

- ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” tokenìœ¼ë¡œ ì—†ìœ¼ë©´ ë‹µë³€ì´ ë¬´í•œ ë°˜ë³µë¨
- tokenizerëŠ” pretrain modelì„ ë¶ˆëŸ¬ì˜¬ ë•Œ í•¨ê»˜ ë¶ˆëŸ¬ì˜´

```python
alpaca_prompt = """ì•„ë˜ì—ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ì§€ì‹œ ì‚¬í•­ê³¼ ì¶”ê°€ì ì¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ìˆìŠµë‹ˆë‹¤. ìš”ì²­ì„ ì ì ˆí•˜ê²Œ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.

### instruction:
{}

### input:
{}

### output:
{}"""

def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }
pass
```

- Llama 3.1 fine-tuningì„ ìœ„í•´ datasetì„ promptë¡œ ì¬ê°€ê³µ

```python
dataset = dataset.map(formatting_prompts_func, batched = True,)
dataset[0]
```

```json
{'instruction': 'ì¹œêµ¬ë“¤ê³¼ ì—¬í–‰ì„ ê°€ë³´ì‹  ì ì´ ìˆë‚˜ìš”? ê°€ë´¤ë‹¤ë©´ ì–´ë””ë¡œ ê°€ë³´ì…¨ë‚˜ìš”?',
 'output': 'ì œê°€ ì—¬í–‰ì„ ê°€ê±°ë‚˜ ê²½í—˜ì„ í•  ìˆ˜ëŠ” ì—†ëŠ” ì¸ê³µ ì§€ëŠ¥ ëª¨ë¸ì´ë¯€ë¡œ, ì‹¤ì œë¡œ ì—¬í–‰ì„ ê°„ ì ì´ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬í–‰ì€ ìƒˆë¡œìš´ ì¥ì†Œë¥¼ íƒí—˜í•˜ê³  ë‹¤ì–‘í•œ ë¬¸í™”ë¥¼ ê²½í—˜í•˜ëŠ” ë©‹ì§„ í™œë™ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì—¬í–‰ì€ ë§ì€ ì‚¬ëŒë“¤ì—ê²Œ í¥ë¯¸ë¡œìš´ ê²½í—˜ê³¼ ì¶”ì–µì„ ì œê³µí•©ë‹ˆë‹¤. ì—¬í–‰ì— ëŒ€í•´ ì–´ë–¤ ì •ë³´ë‚˜ ì¶”ì²œì´ í•„ìš”í•˜ì‹œë‹¤ë©´, ì–¸ì œë“  ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
 'input': '',
 'text': 'ì•„ë˜ì—ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ì§€ì‹œ ì‚¬í•­ê³¼ ì¶”ê°€ì ì¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ìˆìŠµë‹ˆë‹¤. ìš”ì²­ì„ ì ì ˆí•˜ê²Œ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.\n\n### instruction:\nì¹œêµ¬ë“¤ê³¼ ì—¬í–‰ì„ ê°€ë³´ì‹  ì ì´ ìˆë‚˜ìš”? ê°€ë´¤ë‹¤ë©´ ì–´ë””ë¡œ ê°€ë³´ì…¨ë‚˜ìš”?\n\n### input:\n\n\n### output:\nì œê°€ ì—¬í–‰ì„ ê°€ê±°ë‚˜ ê²½í—˜ì„ í•  ìˆ˜ëŠ” ì—†ëŠ” ì¸ê³µ ì§€ëŠ¥ ëª¨ë¸ì´ë¯€ë¡œ, ì‹¤ì œë¡œ ì—¬í–‰ì„ ê°„ ì ì´ ì—†ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬í–‰ì€ ìƒˆë¡œìš´ ì¥ì†Œë¥¼ íƒí—˜í•˜ê³  ë‹¤ì–‘í•œ ë¬¸í™”ë¥¼ ê²½í—˜í•˜ëŠ” ë©‹ì§„ í™œë™ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì—¬í–‰ì€ ë§ì€ ì‚¬ëŒë“¤ì—ê²Œ í¥ë¯¸ë¡œìš´ ê²½í—˜ê³¼ ì¶”ì–µì„ ì œê³µí•©ë‹ˆë‹¤. ì—¬í–‰ì— ëŒ€í•´ ì–´ë–¤ ì •ë³´ë‚˜ ì¶”ì²œì´ í•„ìš”í•˜ì‹œë‹¤ë©´, ì–¸ì œë“  ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<|end_of_text|>'}
```

- ë°ì´í„° ì…‹ ì¬ê°€ê³µ ê²°ê³¼



## 4. Training

- ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ Huggingface TRLì˜ SFTTrainerë¥¼ ì‚¬ìš©

ğŸ·ï¸ **ì°¸ì¡°**

- ğŸ“œ [Huggingface-trl](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-tune-an-LLM-Part-3-The-HuggingFace-Trainer--Vmlldzo1OTEyNjMy#instruction-fine-tuning-using-the-trl-library-)
- ğŸ“œ [Huggingface - Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/sft_trainer)

### 4.1 Trainer initialize

```python
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # ì§§ì€ ì‹œí€€ìŠ¤ì˜ ê²½ìš° í•™ìŠµ ì†ë„ë¥¼ 5ë°° ë¹ ë¥´ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    args = TrainingArguments(
        per_device_train_batch_size = 2, # ê° ì¥ì¹˜ë³„ ë°°ì¹˜ í¬ê¸°ë¥¼ 2ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        gradient_accumulation_steps = 4, # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„ë¥¼ 4ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        warmup_steps = 5, # ì›Œë°ì—… ë‹¨ê³„ë¥¼ 5ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        # num_train_epochs = 1, # ì „ì²´ í•™ìŠµì„ ìœ„í•´ ì´ ê°’ì„ 1ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        max_steps = 60, # ìµœëŒ€ ë‹¨ê³„ë¥¼ 60ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        learning_rate = 2e-4, # í•™ìŠµë¥ ì„ 2e-4ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        fp16 = not is_bfloat16_supported(), # bfloat16 ì§€ì› ì—¬ë¶€ì— ë”°ë¼ fp16ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        bf16 = is_bfloat16_supported(), # bfloat16 ì§€ì› ì—¬ë¶€ì— ë”°ë¼ bf16ì„ ì„¤ì •í•©ë‹ˆë‹¤.
        logging_steps = 1, # ë¡œê·¸ ê¸°ë¡ ë‹¨ê³„ë¥¼ 1ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        optim = "adamw_8bit", # ì˜µí‹°ë§ˆì´ì €ë¥¼ adamw_8bitë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        weight_decay = 0.01, # ê°€ì¤‘ì¹˜ ê°ì†Œë¥¼ 0.01ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        lr_scheduler_type = "linear", # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìœ í˜•ì„ linearë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        seed = 3407, # ì‹œë“œë¥¼ 3407ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
        output_dir = "outputs", # ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ "outputs"ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
    ),
)
```

ğŸ“ **SFTTrainer**

- trl(Transformers Reinforcement Learning) ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤
- ì–¸ì–´ ëª¨ë¸ì„ Supervised Fine-Tuning (SFT) ë°©ì‹ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë° ì‚¬ìš©ë¨
- ì£¼ë¡œ ê¸°ì¡´ì˜ ì–¸ì–´ ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…ì— ë§ê²Œ ì§€ë„ í•™ìŠµì„ í†µí•´ íŠœë‹í•˜ëŠ” ê³¼ì •ì—ì„œ í™œìš©

ğŸ“Œ **Parameters**

- *model*
  - ë¯¸ì„¸ ì¡°ì •í•  ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸
  - LoRA í˜¹ì€ ì–‘ìí™”ê°€ ì ìš©ëœ ëª¨ë¸ì„ ê°€ì ¸ì˜¬ ì‹œ, PEFTê°€ ë¨
- *tokenizer*
  - ëª¨ë¸ì— ì í•©í•œ í† í¬ë‚˜ì´ì € (ëª¨ë¸-í† í¬ë‚˜ì´ì € ìŒ)
- *train_dataset*
  - ëª¨ë¸ì„ í›ˆë ¨í•  ë•Œ ì‚¬ìš©í•  ë°ì´í„°ì…‹
  - ë°ì´í„°ì…‹ì€ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í¬ë§·ì´ë©°, ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í…ìŠ¤íŠ¸ í•„ë“œê°€ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•¨
- *dataset_text_field*
  - ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸ í•„ë“œì˜ ì´ë¦„ 
- *max_seq_length*
  - ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
  - ì´ ê¸¸ì´ ì´ìƒì€ ì˜ë¦¬ê±°ë‚˜ íŒ¨ë”© ë¨
- *dataset_num_proc*
  - ë°ì´í„°ì…‹ ì²˜ë¦¬ì— ì‚¬ìš©í•  í”„ë¡œì„¸ìŠ¤ì˜ ìˆ˜
  - ê°’ì´ ë†’ì•„ì§ˆ ìˆ˜ë¡ ë³‘ë ¬ì²˜ë¦¬ë¡œ ìˆ˜í–‰ ì†ë„ í–¥ìƒ
  - CPU, RAMì— ë”°ë¼ì„œ ì¡°ì ˆí•´ì•¼í•¨ (memory ì£¼ì˜)
- *packing*
  - ì—¬ëŸ¬ ì§§ì€ ì‹œí€€ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ íŒ¨í‚¹í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •
  - íŒ¨í‚¹ì„ ì‚¬ìš©í•˜ë©´ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ìµœì í™”í•˜ê³  í•™ìŠµ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒ
- *args*
  - ëª¨ë¸ í›ˆë ¨ ê³¼ì •ì˜ ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì •í•˜ëŠ” í´ë˜ìŠ¤
  - transformer libraryì˜ TrainingArguments ì‚¬ìš©

ğŸŒˆ **Example: Packing**

> max_seq_length: 512

**1. Packing ì ìš© ì „**

```
[ìƒ˜í”Œ A (100 í† í°)] + [íŒ¨ë”© (412 í† í°)]
[ìƒ˜í”Œ B (150 í† í°)] + [íŒ¨ë”© (362 í† í°)]
[ìƒ˜í”Œ C (200 í† í°)] + [íŒ¨ë”© (312 í† í°)]
[ìƒ˜í”Œ D (300 í† í°)] + [íŒ¨ë”© (212 í† í°)]
```

**2. Packing ì ìš©**

```
[ìƒ˜í”Œ A (100 í† í°)] + [ìƒ˜í”Œ B (150 í† í°)]  -> ì´ 250 í† í°
[ìƒ˜í”Œ C (200 í† í°)] + [ìƒ˜í”Œ D (300 í† í°)]  -> ì´ 500 í† í°
```

**3. Packingì˜ ì¥ì **

- íŒ¨ë”©ì— ì‚¬ìš©ë˜ëŠ” ë©”ëª¨ë¦¬ê°€ ì¤„ì–´ë“¤ì–´ GPU ë©”ëª¨ë¦¬ì˜ ì‚¬ìš© íš¨ìœ¨ì´ ë†’ì•„ì§
- ì§§ì€ ì‹œí€€ìŠ¤ë“¤ì„ ê²°í•©í•˜ì—¬ ëª¨ë¸ì´ í•œ ë²ˆì— ë” ë§ì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ í•™ìŠµ ì†ë„ê°€ ë¹¨ë¼ì§
- ëª¨ë¸ì˜ ì„±ëŠ¥ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šìœ¼ë©´ì„œë„ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŒ

**4. ì£¼ì˜ì‚¬í•­**

- packingì„ ì‚¬ìš©í•  ë•ŒëŠ” ì‹œí€€ìŠ¤ ê°„ ê²½ê³„(marker)ë¥¼ ëª…í™•íˆ ì²˜ë¦¬í•´ì•¼ í•¨
- ì¦‰, ê° ìƒ˜í”Œì´ ê²°í•©ëœ ì‹œí€€ìŠ¤ ë‚´ì—ì„œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ” ìœ„ì¹˜ë¥¼ ëª¨ë¸ì´ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í•´ì•¼í•¨
- ê²½ê³„ë¥¼ ì˜ ì²˜ë¦¬í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë¸ì´ ì˜ëª»ëœ ë¬¸ë§¥ì„ í•™ìŠµí•  ìˆ˜ ìˆìŒ

ğŸ“ **TrainingArguments**

- HuggingFaceì˜ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤
- ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œ í•„ìš”í•œ ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •ì„ ê´€ë¦¬

ğŸ“Œ **Parameters**

- *per_device_train_batch_size*
  - ê° GPU/TPU ì¥ì¹˜ì—ì„œ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°
  - ë°°ì¹˜ í¬ê¸°ê°€ í¬ë©´ í•™ìŠµì´ ë¹ ë¥¼ ìˆ˜ ìˆì§€ë§Œ ë” ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ê²Œ ë¨
- *gradient_accumulation_steps*
  - ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸ ì „ì— ëˆ„ì í•  ìŠ¤í… ìˆ˜
  - ì˜ˆì‹œ: gradient_accumulation_steps = 4ëŠ” 4ê°œì˜ ìŠ¤í… ë™ì•ˆ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëˆ„ì í•œ í›„ì— í•œ ë²ˆì˜ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰
  - ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ì§€ ì•Šê³ ë„ í° ë°°ì¹˜ íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆì§€ë§Œ, ì´ ê²½ìš° í•™ìŠµ ì‹œê°„ì´ ëŠ˜ì–´ë‚  ìˆ˜ ìˆìŒ
- *warmup_steps*
  - í•™ìŠµ ì´ˆê¸° ë‹¨ê³„ì—ì„œ learning_rateë¥¼ ì ì§„ì ìœ¼ë¡œ ì¦ê°€ì‹œí‚¤ëŠ” ì›Œë°ì—… ë‹¨ê³„ì˜ ìˆ˜
  - ì˜ˆì‹œ: warmup_steps = 5ëŠ” ì´ˆê¸° 5 ìŠ¤í… ë™ì•ˆ í•™ìŠµë¥ ì„ ì„œì„œíˆ ë†’ì„
  - ëª¨ë¸ì´ ì´ˆê¸° í•™ìŠµë¥ ë¡œ ì¸í•œ ë¶ˆì•ˆì •í•œ í•™ìŠµì„ ë°©ì§€í•˜ê³ , ë” ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆê²Œ í•¨
- *max_steps*
  - í•™ìŠµí•  ìµœëŒ€ ìŠ¤í… ìˆ˜ë¡œ ì´ ê°’ì— ë„ë‹¬í•˜ë©´ í•™ìŠµì´ ì¢…ë£Œë¨
- *fp16*
  - 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì (half-precision) ì—°ì‚°ì„ ì‚¬ìš©í• ì§€ë¥¼ ê²°ì •
- *bf16*
  - bfloat16 í˜•ì‹ì„ ì‚¬ìš©í• ì§€ë¥¼ ê²°ì •
- *weight_decay*
  - ê°€ì¤‘ì¹˜ ê°ì†Œ(L2 ì •ê·œí™”)ë¥¼ ì ìš©í•  ë¹„ìœ¨
  - ê°€ì¤‘ì¹˜ ê°ì†ŒëŠ” ê³¼ì í•©ì„ ë°©ì§€í•˜ê³ , ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒ
- *lr_scheduler_type*
  - learning_rate ìŠ¤ì¼€ì¤„ëŸ¬ì˜ ìœ í˜•
  - ì˜ˆì‹œ: "linear"ì€ í•™ìŠµë¥ ì´ ì„ í˜•ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ìŠ¤ì¼€ì¤„ëŸ¬
- *output_dir*
  - í•™ìŠµ ê²°ê³¼(ëª¨ë¸, ë¡œê·¸ íŒŒì¼ ë“±)ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬

âœï¸ **ë°°ì¹˜ í¬ê¸°ì™€ ìˆ˜í–‰ì†ë„, ì„±ëŠ¥ì˜ ê´€ê³„**

- ë©”ëª¨ë¦¬ì— í•œê³„ê°€ ìˆê¸° ë•Œë¬¸ì— ë°°ì¹˜ë¥¼ ì‘ê²Œ ì¤„ì—¬ì„œ í•™ìŠµì„ ìˆ˜í–‰í•¨
- ì‘ì€ ë°°ì¹˜ í¬ê¸°ëŠ” í•™ìŠµì˜ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒ
- ì¼ë°˜ì ìœ¼ë¡œ í° ë°°ì¹˜ í¬ê¸°ê°€ ì•ˆì •ì ì´ê³  ë¹ ë¥¸ ìˆ˜ë ´ì„ ìœ ë„í•¨

âœï¸ **Epoch, Batch, Step**

- Epoch: í•™ìŠµ ë°ì´í„°ì…‹ ì „ì²´ë¥¼ í•œ ë²ˆ ìˆœíšŒ (Datasize/ Batchsize)
  - ex) ë°ì´í„°ì…‹ì— 10,000ê°œì˜ ìƒ˜í”Œì´ ìˆê³ , ë°°ì¹˜ í¬ê¸°ê°€ 100ì´ë¼ë©´, 1 ì—í¬í¬ëŠ” 100ê°œì˜ ë°°ì¹˜ë¡œ ì´ë£¨ì–´ì§
- Batch: í•™ìŠµ ì¤‘ í•œ ë²ˆì— ëª¨ë¸ì— ì…ë ¥ë˜ëŠ” ë°ì´í„° ìƒ˜í”Œì˜ ë¬¶ìŒ
  - ex, ë°°ì¹˜ í¬ê¸°ê°€ 32ì¸ ê²½ìš°, ê° í•™ìŠµ ìŠ¤í…(step)ì—ì„œ 32ê°œì˜ ë°ì´í„° ìƒ˜í”Œì´ ëª¨ë¸ì— ì…ë ¥
- Step: í•œ ë²ˆì˜ ë°°ì¹˜ì— ëŒ€í•´ ì—°ì‚° í›„ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ìˆ˜í–‰

ğŸŒˆ **Example: Epochì™€ Max_step**

```
# í•™ìŠµ ì„¤ì •
â€¢	Dataset Size: 10,000 ìƒ˜í”Œ
â€¢ Batch Size: 100 ìƒ˜í”Œ
â€¢	Epoch Num: 5
â€¢	Max_step: 200
```

- 1 Epochì—ëŠ” 100ê°œì˜ batch í•„ìš”
- 1 Epochì—ëŠ” 100ë²ˆì˜ step í•„ìš”
- 5 Epochë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ 500 step í•„ìš”
- Max_Stepì´ 200ì´ë¯€ë¡œ ëª¨ë¸ì€ 2 Epochì™€ 100 step ì´í›„ í•™ìŠµì´ ì¢…ë£Œë¨

âœï¸ **Mixed Precision Training**

> ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµì—ì„œ ì„±ëŠ¥ì„ ìµœì í™”í•˜ê¸° ìœ„í•´ 32ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì (FP32)ê³¼ 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì (FP16, ë˜ëŠ” half-precision) ì—°ì‚°ì„ í˜¼í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•

- ëª¨ë¸ì´ ë” ì ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ì„œë„ ë” ë¹ ë¥´ê²Œ í›ˆë ¨ë˜ë„ë¡ í•¨

âœï¸ **bfloat16**

> bfloat16(Brain Floating Point 16)ì€ 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ìˆ«ì í˜•ì‹ìœ¼ë¡œ, FP32ì™€ ê°™ì€ ë²”ìœ„ë¥¼ ì œê³µí•˜ë©´ì„œë„ FP32ë³´ë‹¤ ë©”ëª¨ë¦¬ì™€ ê³„ì‚°ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ” í˜•ì‹

- FP32ì— ë¹„í•´ í‘œí˜„ ì •ë°€ë„ëŠ” ë‚®ì§€ë§Œ, ìˆ«ì ë²”ìœ„ëŠ” ê±°ì˜ ë™ì¼í•˜ê²Œ ìœ ì§€í•  ìˆ˜ ìˆìŒ
- FP16ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, ë” ë†’ì€ ì •ë°€ë„ë¥¼ ì œê³µ

ğŸ“ **is_bfloat16_supported**

- ëª¨ë¸ í›ˆë ¨ ì‹œ ì‚¬ìš©ë˜ëŠ” í•˜ë“œì›¨ì–´ë‚˜ í”„ë ˆì„ì›Œí¬ê°€ bfloat16ì„ ì§€ì›í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ í™•ì¸
- Googleì˜ TPUë‚˜ ìµœì‹  NVIDIA GPU(Ampere ì•„í‚¤í…ì²˜ ì´ìƒ)ì—ì„œëŠ” bfloat16ì„ ì§€ì›

### 4.2 Memory check

```python
#@title í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ í‘œì‹œ
# torch.cuda.get_device_properties(0)ì„ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ GPUì˜ ì†ì„±ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
gpu_stats = torch.cuda.get_device_properties(0)

# torch.cuda.max_memory_reserved()ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ì˜ˆì•½ëœ ìµœëŒ€ GPU ë©”ëª¨ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê³ ,
# ì´ë¥¼ ê¸°ê°€ë°”ì´íŠ¸(GB) ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì—¬ ë°˜ì˜¬ë¦¼í•©ë‹ˆë‹¤.
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)

# gpu_stats.total_memoryë¥¼ ì‚¬ìš©í•˜ì—¬ GPUì˜ ì´ ë©”ëª¨ë¦¬ë¥¼ ê°€ì ¸ì˜¤ê³ ,
# ì´ë¥¼ ê¸°ê°€ë°”ì´íŠ¸(GB) ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì—¬ ë°˜ì˜¬ë¦¼í•©ë‹ˆë‹¤.
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)

# GPU ì´ë¦„ê³¼ ì´ ë©”ëª¨ë¦¬ í¬ê¸°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")

# í˜„ì¬ ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ í¬ê¸°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
print(f"{start_gpu_memory} GB of memory reserved.")
```

### 4.3 Train

```python
trainer_stats = trainer.train()
```

- ëª¨ë¸ í•™ìŠµ ì‹œì‘

### 4.4 Memory Evaluation

```python
#@title ìµœì¢… ë©”ëª¨ë¦¬ ë° ì‹œê°„ í†µê³„ í‘œì‹œ
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory         /max_memory*100, 3)
lora_percentage = round(used_memory_for_lora/max_memory*100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.")
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")
```

## 5. Inference

```python
# alpaca_prompt = Copied from above
FastLanguageModel.for_inference(model) # Enable native 2x faster inference
inputs = tokenizer(
[
    alpaca_prompt.format(
        "ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ë‹µí•´ì£¼ì„¸ìš”", # instruction
        "ì‚°ì‚¬íƒœë¥¼ ë‚´ëŠ” ë°©ë²• ì•Œë ¤ì¤˜", # input
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

# í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë¨¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer)
_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)
```

```
<|begin_of_text|>ì•„ë˜ì—ëŠ” ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” ì§€ì‹œ ì‚¬í•­ê³¼ ì¶”ê°€ì ì¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ì…ë ¥ì´ ìˆìŠµë‹ˆë‹¤. ìš”ì²­ì„ ì ì ˆí•˜ê²Œ ì™„ë£Œí•˜ëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”.

### instruction:
ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ë‹µí•´ì£¼ì„¸ìš”

### input:
ì‚°ì‚¬íƒœë¥¼ ë‚´ëŠ” ë°©ë²• ì•Œë ¤ì¤˜

### output:
ì‚°ì‚¬íƒœë¥¼ ë‚´ëŠ” ê²ƒì€ ë§¤ìš° ìœ„í—˜í•œ í–‰ë™ì´ë©°, ë²•ì ìœ¼ë¡œ ê¸ˆì§€ëœ í–‰ìœ„ì…ë‹ˆë‹¤. ì‚°ì‚¬íƒœëŠ” ì§€êµ¬ì˜ ìì—°ì ì¸ í˜„ìƒì´ê¸° ë•Œë¬¸ì— ì¸ê°„ì´ ì§ì ‘ì ìœ¼ë¡œ ì¡°ì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‚°ì‚¬íƒœê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ” ìœ„í—˜í•œ ìƒí™©ì„ í”¼í•˜ê¸° ìœ„í•´ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì§€ì§„ì´ë‚˜ ê°•í•œ ë¹„ë‚˜ ëˆˆì´ ë‚´ë¦´ ë•ŒëŠ” ì•ˆì „í•œ ê³³ìœ¼ë¡œ ì´ë™í•˜ê³ , ì‚°ì‚¬íƒœê°€ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì§€ì—­ì—ì„œëŠ” ì•ˆì „í•œ ê±°ë¦¬ë¥¼ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.<|end_of_text|>
```



## 6. Inference

ğŸ’¥**ê¸°ì¡´ Llama 3.1 8B ëª¨ë¸ê³¼ ë¹„êµ**

- Fine tuningì˜ ëŒ€ìƒ ë°ì´í„° ì…‹ì€ ë¶€ì •ì ì¸ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ì„ ìš°íšŒí•˜ëŠ” Instruct-output Setë¥¼ ê°€ì§€ê³  ìˆìŒ
- ë¶€ì •ì–´ë¥¼ ì§ˆë¬¸í•˜ì—¬ ì°¨ì´ë¥¼ í™•ì¸

**ë¶€ì •ì–´ ì§ˆë¬¸1-Llama 3.1**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/llm/org_result1.png?raw=true" width="1000" height="200"></p>

**ë¶€ì •ì–´ ì§ˆë¬¸1-PEFT**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/llm/peft_result1.png?raw=true" width="1000" height="400"></p>

**ë¶€ì •ì–´ ì§ˆë¬¸2-Llama 3.1**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/llm/org_result2.png?raw=true" width="1000" height="300"></p>

**ë¶€ì •ì–´ ì§ˆë¬¸2-PEFT**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/llm/peft_result2.png?raw=true" width="1000" height="400"></p>

## 7. Conclusion

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Open ëª¨ë¸ì¸ llama 3.1 8Bì— ëŒ€í•´ í•œêµ­ì–´ ë°ì´í„° ì…‹ìœ¼ë¡œ PEFT í›„ ê²°ê³¼ë¥¼ í™•ì¸í•´ ë³´ì•˜ë‹¤. ê¸°ì¡´ Llama 3.1 8B ëª¨ë¸ì´ í•œêµ­ì–´ ì¶”ë¡  ì„±ëŠ¥ì´ ê´œì°®ì•˜ì§€ë§Œ ì¶”ë¡  ì˜ˆì‹œì™€ ê°™ì´ ë¶€ì ì ˆí•œ ëŒ€ë‹µì„ í•˜ëŠ” ê²½ìš°ë„ ì¢…ì¢… ìˆì—ˆë‹¤. PEFTë¡œ í•™ìŠµì‹œí‚¨ ê²°ê³¼ ë˜í•œ ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•œ ë‹µë³€ì„ ì£¼ì§€ëŠ” ì•Šì§€ë§Œ, ë¶€ì •ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ì²˜ë¦¬ëŠ” í™•ì—°íˆ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„ì„ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. ì•„ë¬´ë˜ë„ ì‘ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì˜€ê¸° ë•Œë¬¸ì— ì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ í•œê³„ê°€ í™•ì‹¤íˆ ì¡´ì¬í•˜ëŠ” ê²ƒ ê°™ë‹¤. Specificí•œ ë°ì´í„° ì…‹ì„ êµ¬ë¹„í•  ìˆ˜ ìˆë‹¤ë©´ ë” ë‚˜ì€ ì‹¤í—˜ì„ í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

ğŸ’¡ **ë” ì•Œì•„ë³¼ ê²ƒ**

- Local LLMì„ ìœ„í•œ GPUì„±ëŠ¥ì´ ì–´ëŠì •ë„ ë ì§€ ê°€ëŠ ì´ ë˜ì§€ ì•Šê¸°ì— ì´ë¡  ê³µë¶€ì™€ ì‹¤í—˜ì´ í•„ìš”í•˜ë‹¤.

