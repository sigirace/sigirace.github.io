---
layout: single
title:  'LM(5) Sequence to Sequence êµ¬í˜„'
toc: true
categories: [Language Model]
tags: [LM]

---

ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” Pytorchë¡œ seq2seq ì˜í•œ ë²ˆì—­ê¸°ë¥¼ êµ¬í˜„í•´ë³¸ë‹¤.
{: .notice}

## 1. Prepare

### 1.1 Mecab ì„¤ì¹˜

ì´ë²ˆ ì‹¤ìŠµì€ colab í™˜ê²½ì—ì„œ ìˆ˜í–‰í•˜ë©°, í•œêµ­ì–´ tokenizingì„ ìœ„í•œ mecab ì„¤ì¹˜ ì½”ë“œëŠ” [ì—¬ê¸°]()ì—ì„œ ë³µë¶™í•´ì˜¨ë‹¤.

### 1.2 Import module

ì„¤ì¹˜ê°€ ëë‚¬ë‹¤ë©´ ì•„ë˜ ëª¨ë“ˆë“¤ì„ import ì‹œí‚¨ë‹¤. ë§Œì•½ wandbë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ wandb ê´€ë ¨ ì½”ë“œë¥¼ ëª¨ë‘ ì‚­ì œí•´ë„ ë¬´ë°©í•˜ë‹¤.

````python
import os

import wandb
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence
from torchtext.data.metrics import bleu_score
from sklearn.model_selection import train_test_split

from collections import Counter
import random
import pandas as pd

import re
import nltk
from konlpy.tag import Mecab
nltk.download('punkt')

os.environ["WANDB_API_KEY"] = 'your api key'
os.environ['WANDB_SILENT']="true"
````



## 2. Dataset

### 2.1 ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°

ë²ˆì—­ê¸° ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” í›ˆë ¨ ë°ì´í„°ë¡œ ë³‘ë ¬ ì½”í¼ìŠ¤(parallel corpus)ê°€ í•„ìš”í•˜ë‹¤.

ğŸ“**ë³‘ë ¬ ì½”í¼ìŠ¤(parallel corpus)ë€?**

> ë‘ ê°œ ì´ìƒì˜ ì–¸ì–´ê°€ ë³‘ë ¬ì ìœ¼ë¡œ êµ¬ì„±ëœ ì½”í¼ìŠ¤ë¥¼ ì˜ë¯¸í•œë‹¤.<br>ex) ì˜ì–´-í•œêµ­ì–´/ í”„ë‘ìŠ¤ì–´-ì˜ì–´ ..

ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ì˜ì–´-í•œêµ­ì–´ ë³‘ë ¬ ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•  ê²ƒì´ë©°, [ì—¬ê¸°](http://www.manythings.org/anki)ì—ì„œ kor-eng.zip íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ ë°›ì„ ìˆ˜ ìˆë‹¤. ë‹¤ìš´ë¡œë“œ ë°›ì€ íŒŒì¼ì„ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì €ì¥í•˜ê³ , ì½”ë©ì„ í†µí•´ í™•ì¸í•´ë³¸ë‹¤.

````python
data_path = 'your data path'

# data load
df = pd.read_csv(data_path, delimiter='\t', names=['src', 'trg', 'etc'])[['src', 'trg']]
print('ì „ì²´ ë°ì´í„° ê°œìˆ˜ :',len(df))
````

````
ì „ì²´ ë°ì´í„° ê°œìˆ˜ : 5749
````

ì „ì²´ ì½”í¼ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì „ì²´ ë³‘ë ¬ ë°ì´í„° ìŒì€ ì´ 5749ê°œì´ë‹¤. êµ¬ì„±ì„ í™•ì¸í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

````
df.head()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/seq2seq/seq_imp1.png?raw=true" width="150"></p>

### 2.2 ë°ì´í„° ì „ì²˜ë¦¬

ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜ë¥¼ ì‘ì„± ë° ìˆ˜í–‰í•œë‹¤. ì „ì²˜ë¦¬ëŠ” ì •ê·œí‘œí˜„ì‹ì„ í†µí•´ íŠ¹ìˆ˜ë¬¸ìì™€ ê³µë°±ì„ ì œê±°í•˜ê³  ê° ë¬¸ì¥ì— ëŒ€í•´ í† í¬ë‚˜ì´ì§•ì„ ìˆ˜í–‰í•˜ë©°, ì˜ì–´ì˜ ê²½ìš° nltk, í•œêµ­ì–´ì˜ ê²½ìš° mecabì„ ì‚¬ìš©í•œë‹¤.

````python
def preprocess(text, text_type):
  pattern = "[^\w\s]+" # \w: word character (a-z, A-Z, 0-9), \s: whitespace character
  result = re.sub(pattern, "", text)
  if text_type == 'eng':
    tokens = nltk.word_tokenize(result)
  else:
    mecab = Mecab()
    tokens = mecab.morphs(result)
  return tokens
````

````python
df['src'] = df['src'].apply(lambda x: preprocess(x, 'eng'))
df['trg'] = df['trg'].apply(lambda x: preprocess(x, 'kor'))
df.tail()
````

ìˆ˜í–‰í•œ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ì „ì²˜ë¦¬ê°€ ì•„ë‹Œ ëª¨ë¸ì— ì§‘ì¤‘í•  ê²ƒì´ê¸°ì— ë‹¨ìˆœí•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì˜€ë‹¤.

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/seq2seq/seq_imp2.png?raw=true" width="700"></p>

### 2.3 ë°ì´í„° ì…‹ ë¶„ë¦¬

ë³´ìœ í•œ ë¬¸ì¥ì´ ë§ì§€ ì•Šê¸°ì—, ë°ì´í„° ì…‹ì€ ì•½ 7:2:1 ì •ë„ë¡œ ë¶„ë¦¬í•œë‹¤.

````python
train_val, test = train_test_split(df, test_size=0.1, random_state=42)
train, val = train_test_split(df, test_size=0.22, random_state=42)

# ì¸ë±ìŠ¤ ì •ë ¬
train.reset_index(drop=True, inplace=True)
val.reset_index(drop=True, inplace=True)
test.reset_index(drop=True, inplace=True)

print("Train set size:", len(train))
print("Validation set size:", len(val))
print("Test set size:", len(test))
````

````
Train set size: 4484
Validation set size: 1265
Test set size: 575
````

### 2.4 ë‹¨ì–´ ì§‘í•© ìƒì„±

ë‹¨ì–´ ì§‘í•©, ë‹¨ì–´ì¥ì„ ë§Œë“œëŠ” ì‘ì—…ì€ ë¬¸ìë¥¼ ì¸ë±ìŠ¤í™” ì‹œì¼œ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ì‘ì—…ìœ¼ë¡œ, ì˜ì–´ì™€ í•œêµ­ì–´ ë¬¸ì¥ ê°ê° ìˆ˜í–‰í•˜ì—¬ì•¼ í•œë‹¤. ë‹¨ì–´ì¥ì—ëŠ” ë¬¸ì¥ ì‹œì‘, ë, ë¹ˆì¹¸, ëª¨ë¦„ì„ ëœ»í•˜ëŠ” í† í°ì„ ê¸°ë³¸ì ìœ¼ë¡œ ì¶”ê°€í•˜ê³ , ì´í›„ ë¬¸ì¥ì—ì„œ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ì˜ ë¹ˆë„ ìˆœì„œëŒ€ë¡œ ë°°ì—´ì— ì €ì¥í•œë‹¤. ì´í›„ ë°°ì—´ì˜ ì¸ë±ìŠ¤ì™€ ë‹¨ì–´ë¥¼ ë§¤í•‘í•˜ëŠ” ë‹¨ê³„ë¥¼ ê±°ì³ ê° ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ ë° ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë”•ì…”ë„ˆë¦¬ë„ ìƒì„±í•œë‹¤.

ğŸ“ **Example**

> ë‹¨ì–´ ì§‘í•©ì˜ 0ë²ˆ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ëŠ” `<sos>` í† í°ì´ê³ , 1ë²ˆì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ëŠ” `<eos>`...

````python
def build_vocab(sentence_list):
  word_counts = Counter()
  for words in sentence_list:
      for word in words:
          word_counts[word] += 1
  # ë¹ˆë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
  vocab = ['<sos>', '<eos>', '<pad>', '<unk>'] + [word for word, _ in word_counts.most_common()]

  # ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ë¥¼ ë§¤í•‘í•´ì£¼ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
  word_to_idx = {word: idx for idx, word in enumerate(vocab)}
  idx_to_word = {idx: word for idx, word in enumerate(vocab)}

  return word_to_idx, idx_to_word, vocab
````

````python
eng_word_to_idx, eng_idx_to_word, eng_vocab = build_vocab(train['src'])
kor_word_to_idx, kor_idx_to_word, kor_vocab = build_vocab(train['trg'])
````

ì‹¤ì œ í•™ìŠµì€ train dataë¡œë§Œ ìˆ˜í–‰ë  ê²ƒì´ê¸° ë•Œë¬¸ì— train dataì˜ ë¬¸ì¥ìœ¼ë¡œë§Œ ë‹¨ì–´ ì§‘í•©ì„ ë§Œë“ ë‹¤. ì´í›„ ê° ë°ì´í„° ì…‹ì— ì‹œì‘ê³¼ ëì— í•´ë‹¹í•˜ëŠ” í† í°ì„ ë¶™ì—¬ì¤€ë‹¤.

````python
train['src'] = train['src'].apply(lambda x: ['<sos>'] + x + ['<eos>'])
train['trg'] = train['trg'].apply(lambda x: ['<sos>'] + x + ['<eos>'])

val['src'] = val['src'].apply(lambda x: ['<sos>'] + x + ['<eos>'])
val['trg'] = val['trg'].apply(lambda x: ['<sos>'] + x + ['<eos>'])

test['src'] = test['src'].apply(lambda x: ['<sos>'] + x + ['<eos>'])
test['trg'] = test['trg'].apply(lambda x: ['<sos>'] + x + ['<eos>'])
````

### 2.5 Padding

ìš°ë¦¬ê°€ êµ¬ì„±í•œ ë°ì´í„° ì…‹ì˜ ê° ë¬¸ì¥ì˜ ê¸¸ì´ëŠ” ëª¨ë‘ ë‹¤ë¥´ë‹¤. ë”°ë¼ì„œ í† í°ë‚˜ì´ì§•ì„ ê±°ì³ ìƒì„±ëœ ë°°ì—´ë„ ëª¨ë‘ ë‹¤ë¥¸ ê¸¸ì´ë¥¼ ê°€ì§€ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê¸¸ì´ê°€ ë‹¤ë¥¸ ê²ƒì€ rnn êµ¬ì¡°ì—ì„œ ë¬¸ì œê°€ ë˜ì§€ ì•Šìœ¼ë‚˜, ë°°ì¹˜ë‹¨ìœ„ì˜ í•™ìŠµì‹œ ë¬¸ì œê°€ ëœë‹¤. ë§Œì•½ batch sizeë¥¼ 3ì´ë¼ê³  ê°€ì •í•˜ê³  ë°°ì—´ ê·¸ëŒ€ë¡œ ë°ì´í„° ì…‹ì„ êµ¬ì„±í•œë‹¤ë©´ ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œê°€ ë°œìƒí•œë‹¤.

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/seq2seq/seq_imp3.png?raw=true" width="300"></p>

ì´ëŠ” ëª¨ë¸(RNN)ì„ êµ¬ì„±ì‹œ ë¯¸ë¦¬ ë‚´ë¶€ Input sizeë¥¼ ì •í•´ì£¼ê¸° ë•Œë¬¸ì— ë°œìƒí•œë‹¤. ìœ„ì˜ ê²½ìš° ê° time-stepë§ˆë‹¤ ëª¨ë¸ì€ 3ê°œì˜ ì¸í’‹ì„ ë°›ê¸°ë¡œ ë˜ì–´ìˆìœ¼ë‚˜ 6, 7ë²ˆì§¸ì˜ stepì—ì„œëŠ” ê·¸ë ‡ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ê° ë°°ì¹˜ë§ˆë‹¤ ëª¨ë‘ ê°™ì€ ê¸¸ì´ë¡œ ë§ì¶°ì£¼ê¸° ìœ„í•´ ì§§ì€ ë¬¸ì¥ì€ `<pad>` tokenìœ¼ë¡œ ì±„ì›Œì£¼ëŠ” í•¨ìˆ˜ê°€ í•„ìš”í•˜ë‹¤. ë˜í•œ ìš°ë¦¬ëŠ” ì•ì„œ ë‹¨ì–´ ì§‘í•©ì„ train ë‹¨ìœ„ë¡œë§Œ êµ¬ì„±í•˜ì˜€ë‹¤. ë”°ë¼ì„œ trainì— ë“±ì¥í•˜ì§€ ì•Šì€ ë‹¨ì–´ê°€ val ë° test datasetì— ë“±ì¥í•  ìˆ˜ ìˆê¸°ì— ëª¨ë¥´ëŠ” ë‹¨ì–´ë¥¼ `<unk>` tokenìœ¼ë¡œ ì¹˜í™˜í•˜ì—¬ì•¼ í•œë‹¤. ë§Œì•½ ì´ë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ë‹¨ì–´ ì§‘í•©ì— ì—†ëŠ” ë‹¨ì–´ê°€ ë‚˜ì™”ì„ ë•Œ ì´ë¥¼ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ê²ƒì´ë‹¤.

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/seq2seq/seq_imp4.png?raw=true" width="300"></p>

````python
def collate_fn(batch):
    # ì˜ì–´ì™€ í•œêµ­ì–´ ë¬¸ì¥ì„ ë°°ì¹˜ì—ì„œ ê°ê° ì¶”ì¶œ
    english_sentences = [row['src'] for row in batch]
    korean_sentences = [row['trg'] for row in batch]
    
    # ì˜ì–´ì™€ í•œêµ­ì–´ ë¬¸ì¥ì„ ì •ìˆ˜ë¡œ ë³€í™˜
    english_seqs = [torch.LongTensor([eng_word_to_idx.get(word, eng_word_to_idx['<unk>']) for word in row]) for row in english_sentences]
    korean_seqs = [torch.LongTensor([kor_word_to_idx.get(word, eng_word_to_idx['<unk>']) for word in row]) for row in korean_sentences]

    # ìµœëŒ€ê¸¸ì´ 100ìœ¼ë¡œ íŒ¨ë”©
    english_padded = pad_sequence(english_seqs, batch_first=True, padding_value=0)
    korean_padded = pad_sequence(korean_seqs, batch_first=True, padding_value=0)
    
    max_len = min(100, english_padded.shape[1])
    english_padded = english_padded[:, :max_len]

    max_len = min(100, korean_padded.shape[1])
    korean_padded = korean_padded[:, :max_len]

    return {'english': english_padded, 'korean': korean_padded}
````

ìš°ë¦¬ê°€ í˜„ì¬ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ëŠ” dataframe í˜•íƒœì´ë‹¤. ì´ë¥¼ pytorchê°€ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë°°ì¹˜ ë‹¨ìœ„ì˜ ë°ì´í„°ë¡œ êµ¬ì„±í•˜ê¸° ìœ„í•´ì„  DataLoaderë¥¼ ì ìš©í•´ì•¼ í•˜ë©°, ì ìš©ì‹œ ì•ì„œ ì‘ì„±í–ˆë˜ padding ë° unknownì— ê´€í•œ í•¨ìˆ˜ë¥¼ parameterë¡œ ë„£ê²Œ ë˜ë©´ ì´ë¥¼ ë°°ì¹˜ë‹¨ìœ„ë¡œ ìˆ˜í–‰í•˜ê²Œ ëœë‹¤.

````python
# train ë°ì´í„°ì…‹ê³¼ ë°ì´í„°ë¡œë” ìƒì„±
train_dataset = train.to_dict('records')
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)

# valid ë°ì´í„°ì…‹ê³¼ ë°ì´í„°ë¡œë” ìƒì„±
val_dataset = train.to_dict('records')
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)

# test ë°ì´í„°ì…‹ê³¼ ë°ì´í„°ë¡œë” ìƒì„±
test_dataset = train.to_dict('records')
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)
````



## 3. Model

ëª¨ë¸ì€ Encoder, Decoder ê·¸ë¦¬ê³  ì´ë¥¼ í•©ì¹œ Seq2sqìœ¼ë¡œ êµ¬ì„±í•  ê²ƒì´ë©° ë‚´ë¶€ layerëŠ” lstmì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.

### 3.1 Encoder

````python
class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        
        self. hid_dim = hid_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src):
        # src = [batch_size, src len]
        embedded = self.dropout(self.embedding(src))
        
        # embedded = [batch_size, src len, emb dim]
        outputs, (hidden, cell) = self.rnn(embedded)
        
        # outputs = [batch size, src len, hid dim]
        # hidden = [batch size, n layers, hid dim]
        # cell = [batch size, n layers, hid dim]
        
        return hidden, cell
````

<br>

### 3.2 Decoder

````python
class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        
        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)
        self.fc_out = nn.Linear(hid_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input, hidden, cell):
        # input = [batch_size]
        # hidden = [batch_size, n layers, hid dim]
        # cell = [batch_size, n layers, hid dim]
        # context = [n layers, batch size, hid dim]
        
        # input = [1, batch_size]
        input = input.unsqueeze(0)
        
        # embedded = [batch_size, 1, emb dim]
        embedded = self.dropout(self.embedding(input)).permute(1,0,2).contiguous()
        
        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
        
        # output = [batch size, seq len, hid dim]
        # hidden = [batch size, n layers, hid dim]
        # cell = [batch size, n layers, hid dim]
        
        # output = [batch size, 1, hid dim]
        # hidden = [batch size, n layers, hid dim]
        # cell = [batch size, n layers, hid dim]
        
        # prediction = [batch size, output dim]
        prediction = self.fc_out(output.squeeze(1))
        
        return prediction, hidden, cell
````

### 3.3 Seq2Seq

````python
class Seq2Seq(nn.Module):
   def __init__(self, encoder, decoder):
       super().__init__()
       
       self.encoder = encoder
       self.decoder = decoder
       
       # Encoderì™€ Decoderì˜ hid_dimê³¼ n_layerê°€ ê°™ì•„ì•¼ í•¨
       # Encoderì˜ ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœê°€ Decoderì˜ ì´ˆê¸° ì€ë‹‰ìƒíƒœë¡œ ì“°ì´ê¸° ë•Œë¬¸
       assert encoder.hid_dim == decoder.hid_dim
       assert encoder.n_layers == decoder.n_layers
       
   def forward(self, src, trg, teacher_forcing_ratio=0.5):
       # src = [batch size, src len]
       # trg = [batch size, trg len]
       trg_len = trg.shape[1]
       batch_size = trg.shape[0]
       trg_vocab_size = self.decoder.output_dim
       
       # decoder ê²°ê³¼ë¥¼ ì €ì¥í•  í…ì„œ
       outputs = torch.zeros(batch_size, trg_len, trg_vocab_size)
       
       # Encoderì˜ ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœê°€ Decoderì˜ ì´ˆê¸° ì€ë‹‰ìƒíƒœë¡œ ì“°ì„
       hidden, cell = self.encoder(src)
       
       # Decoderì— ë“¤ì–´ê°ˆ ì²« inputì€ <sos> í† í°
       input = trg[:, 0]
       
       # target lengthë§Œí¼ ë°˜ë³µ
       # range(0,trg_len)ì´ ì•„ë‹ˆë¼ range(1,trg_len)ì¸ ì´ìœ  : 0ë²ˆì§¸ trgëŠ” í•­ìƒ <sos>ë¼ì„œ ê·¸ì— ëŒ€í•œ outputë„ í•­ìƒ 0 
       for t in range(1, trg_len):
           
           output, hidden, cell = self.decoder(input, hidden, cell)
           outputs[:,t] = output
           
           # random.random() : [0,1] ì‚¬ì´ ëœë¤í•œ ìˆ«ì 
           # ëœë¤ ìˆ«ìê°€ teacher_forcing_ratioë³´ë‹¤ ì‘ìœ¼ë©´ Trueë‹ˆê¹Œ teacher_force=1
           teacher_force = random.random() < teacher_forcing_ratio
           
           # í™•ë¥  ê°€ì¥ ë†’ê²Œ ì˜ˆì¸¡í•œ í† í°
           top1 = output.argmax(1) 
           
           # techer_force = 1 = Trueì´ë©´ trg[t]ë¥¼ ì•„ë‹ˆë©´ top1ì„ inputìœ¼ë¡œ ì‚¬ìš©
           input = trg[:,t] if teacher_force else top1
       
       return outputs
````

### 3.4 Parameter ì„¸íŒ…

ì´ë²ˆ ì‹¤ìŠµì‹œ ì‚¬ìš©í•  parameterëŠ” ì•„ë˜ì™€ ê°™ë‹¤. ì´ëŠ” ë³¸ì¸ ì…ë§›ì— ë§ì¶”ì–´ ë°”ê¿”ë„ ë¬´ë°©í•˜ë‹¤.

````python
# device setting
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

input_dim = len(eng_vocab)
output_dim = len(kor_vocab)

# Encoder embedding dim
enc_emb_dim = 256
# Decoder embedding dim
dec_emb_dim = 256

hid_dim=512
n_layers=2

enc_dropout = 0.5
dec_dropout=0.5

enc = Encoder(input_dim, enc_emb_dim, hid_dim, n_layers, enc_dropout).to(device)
dec = Decoder(output_dim, dec_emb_dim, hid_dim, n_layers, dec_dropout).to(device)

model = Seq2Seq(enc, dec).to(device)
model.init_weights()

optimizer = optim.Adam(model.parameters())

# <pad> = padding
trg_pad_idx = kor_word_to_idx["<pad>"]

# <pad> í† í°ì˜ indexë¥¼ ë„˜ê²¨ ë°›ìœ¼ë©´ ì˜¤ì°¨ ê³„ì‚°í•˜ì§€ ì•Šê³  ignoreí•˜ê¸°
criterion = nn.CrossEntropyLoss(ignore_index = trg_pad_idx)
````

lossë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ criterion ë¶€ë¶„ì— ignore_index ë¶€ë¶„ì´ ì¶”ê°€ë¨ì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ì•ì„œ êµ¬ì„±í•œ batch ë‹¨ìœ„ í•™ìŠµì„ ìœ„í•´ padë¥¼ ì¶”ê°€í–ˆê¸° ë•Œë¬¸ì´ë‹¤. ë§Œì•½ batchì— ì†í•œ ë¬¸ì¥ì˜ ê¸¸ì´ëŠ” ê¸´ ë°˜ë©´ ì–´ë–¤ í•œ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ë§¤ìš° ì§§ë‹¤ë©´ ëŒ€ë¶€ë¶„ì€ `<pad>` í† í°ì¼ ê²ƒì´ê³ , ì´ì—ëŒ€í•œ í•™ìŠµì€ ë¶ˆí•„ìš”í•˜ê¸° ë•Œë¬¸ì— ignoreì„ í†µí•´ í•™ìŠµì—ì„œ ì œì™¸ì‹œí‚¨ë‹¤.

## 4. Translate

ì´ì œ train ë°ì´í„°ë¥¼ í†µí•´ ëª¨ë¸ì„ í†µí•´ í•™ìŠµì„ ì§„í–‰í•˜ê³ , epochë§ˆë‹¤ val ë°ì´í„°ë¥¼ í†µí•´ ëª¨ë¸ì„ ê²€ì¦í•  ê²ƒì´ë‹¤. ë˜í•œ ëª¨ë“  í•™ìŠµì´ ëë‚¬ì„ ë•Œ, test ë°ì´í„°ë¥¼ í†µí•´ ëª¨ë¸ì„ í‰ê°€í•  ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ê³¼ì •ì„ trackingì„ í•˜ê³  ì‹¶ë‹¤ë©´ wandbë¥¼ ì‚¬ìš©í•¨ì„ ì¶”ì²œí•œë‹¤.

### 4.1 Train code

````python
def train(model, iterator, optimizer, criterion, clip):
    model.train()
    epoch_loss=0
    
    for i, batch in enumerate(iterator):
        src = batch['english'].to(device)
        trg = batch['korean'].to(device)
        
        optimizer.zero_grad()
        
        output = model(src, trg)
        
        # trg = [batch size, trg len]
        # output = [batch size, trg len, output dim]
        output_dim = output.shape[-1]
        
        # <sos>ë¥¼ tokenì„ ì œì™¸í•˜ê¸° ìœ„í•œ indexing
        # loss ê³„ì‚°ì„ ìœ„í•œ tensor size ë³€ê²½
        output = output[:,1:].contiguous().view(-1, output_dim).to(device)
        trg = trg[:,1:].contiguous().view(-1)

        # output = [(trg len-1) * batch size, output dim)]
        # trg = [(trg len-1) * batch size]
        # loss ê³„ì‚°ì‹œì— output dimì€ softmaxë¥¼ í†µí•´ ì›ì†Œ í•˜ë‚˜ë¡œ ë³€í™˜ë¨
        loss = criterion(output, trg)
        
        loss.backward()
        
        # ê¸°ìš¸ê¸° í­ë°œ ë§‰ê¸° ìœ„í•œ clip
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        optimizer.step()
        
        epoch_loss+=loss.item()
        
    return epoch_loss/len(iterator)
````

### 4.2 Validation code

````python
def validation(model, iterator, criterion):
    model.eval()
    epoch_loss = 0
    
    with torch.no_grad():
        for i, batch in enumerate(iterator):
            src = batch['english'].to(device)
            trg = batch['korean'].to(device)
            
            # teacher_forcing_ratio = 0 (í•™ìŠµì´ ì•„ë‹ˆê¸°ì— ì •ë‹µì´ ë“¤ì–´ê°€ì§€ ì•ŠìŒ)
            output = model(src, trg, 0)
            
            # trg = [batch size, trg len]
            # output = [batch size, trg len, output dim]
            output_dim = output.shape[-1]
            
            output = output[:,1:].contiguous().view(-1, output_dim).to(device)
            trg = trg[:,1:].contiguous().view(-1)

            # output = [(trg len - 1) * batch size, output dim]
            # trg = [(trg len - 1) * batch size]
            loss = criterion(output, trg)
            
            epoch_loss+=loss.item()
        
        return epoch_loss/len(iterator)
````

### 4.3 Train & Validation

í•™ìŠµì˜ ì´ epochëŠ” 100ìœ¼ë¡œ ì„¤ì •í•˜ì˜€ìœ¼ë©°, early stopì€ ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ì ìš©í•˜ì§€ ì•Šì•˜ë‹¤.

````python
N_EPOCHS = 100
CLIP = 1
import time
import math 

best_valid_loss = float('inf')
wandb.init(project="nlp_translate", entity='sigi', name='Seq2Seq')

for epoch in range(N_EPOCHS):
    
    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)
    valid_loss = evaluate(model, val_dataloader, criterion) 
    
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'your path to save model')

    print(f'Epoch: {epoch+1:02}')
    print(f'\tTrain Loss: {train_loss:.3f}')
    print(f'\t Val. Loss: {valid_loss:.3f}')
    wandb.log({"train loss":train_loss, "val_loss": valid_loss})
````

### 4.4 Tracking

wnadbë¥¼ í†µí•´ train ë° val lossê°€ ì •ìƒì ìœ¼ë¡œ ì˜ ë–¨ì–´ì§€ê³  ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/seq2seq/seq_imp5.png?raw=true" width="800" height="300"></p>

### 4.5 Inference

ë§ˆì§€ë§‰ìœ¼ë¡œ ë²ˆì—­ì´ ì˜ ë˜ëŠ”ì§€ í•™ìŠµí•œ ëª¨ë¸ì„ í†µí•´ test ë°ì´í„°ì— ëŒ€í•œ ì¶”ë¡ ì„ ì§„í–‰í•´ ë³¸ë‹¤.

````python
def get_result(sentence_list, wti, itw):    
  result = []
  for sentence in sentence_list:
    try:
      eos_index = sentence.index(wti['<eos>'])
      result.append([itw[word] for word in sentence[1:eos_index]])
    except:
      continue
  return result

def inference(model, iterator):
  model.eval()
  df_result = pd.DataFrame()

  with torch.no_grad():
    for i, batch in enumerate(iterator):
      src = batch['english'].to(device)
      trg = batch['korean'].to(device)
      
      output = model(src, trg, 0)
      output = output.squeeze(1)
      output = output.argmax(dim=-1).tolist()

      origin_sentence = get_result(src.tolist(), eng_word_to_idx, eng_idx_to_word)
      answer_sentence = get_result(trg.tolist(), kor_word_to_idx, kor_idx_to_word)
      translate_sentence = get_result(output, kor_word_to_idx, kor_idx_to_word)

      english = pd.Series(origin_sentence, name='english')
      korean = pd.Series(answer_sentence, name='korean')
      translate = pd.Series(translate_sentence, name='translate')

      df_tmp = pd.concat([english, korean, translate], axis=1)
      df_result = pd.concat([df_result, df_tmp]

  return df_result
````

````python
df_result = inference(model, test_dataloader)
df_result
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/seq2seq/seq_imp6.png?raw=true" width="800" height="350"></p>

### 5. Lessons & Learned

- ì£¼ì–´ì§„ ë°ì´í„° ì…‹ì„ ì „ì²˜ë¦¬ í•˜ëŠ” ê³¼ì •
- pytorchì—ì„œ paddingí•˜ëŠ” ê³¼ì •
- NLP í‰ê°€ ì§€í‘œì— ëŒ€í•œ ì •ë¦¬ê°€ í•„ìš”í•¨
- ë²ˆì—­ ìˆ˜í–‰ í›„ ì´ë¥¼ ë‹¤ì‹œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë°”ê¾¸ëŠ” ë°©ë²•ì„ êµ¬í˜„í•´ì•¼í•¨

## ì°¸ì¡°

https://codlingual.tistory.com/91

https://github.com/VumBleBot/Group-Activity/issues/7