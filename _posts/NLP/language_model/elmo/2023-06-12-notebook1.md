---
layout: single
title:  'ELMO(1) 논문 러프하게 실습'
toc: true
categories: [ELMO]
tags: [LM, ELMO]
---

본 포스트는 [long8v](https://github.com/long8v)님께서 수행한 ELMO 논문 스터디 내용을 보고 정리하는 글이다. 전체 소스는 [이곳](https://github.com/long8v/torch_study)을 통해 확인 할 수 있따.
{: .notice}

### 1. Mecab 설치

colab에서 사용하기 위한 mecab 설치 방법은 [이곳](https://sigirace.github.io/knowledge/colab_mecab/)을 참조한다.

### 2. Module Import

````python
from konlpy.tag import Mecab
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
from Korpora import Korpora
import pickle
import torchtext
import sys

sys.path.append('본인경로')
from txt_cleaner.clean.master import MasterCleaner
from txt_cleaner.utils import *
from torch8text.data import Vocab, Field
````

### 3. Data Download

매번 다운로드 하기에 시간이 아까우니 한번 다운로드하여 pickle로 저장한 뒤 불러오는 방식을 사용한다.

1. download & save

````python
# Korpora.fetch('namuwikitext')

# corpus = Korpora.load('namuwikitext')
# with open('본인경로 + /kor.p', 'wb') as f:
#     pickle.dump(corpus, f)
````

2. load

````python
with open('본인경로+/kor.p', 'rb') as f:
    corpus = pickle.load(f)
````

### 4. Tokenize Function

````python
mecab = Mecab()

def tokenize_pos(inp):
    if type(inp) == str:
        return mecab.morphs(inp)
    if type(inp) == list:
        return [tokenize_pos(i) for i in inp]

tokenize_pos('안녕하세요'), tokenize_pos(['안녕하세요', '안녕?'])
````

````
(['안녕', '하', '세요'], [['안녕', '하', '세요'], ['안녕', '?']])
````

mecab의 morphs는 형태소를 추출하기 위한 모듈로, 문장을 형태소 단위로 tokenize하기 위해 사용한다.

### 5. Cleaner

MasterCleaner는 문장을 정제하기 위한 class로 해당 코드를 살펴보면 정규식을 통해 문장을 깨끗하게 만들어주는 역할을 함을 알 수 있다.

````python
import json

# Open the JSON file
with open('본인경로 + /txt_cleaner/cleaner_config.json') as json_file:
    # Load the JSON data
    config = json.load(json_file)

config['minimum_space_count'] = 2
config
````

````
{'minimum_space_count': 2}
````

````python
cleaner = MasterCleaner(config)
cleaner.cleaning('안녕하세요? 반갑습니다! 행복하세요~**')
````

````
안녕하세요? 반갑습니다! 행복하세요
````

### 6. Mecab Field

````python
mecab_field = Field(tokenize = tokenize_pos, 
                 preprocessing = cleaner.cleaning,
                    init_token = False,
                    eos_token = False
                )
````



