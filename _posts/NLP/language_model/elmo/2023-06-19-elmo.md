---
layout: single
title:  'LM(6) ELMO'
toc: true
categories: [Language Model]
tags: [LM]

---

ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” ê°•í•„ì„± êµìˆ˜ë‹˜ì˜ [ELMO ê°•ì˜](https://github.com/pilsung-kang/Text-Analytics/blob/master/08%20Seq2Seq%20Learning%20and%20Pre-trained%20Models/08-3_ELMo.pdf)ë¥¼ í†µí•´ ë°©ë²•ë¡ ê³¼ ì•„í‚¤í…ì²˜ë¥¼ ì†Œê°œí•œë‹¤.
{: .notice}

## 1. ELMo

### 1.1 ELMoë€?

- **E**mbedding from **L**anguage **Mo**dels

- **Pre-trained word representaitions**

  - ì‚¬ì „ì— í•™ìŠµëœ word representationì´ downstream taskì—ì„œ key component
  - ì´ëŠ” ë…¼ë¬¸(*Deep contextualized word representations*)ì—ì„œ ì €ìë“¤ì´ ì£¼ì¥í•˜ëŠ” ê²ƒ

- ì¢‹ì€ í’ˆì§ˆì˜ word representationì€ ì•„ë˜ 2ê°€ì§€ë¥¼ ëª¨ë¸ë§ í•  ìˆ˜ìˆì–´ì•¼ í•¨

  - ë‹¨ì–´ ì‚¬ìš©ì˜ ë³µì¡í•œ íŠ¹ì„±ì„ ëª¨ë¸ë§ (ë¬¸ë²•, ì˜ë¯¸ ë“±)

  - ë¬¸ë§¥ì— ë§ëŠ” í‘œí˜„ì´ ê°€ëŠ¥í•´ì•¼ í•¨ (ë‹¤ì˜ì–´ ë“±)

    â˜ ë‹¤ì˜ì–´ì¸ ê²½ìš° í•˜ë‚˜ì˜ ë‹¨ì–´ê°€ í‘œí˜„í•˜ëŠ” ë²¡í„°ê°€ ë¬¸ë§¥ì— ë”°ë¼ ë‹¬ë¼ì•¼ í•¨ 

ğŸ‘€ **Word Representation**

> NLPì—ì„œ ë‹¨ì–´ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•<br>ê°€ì¥ ê°„ë‹¨í•œ one-hot encodingë¶€í„° word2vec, glove, fast-textë“±ì´ ìˆìŒ

### 1.2 GloVe vs ELMo 

1. Glove
   - LSAì™€ Word2Vecì˜ ë¬¸ì œì ì„ ê°œì„ í•˜ê¸° ìœ„í•œ word representation ë°©ë²•ë¡ 
     - LSAëŠ” ë¬¸ì„œ ì „ì²´ì˜ í†µê³„ì  ì •ë³´ë¥¼ í™œìš©í•˜ì§€ë§Œ ë‹¨ì–´ê°„ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ê¸° ì–´ë ¤ì›€
     - Word2Vecì€ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆì§€ë§Œ, ì‚¬ìš©ìê°€ ì§€ì •í•œ window size ë‚´ì—ì„œë§Œ ê²°ê³¼ë¥¼ ë„ì¶œ
   - ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ë„ ì¸¡ì •í•  ìˆ˜ ìˆê³  ë¬¸ì„œ ì „ì²´ì˜ í†µê³„ì ì¸ ì •ë³´ë„ í™œìš© í•  ìˆ˜ ìˆìŒ

2. ELMo
   - ë‹¤ì˜ì–´ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ë²¡í„°ë¡œ ì„ë² ë”© í•  ìˆ˜ ìˆìŒ
   - ë¬¸ë§¥ì„ ê³ ë ¤í•œ contextualized word-embeddings ë°©ë²•
   - ê° ë‹¨ì–´ì— ê³ ì •ëœ ë²¡í„°ë¥¼ ì£¼ëŠ” ê²ƒì´ ì•„ë‹Œ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì„ë² ë”©ì„ ìˆ˜í–‰

ğŸ“**Example**

ğŸ˜—[Image1]

- playë¼ëŠ” í•˜ë‚˜ì˜ ë‹¨ì–´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, gloveëŠ” ìŠ¤í¬ì¸ ìª½ì— ì¹˜ìš°ì³ì§ì„ ì•Œ ìˆ˜ ìˆìŒ

- ì´ë¥¼ elmoë¥¼ í†µí•´ ì‚´í´ë³´ë©´ ì£¼ë³€ ë‹¨ì–´ êµ¬ì„±ì— ë”°ë¼ ì˜ë¯¸ë¥¼ êµ¬ë¶„í•¨

  â˜ gloveì™€ ë‹¤ë¥´ê²Œ elmoëŠ” ìƒí™©ì„ ë³´ê³  ì˜ë¯¸ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŒ

### 1.3 ELMo Feature of Representation

- ELMoì˜ embedding vector(representation)ëŠ” Bidirectional LSTMìœ¼ë¡œ ë¶€í„° ìƒì„±ë¨
  - ê° representationì€ ì „ì²´ ë¬¸ì¥ì„ ì…ë ¥ë°›ì•„ ìƒì„±ë¨
  - bi-LSTMì€ language modelì— ëŒ€í•œ ëª©ì í•¨ìˆ˜ë¥¼ ê°€ì§€ê³  í•™ìŠµë¨
- bi-LSTMë¥¼ êµ¬ì„±í•˜ëŠ” ê° layerë¥¼ ì‚¬ìš©í•˜ê¸°ì— deep í•œ representation
  - íŠ¹ì •í•œ layerì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ê° layerì—ì„œì˜ hiddenì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•¨
  - ì´ë¥¼ í†µí•´ ê°€ì¥ ìƒìœ„ layerë§Œì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„

ğŸ“**Example**

ğŸ˜—[Image2]

- Stackë˜ì–´ ìˆëŠ” ê° layerë“¤ì˜ ì„ í˜• ê²°í•©ì„ ì‚¬ìš©
  - ê°€ì¥ ìœ„ì— ìˆëŠ” ê²ƒì´ top LSTM layer
- ê° Layer ë§ˆë‹¤ ë‚´ì¬ëœ ì˜ë¯¸ê°€ ë‹¤ë¦„
  - ìƒìœ„ layerëŠ” context dependent (ì˜ë¯¸)
  - í•˜ìœ„ layerëŠ” syntax (êµ¬ë¬¸, POS tagging)
- ê° ë ˆì´ì–´ë§ˆë‹¤ ê°€ì§€ê³ ìˆëŠ” ì •ë³´ì˜ ê¹Šì´ í˜¹ì€ ë ˆë²¨ì´ ë‹¤ë¥´ê¸°ì— down taskì— ë”°ë¼ ì–´ëŠ layerì— ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ì¤„ ì§€ ì •í•  ìˆ˜ ìˆë‹¤

ğŸ‘€ **Context Dependent**

> ë§¥ë½ì— ì˜ì¡´ ë˜ëŠ” ë§¥ë½ì— ì˜í–¥ì„ ë°›ëŠ” ê²ƒì„ ëœ»í•˜ë©°, ë‹¤ì˜ì–´ì¼ ê²½ìš° ë§¥ë½ì— ë§ì¶”ì–´ ì˜ë¯¸ë¥¼ ì‚¬ìš©í•¨

## 2. Methodology

### 2.1 Language Modeling

ğŸ˜—[image3]

ğŸ‘€ **Language Modeling**

> ì£¼ì–´ì§„ ë‹¨ì–´ë“¤ë¡œ ë¶€í„° ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” task

### 2.2 Bidirectional LSTM

- ELMoëŠ” bi-directional LSTMì„ í†µí•´ í•™ìŠµë¨
- tokenì˜ ì´ˆê¸° ë‹¨ì–´ ì„ë² ë”©ì€ ë¬¸ì ë‹¨ìœ„ì˜ cnn í˜¹ì€ glove, word2vec ê³¼ ê°™ì€ pretrained ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ë„ ë¨ 
- ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ ë¿ ì•„ë‹ˆë¼ ì´ì „ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” language model
- ê° ë°©í–¥ì˜ LSTMì˜ layerì˜ hidden stateë¥¼ ê°€ì¤‘í•© í•˜ì—¬ ì‚¬ìš©í•¨

ğŸ“ **Example : Lets stick toì—ì„œì˜ stickì— ëŒ€í•œ embedding**

ğŸ˜—[image4]

ğŸ˜—[image5]

### 2.3 ELMo for Downstream Task

ğŸ˜—[image6]

- ELMoëŠ” ì…ë ¥ìœ¼ë¡œ ë°›ì€ íŠ¹ì • token kë¥¼ í•´ë‹¹ tokenì— ëŒ€ì‘ë˜ëŠ”(ê·¸ë¦¼ì—ì„œ ìœ„ì— ìŒ“ì´ëŠ”) layerë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ representation
- ê° layerì˜ hidden stateë“¤ ê°€ì¤‘í•© í˜•íƒœë¡œ ê²°í•©ë˜ëŠ”ë°, ì´ë•Œ ì‚¬ìš©ëœ ê°€ì¤‘ì¹˜ë“¤ì€ hyper param
  - ì´ë•Œ ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜ëŠ” softmax-normalized weight
- ìƒì„±ëœ ê°€ì¤‘í•©ì€ ë§ˆì§€ë§‰ìœ¼ë¡œ scaling factorë¥¼ ê±°ì³ downstream taskì— ì‚¬ìš©ë˜ê²Œ ë¨

### 2.4 Mathematical Demonstration

- ELMoë¥¼ í†µí•´ ì´ 2L+1ê°œì˜ representationì´ ê³„ì‚°ë¨
  - initial embedding 1ê°œ, forward Lê°œ, backward Lê°œ
- ëª¨ë“  representationë“¤ì€ ì´í›„ downstream taskì— ì‚¬ìš©ë˜ê¸° ìœ„í•´ ì•„ë˜ ì‹ì„ í†µí•´ ë‹¨ì¼ vectorë¡œ ì¶•ì†Œë¨

ğŸ˜—[image7]

### 2.5 Downstream Task

ğŸ˜—[image8]

- ìƒì„±ëœ ELMo represnetationì€ ìœ„ì™€ ê°™ì´ ì‚¬ìš© ë˜ë©°, ê°€ì¥ ì™¼ìª½ì˜ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í–ˆì„ ë•Œ íš¨ê³¼ê°€ ë†’ì•˜ìŒ