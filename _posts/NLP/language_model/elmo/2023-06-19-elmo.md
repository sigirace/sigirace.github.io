---
layout: single
title:  'LM(6) ELMO'
toc: true
categories: [Language Model]
tags: [LM]

---

이번 포스트는 강필성 교수님의 [ELMO 강의](https://github.com/pilsung-kang/Text-Analytics/blob/master/08%20Seq2Seq%20Learning%20and%20Pre-trained%20Models/08-3_ELMo.pdf)를 통해 방법론과 아키텍처를 소개한다.
{: .notice}

## 1. ELMo

### 1.1 ELMo란?

- **E**mbedding from **L**anguage **Mo**dels

- **Pre-trained word representaitions**

  - 사전에 학습된 word representation이 downstream task에서 key component
  - 이는 논문(*Deep contextualized word representations*)에서 저자들이 주장하는 것

- 좋은 품질의 word representation은 아래 2가지를 모델링 할 수있어야 함

  - 단어 사용의 복잡한 특성을 모델링 (문법, 의미 등)

  - 문맥에 맞는 표현이 가능해야 함 (다의어 등)

    ☞ 다의어인 경우 하나의 단어가 표현하는 벡터가 문맥에 따라 달라야 함 

👀 **Word Representation**

> NLP에서 단어를 컴퓨터가 이해할 수 있는 형태로 표현하는 방법<br>가장 간단한 one-hot encoding부터 word2vec, glove, fast-text등이 있음

### 1.2 GloVe vs ELMo 

1. Glove
   - LSA와 Word2Vec의 문제점을 개선하기 위한 word representation 방법론
     - LSA는 문서 전체의 통계적 정보를 활용하지만 단어간 유사도를 측정하기 어려움
     - Word2Vec은 단어 간 유사도를 측정할 수 있지만, 사용자가 지정한 window size 내에서만 결과를 도출
   - 단어 간 유사도도 측정할 수 있고 문서 전체의 통계적인 정보도 활용 할 수 있음

2. ELMo
   - 다의어에 대해 서로 다른 벡터로 임베딩 할 수 있음
   - 문맥을 고려한 contextualized word-embeddings 방법
   - 각 단어에 고정된 벡터를 주는 것이 아닌 문맥을 고려하여 임베딩을 수행

📍**Example**

😗[Image1]

- play라는 하나의 단어가 주어졌을 때, glove는 스포츠쪽에 치우쳐짐을 알 수 있음

- 이를 elmo를 통해 살펴보면 주변 단어 구성에 따라 의미를 구분함

  ☞ glove와 다르게 elmo는 상황을 보고 의미를 파악할 수 있음

### 1.3 ELMo Feature of Representation

- ELMo의 embedding vector(representation)는 Bidirectional LSTM으로 부터 생성됨
  - 각 representation은 전체 문장을 입력받아 생성됨
  - bi-LSTM은 language model에 대한 목적함수를 가지고 학습됨
- bi-LSTM를 구성하는 각 layer를 사용하기에 deep 한 representation
  - 특정한 layer에 해당하는 값을 사용하지 않고, 각 layer에서의 hidden을 결합하여 사용함
  - 이를 통해 가장 상위 layer만을 사용하는 것 보다 더 나은 성능을 보임

📍**Example**

😗[Image2]

- Stack되어 있는 각 layer들의 선형 결합을 사용
  - 가장 위에 있는 것이 top LSTM layer
- 각 Layer 마다 내재된 의미가 다름
  - 상위 layer는 context dependent (의미)
  - 하위 layer는 syntax (구문, POS tagging)
- 각 레이어마다 가지고있는 정보의 깊이 혹은 레벨이 다르기에 down task에 따라 어느 layer에 높은 가중치를 줄 지 정할 수 있다

👀 **Context Dependent**

> 맥락에 의존 또는 맥락에 영향을 받는 것을 뜻하며, 다의어일 경우 맥락에 맞추어 의미를 사용함

## 2. Methodology

### 2.1 Language Modeling

😗[image3]

👀 **Language Modeling**

> 주어진 단어들로 부터 다음 단어를 예측하는 task

### 2.2 Bidirectional LSTM

- ELMo는 bi-directional LSTM을 통해 학습됨
- token의 초기 단어 임베딩은 문자 단위의 cnn 혹은 glove, word2vec 과 같은 pretrained 임베딩을 사용하여도 됨 
- 다음 단어를 예측하는 것 뿐 아니라 이전단어를 예측하는 language model
- 각 방향의 LSTM의 layer의 hidden state를 가중합 하여 사용함

📍 **Example : Lets stick to에서의 stick에 대한 embedding**

😗[image4]

😗[image5]

### 2.3 ELMo for Downstream Task

😗[image6]

- ELMo는 입력으로 받은 특정 token k를 해당 token에 대응되는(그림에서 위에 쌓이는) layer들의 선형 결합으로 representation
- 각 layer의 hidden state들 가중합 형태로 결합되는데, 이때 사용된 가중치들은 hyper param
  - 이때 사용되는 가중치는 softmax-normalized weight
- 생성된 가중합은 마지막으로 scaling factor를 거쳐 downstream task에 사용되게 됨

### 2.4 Mathematical Demonstration

- ELMo를 통해 총 2L+1개의 representation이 계산됨
  - initial embedding 1개, forward L개, backward L개
- 모든 representation들은 이후 downstream task에 사용되기 위해 아래 식을 통해 단일 vector로 축소됨

😗[image7]

### 2.5 Downstream Task

😗[image8]

- 생성된 ELMo represnetation은 위와 같이 사용 되며, 가장 왼쪽의 방식으로 사용했을 때 효과가 높았음