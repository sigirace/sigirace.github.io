---
layout: single
title:  'LM(3) 신경망 언어모델'
toc: true
categories: [Language Model]
tags: [Nueral Language Model]

---

신경망 언어모델의 접근 방법에 대해 알아본다.
{: .notice}

## 1. 신경망 언어 모델

통계적 언어 모델은 희소 문제를 해결하지 못하는 문제가 있었고, 이러한 단점을 개선할 것이 신경망 언어 모델(Neural Network Language Model)이다. NNLM은 Feed-Forward 신경망 모델로 부터 시작하였는데, 이는 지금의 Embedding 레이어의 아이디어인 모델이다.

### 1.1 단어의 의미적 유사성

희소 문제는 모델이 단어의 의미적 유사성을 학습할 수 있다면 해결 할 수 있는 문제이다. 예를들어 요즘 '맛있다'라는 단어는 단순 음식에 대해서만 쓰이지 않고, 어떤 행동에 대해 좋은 의미로 많이쓰이고 있다. 이처럼 나는 유튜브를 통해 이 단어의 쓰임새와 의미를 학습했기 때문에, '휴가는 맛있다'라는 문장을 생성 할 수 있다. 

'휴가는 좋다'라는 시퀀스는 존재하지만 '휴가는 맛있다'라는 시퀀스는 없는 코퍼스를 학습한 언어 모델이 있다고 가정했을 때, 언어모델이 다음 단어를 예측하는 과정을 살펴본다.


$$
P(맛있다\, |\,휴가는 )\\
P(마셨다\, |\,휴가는)
$$


나의 경우 '좋다'와 '맛있다'의 유사성을 학습하였기에 두 선택지 중에서 '맛있다'가 맞다(=높은 확률)고 판단할 수 있다. 하지만 n-gram 언어 모델은 두 단어 모두 등장한 적이 없기에 확률을 동일하게 0으로 연산한다. 즉, 단어의 연관성을 알 수 없기에 실생활에 근접한 예측을 할 수 없다. 이는 언어 모델의 목적과 동떨어짐을 의미한다.

따라서 언어 모델 또한 의미적 유사성을 학습할 수 있도록 설계한다면, 훈련 코퍼스에 없는 단어의 시퀀스에 대한 예측이라도 유사 단어가 사용된 단어 시퀀스를 참고하여 보다 정확한 예측을 할 수 있을 것이다. 이러한 아이디어를 반영한 언어 모델이 신경망 언어모델 NNLM이다. 또한 이러한 아이디어는 단어 벡터간 유사도를 구할 수 있는 벡터를 얻어내는 워드 임베딩(word embedding)의 아이디어이기도 하다. 다음으로 NNLM이 어떻게 훈련에서 단어간 유사도를 학습할 수 있는지 알아본다.



## 2. 피드 포워드 신경망 언어 모델 학습 과정

아래 예시 문장이 NNLM 언어 모델에서 학습되는 과정을 살펴본다.

📍**Example**

> What will the fat cat sit on

### 2.1 One-hot Encoding

코퍼스가 준비된 상태에서 가장 먼저 해야 할 일은 모델이 단어를 인식할 수 있도록 모든 단어를 수치화 하는 것이다. 단어를 수치로 변환하는 가장 간단한 방법은 one-hot encoding이며, 위 예시 단어들은 아래와 인코딩 할 수 있다.

```
what = [1, 0, 0, 0, 0, 0, 0]
will = [0, 1, 0, 0, 0, 0, 0]
the = [0, 0, 1, 0, 0, 0, 0]
fat = [0, 0, 0, 1, 0, 0, 0]
cat = [0, 0, 0, 0, 1, 0, 0]
sit = [0, 0, 0, 0, 0, 1, 0]
on = [0, 0, 0, 0, 0, 0, 1]
```

모든 단어가 최대 단어 개수에 따라 7의 차원을 가지는 원-핫 벡터가 되었다. 

### 2.2 Window Size Setting

NNLM은 n-gram 언어 모델 처럼 다음 단어를 예측할 때, 정해진 개수의 단어만을 참고한다. 이 개수를 window size(=n)이라하고, n을 4라고 가정한다. 이때 예시의 sit을 예측하기 위해서는 앞의 4단어 'will the fat cat'의 원-핫 벡터들이 입력으로 들어가고, 모델의 연산을 거친 후 나온 웟-핫 벡터는 'sit'이 되어야 한다. 아래는 이러한 예시의 구조를 도식화 한 것이다.

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/nnlm/nnlm1.png?raw=true" width="600"></p>

### 2.3 Projection

위 구조의 첫번째 입력층은 앞서 window size를 4로 구성하였기에, 4개의 원-핫 벡터가 들어감을 볼 수 있다. 그다음 Projection layer를 지나게 되는데, 이는 단순 가중치 행렬과의 선형 곱 연산을 수행한다. 아래는 그 예시이다.

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/nnlm/nnlm2.png?raw=true" width="600"></p>

각 단어의 원-핫 벡터와 가중치 행렬 W의 곱이 이루어지는 과정을 보면 W의 한 행을 그저 읽어오는 역할을 함을 알 수 있다. 이는 곱해지는 것중 원-핫 벡터가 어떤 열에만 1의 값이 있기 때문이다. 따라서 이러한 곱셈 작업을 다른말로 lookup table이라고도 한다. 또한 곱셈 연산을 통해 벡터의 차원 또한 바뀌었음을 알 수 있다. 이는 대부분 0으로 이루어졌던 원-핫 벡터가 압축되고 값을 가지게 되면서 의미를 내재(=latent)하게 되는 과정이라고 볼 수 있다. 연산을 통해 나온 벡터는 초기에는 랜덤한 값을 가지지만, 학습 과정을 거쳐 정확한 의미를 가진 벡터로 변경되는데 이를 임베딩 벡터(=embedding vector)라고 한다.

📍 **Projection layer vs Hidden layer**

> 보통 입력과 출력 사이의 layer는 hidden layer라고 부르는데, 이곳에서는 활성화 함수가 따로 작용하지 않고 단순 곱 연산을 수행하기에 projection layer라고 부른다. 

📍**Weight Matrix Dimension**

> Weight Matrix의 행(V)은 원-핫벡터와 연산이 이루어져야 하기 때문에, 코퍼스가 가진 단어의 수가 된다. 열(M)은 내재될 의미의 크기로 사용자가 지정하는 hyper-parameter이다.



### 2.4 Concatenate

각 단어가 테이블 룩업을 통해 임베딩 벡터로 변경되고 난 후, 모든 임베딩 벡터들의 값은 연결(concatenate)된다. 여기서 벡터의 연결은 단순히 벡터들을 이어 붙이는 것을 의미한다. 이를 도식화하면 아래와 같다.

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/nnlm/nnlm3.png?raw=true" width="600"></p>

즉, Projection Layer의 output은 각 단어들이 임베딩 벡터로 변환된 것들의 연결임을 알 수 있다. 수식은 아래와 같으며 연결은 ;로 표현한다.


$$
P^{layer} = (lookup(x_{t-n});...;lookup(x_{t-1})) = (e_{t-n};...;e_{t-1})
$$


### 2.5 Hidden Layer

앞서 생성한 임베딩 벡터의 연결을 입력으로 하는 은닉층을 통과한다. 이때 사용하는 비선형 활성화 함수는 다양하게 쓰일 수 있지만 보통 tanh를 쓴다.


$$
h^{layer} = tanh(W_hp^{layer} + b_{h})
$$


이때, 입력이 가중치와 곱해지면서 출력 차원이 원래의 원-핫 벡터의 차원과 동일하게 된다. 예시에서 원-핫 벡터의 차원이 7이였기 때문에 이 은닉층의 출력도 차원이 7인 벡터를 갖게 된다.

### 2.6 Softmax

Hidden layer의 출력은 활성화 함수 softmax를 지나며 벡터의 각원소가 0에서 1사이의 실수값을 가지게된다. 또한, 모든 벡터의 합은 1이 되므로 softmax 함수를 거친 결과는 단어에 대해 모델이 예측한 확률이라고 보아도 무방하다.

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/nnlm/nnlm4.png?raw=true" width="600"></p>

$$
\hat{y} = softmax(W_yh^{layer}+b_y)
$$


softmax를 거친 벡터 $ \hat{y} $의 각 요소가 갖는 의미는 다음 단어로 해당 단어가 나올 확률을 의미한다. 위의 예시에서 원-핫 벡터에서 sit의 인덱스는 5이고, $ \hat{y} $는 인덱스가 5일때 가장 큰 값 0.7을 갖기 때문에 다음 나올 단어중 가장 높은 확률을 나타내며, 다음 나올 단어로 sit를 선택하는 근거가 된다. 

### 2.7 Cross-Entropy

예측값 $ \hat{y} $은 실제 sit의 원-핫 벡터와 최대한 동일한 값을 가져야 한다. 즉, 두 벡터간의 손실을 최소화 시켜야 하며, 이를 위해 크로스 엔트로피(cross-entropy) 함수를 사용한다. 해당 문제는 단어 집합의 모든 단어라는 V개의 선택지 중 정답인 'sit'를 예측해야 하는 다중 클래스 분류 문제이기 때문이다. 

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/nlp/nnlm/nnlm5.png?raw=true" width="300"></p>

### 2.8 Backpropagation

손실 함수를 통해 구한 loss 값을 사용하여 backpropagation을 진행하면, 모든 가중치 행렬(projection layer의 weight matrix, hidden layer의 weight matrix)들이 학습된다. 

위의 예시에서는 한 문장, 7개 단어 만을 통해 학습을 진행하였지만, 만약 충분한 훈련 데이터가 있다면 수 많은 문장에서 유사한 목적으로 사용되는 단어들은 결국 유사한 임베딩 벡터를 얻게 된다. 단어간 유사도를 구할 수 있는 임베딩 벡터의 아이디어는 Word2Vec, FastText, GloVe등으로 발전되어 딥 러닝 자연어 처리 모델에서는 필수적으로 사용되는 방법이 되었다. 이에 대해서는 나중에 자세히 포스팅할 것이다.



## 3. NNLM 이점과 한계

### 3.1 개선점

NNLM은 단어를 표현하기 위해 임베딩 벡터를 사용하므로, 단어의 유사도를 계산할 수 있다. 그리고 이를 통해 희소 문제를 해결하였다.

### 3.2 한계

NNLM은 n-gram과 같이 모든 이전 단어가 아닌 특정 n개의 단어만을 참고한다. 따라서 문맥상 앞의 위치에 중요한 내용이 있어도 무시해버리는 문제가 발생한다. 이 한계를 극복하기 위해 RNN을 사용한 언어 모델이 등장하였다.

## 4. 참조

https://wikidocs.net/45609
