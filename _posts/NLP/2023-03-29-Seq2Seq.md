---
layout: single
title:  'LM(4) Sequence to Sequence'
toc: true
categories: [Language Model]
tags: [LM]

---

Sequence to Sequence의 방법에 대해 알아본다.
{: .notice}

## 1. 시퀀스-투-시퀀스(Sequence-to-Sequence)

앞서 알아본 NNLM은 다음 단어 예측을 위해 window size만큼을 지정해주었기에 유연성에 한계가 있었다. 따라서 단어의 개수에 무관하게 처리할 수 있는 네트워크가 필요했고, 그것이 RNN 그리고 LSTM의 아이디어로 이어졌다. LSTM은 RNN 계열의 모델이기에 내부 구성의 차이가 있을 뿐, 동작 방식은 동일하다. 이에 대한 내용은 아래 링크에서 확인할 수 있으니 이번 포스트를 읽기 전 반드시 확인이 필요하다. 



<div class="notice">
<li><a href="https://sigirace.github.io/deep%20learning/LSTM_method///">LSTM의 모든것 (1) LSTM 및 내부 Gate에 대한 이해</a></li>
<li><a href="https://sigirace.github.io/deep%20learning/LSTM_components/">LSTM의 모든것 (2) PyTorch 공식 문서로 보는 구성요소</a></li>
</div>



RNN 계열의 모듈이 사용된 시퀀스 투 시퀀스(=seq2seq)는 단어 시퀀스를 입력으로 받아 또 다른 단어 시퀀스를 출력하는 특징을 가지고 있다. 대표적인 예시는 챗봇과 기계번역인데, 입력 시퀀스와 출력 시퀀스를 질문-답으로 구성하면 챗봇으로 만들 수 있고, 영어-한국어 등 번역 문장으로 구성하면 번역기로 만들 수 있다. 따라서 이번 포스트는 seq2seq에 대한 이해를 위해 번역기를 예시로 한다.



### 1.1 Model Architecture

😗 [Image1]

Seq2seq은 크게 인코더와 디코더라는 두 개의 모듈로 구성되며, 이를 번역기의 예시로 도식화 하면 위와 같다. 단어의 시퀀스(=문장)가 입력되면 인코더안의 RNN 계열 모듈들은 순차적 연산을 통해 모든 단어가 압축된 하나의 벡터(=마지막 timestep의 출력)를 만드는데, 이를 context 벡터라고 한다. 이렇게 입력 문장의 정보가 하나의 context 벡터로 압축되면, 인코더는 이를 디코더로 전송한다. 전송된 context 벡터는 디코더의 첫번째 hidden state로 첫번째 입력과 함께 연산된다. 디코더 또한 RNN 계열 모듈로 이루어져있기 때문에 인코더와 같은 연산을 진행하나, 각 timestep의 출력을 모두 사용한다는 차이점이 있다.



### 1.2 Word Embedding

언어 모델의 연산을 위해서 기계에게 텍스트가 아닌 숫자로 이루어진 벡터를 입력으로 주어야한다. 이를 위한 방법으로 word embedding을 사용하며, 보통은 framework에서 지원하는 embedding layer를 사용하곤 한다. 이는 텍스트를 숫자에 매핑하고 lookup table을 읽어오는 단순한 구조이며, 결국 텍스트를 벡터로 변환하게 된다. 이러한 embedding 과정은 인코더와 디코더 모두 적용되어야 한다. 아래는 연산하기 전, 임베딩 레이어를 거치는 과정을 도식화 한 것이다.

😗 [Image2]

Word embedding의 기법은 embedding layer 뿐만 아니라, word2vec, glove 등등 다양하게 존재한다. 만약 높은 성능의 기계 번역 혹은 언어 모델을 구축하고 싶다면 초기값을 가진 embedding layer가 아닌 pre-trained된 모델들을 적용시키는 방법들도 가능한다. 이에 대한 자세한 내용은 추후 포스팅하도록 할 것이다.



### 1.3 Predict

앞서 말했듯, 디코더는 인코더의 마지막 출력인 context 벡터를 첫번째 hidden state로, sos(=start of sentence)를 첫번째 입력으로 사용한다. 이 두가지 요소가 RNN 계열 모듈을 통해 연산되면 하나의 벡터 값이 나오게 되고, 이것이 seq2seq 언어 모델의 첫번째 timestep(t=0)의 예측 값이 된다. 하지만 이는 텍스트가 아닌 숫자 벡터이기에 이를 다시 텍스트로 변환시키기 위한 softmax 연산이 필요하다. 이를 도식화하면 아래와 같다. 예시에서는 RNN 계열 모듈 중 LSTM을 사용하였고, softmax 함수를 통해 텍스트 변환을 하기 전 dense layer를 거치도록 구성하였다.

😗 [Image3]

해당 과정을 통해 나온 첫번째 timestep(t=0)의 예측은 je이다. 위 그림에서는 예측한 je가 다음 LSTM에 들어가는 것으로 보이지만, 실상 LSTM은 하나이며, sos와 똑같은 연산(embedding -> lstm -> dense -> softmax)이 수행된다. 단, 연산에 필요한 hidden state는 contex 벡터가 아닌 첫번째 timestep(t=1)의 연산 결과인 벡터로 바뀌게 된다. 이러한 과정이 각 timestep(t=1, 2, 3...)마다 반복되며 결국 LSTM의 예측 단어가 eos가 되면 언어 모델의 번역이 끝나게 된다.



### 1.3 Teacher forcing (=교사 강요)

모델을 설계하고 훈련하기 앞서 teacher forcing(=교사 강요)의 개념이 필요하다.  



## 참조

https://wikidocs.net/24996

https://heegyukim.medium.com/keras-embedding%EC%9D%80-word2vec%EC%9D%B4-%EC%95%84%EB%8B%88%EB%8B%A4-619bd683ded6

https://wikidocs.net/33793

https://ebbnflow.tistory.com/154

https://jiho-ml.com/weekly-nlp-22/









