{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 데이터 전처리\n",
    "\n",
    "## 5.1 이상치 확인 및 정제\n",
    "\n",
    "### 5.1.1 이상치\n",
    "\n",
    "👀 **Definition**\n",
    "> 결측치 혹은 값이 크게 차이가 나는 데이터\n",
    "\n",
    "- 이상치는 측정의 변동성, 실험의 오류, 측정 장비의 이상 등으로 발생할 수 있음\n",
    "- 이상치는 분석 모델의 성능을 떨어뜨리거나 분석 결과에 악영향을 줄 수 있음 ☞ 제거 혹은 처리 필요\n",
    "\n",
    "📍 **이상치의 종류**\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/sigirace/page-images/blob/main/kang_lectures/python_preprocessing/outlier_type.png?raw=true\" width=\"400\" height=\"200\"></p>\n",
    "\n",
    "✏️ **더 알아보기**\n",
    "- 😗: 데이터가 너무 많아서 일일이 확인하기 어려운데 이상치를 한방에 확인할 수 있을까요?\n",
    "- 😀: 4.2 상자그림에서 배운 IQR 방식을 사용하여 이상치를 확인할 수 있습니다!\n",
    "\n",
    "📜 **예제**\n",
    "\n",
    "> wine 데이터 세트의 color_intesnity 컬럼에서 IQR 방식으로 이상치를 탐색해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# 데이터 가져오기\n",
    "wine_load = load_wine()\n",
    "wine = pd.DataFrame(wine_load.data, columns=wine_load.feature_names)\n",
    "wine['Class'] = wine_load.target\n",
    "wine['Class'] = wine['Class'].map({0:'class_0', 1:'class_1', 2:'class_2'})\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot 그리기\n",
    "plt.boxplot(wine['color_intensity'])\n",
    "plt.title('color_intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 이상치 함수\n",
    "def outliers_iqr(dt, col):\n",
    " quartile_1, quartile_3 = np.percentile(dt[col], [25, 75])\n",
    " iqr = quartile_3 - quartile_1\n",
    " lower_whis = quartile_1 - (iqr * 1.5)\n",
    " upper_whis = quartile_3 + (iqr * 1.5)\n",
    " outliers = dt[(dt[col] > upper_whis) | (dt[col] < lower_whis)]\n",
    " return outliers[[col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 확인\n",
    "outliers = outliers_iqr(wine,'color_intensity')\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📜 **예제**\n",
    "\n",
    "> 이상치를 제거하거나 대체하는 방식으로 데이터 전처리를 수행해보기\n",
    "\n",
    "**1. 이상치 제거**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 기준으로 이상치 행 추출\n",
    "drop_index = wine[wine.index.isin(outliers.index)].index\n",
    "drop_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"이상치 삭제 전:\", wine.shape)\n",
    "# 이상치 삭제\n",
    "drop_wine = wine.drop(drop_index)\n",
    "print(\"이상치 삭제 후:\", drop_wine.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 이상치 대체**\n",
    "\n",
    "- 이상치를 결측치로 변환 후 결측치 대체 방식 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 기준으로 이상치를 결측치로 변경\n",
    "wine.loc[drop_index, 'color_intensity'] = np.nan\n",
    "print(\"결측치:\", wine['color_intensity'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치를 평균값으로 대체\n",
    "wine['color_intensity'].fillna(wine['color_intensity'].mean(), inplace=True)\n",
    "# 확인\n",
    "wine.loc[drop_index, 'color_intensity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 범주형 변수 처리\n",
    "\n",
    "범주형 변수는 값이 수학적 연산으로 모델을 생성하는 대부분의 분석 도구에서 직접 사용할 수 없어 특별한 처리가 필요합니다. 이를 인코딩이라고 부르며 라벨 인코딩과 원핫 인코딩이 대표적입니다.\n",
    "\n",
    "### 5.2.1 라벨 인코딩\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/sigirace/page-images/blob/main/kang_lectures/python_preprocessing/label.png?raw=true\" width=\"300\" height=\"150\"></p>\n",
    "\n",
    "👀 **Definition**\n",
    "> 알파벳 순서로 숫자를 할당\n",
    "- 변환된 숫자가 순위를 잘못 나타내는지 확인 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 라벨 인코딩을 위한 데이터 생성\n",
    "item_label = ['b','a','c','d','a','b']\n",
    "\n",
    "# 라벨 인코딩 객체 생성\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# 라벨 인코딩 수행\n",
    "encoder.fit(item_label)\n",
    "\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 인코딩 결과 확인\n",
    "vars(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 중복이 제거되며 각 원소마다 라벨이 부여됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 인코딩 적용\n",
    "test_label = ['a','a','b','d','c']\n",
    "digit_label = encoder.transform(test_label)\n",
    "print(digit_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 인코딩 원복\n",
    "print(encoder.inverse_transform(digit_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 원핫 인코딩\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/sigirace/page-images/blob/main/kang_lectures/python_preprocessing/onehot.png?raw=true\" width=\"450\" height=\"150\"></p>\n",
    "\n",
    "👀 **Definition**\n",
    "> 범주형 변수를 이진화 시킴 \n",
    "- 0 또는 1로써 범주형 변수를 표현하는 기법\n",
    "- 범주가 다양할 경우 0이 많아지는 문제가 발생함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# 원핫 인코딩을 위한 데이터 생성\n",
    "data_dic = {'label':['Apple', 'Banana', 'Pear', 'Apple', 'Mango']}\n",
    "df = pd.DataFrame(data_dic)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫 인코딩 객체 생성\n",
    "oh = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# 원핫 인코딩 수행\n",
    "oh.fit(df)\n",
    "oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫 인코딩 결과 확인\n",
    "vars(oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫 인코딩 적용\n",
    "oh_encoded = oh.transform(df)\n",
    "oh_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원핫 인코딩 수행시 배열 형태가 생성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_df = pd.DataFrame(oh_encoded.astype('int'), columns=oh.get_feature_names_out(['label']), index=df.index)\n",
    "oh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원핫 인코딩 원복\n",
    "oh.inverse_transform(pd.DataFrame([1, 0, 0, 0]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **더 알아보기**\n",
    "- 😗: 언제 어떤 인코딩을 사용해야 하나요?\n",
    "- 😀: Label 인코딩은 범주형 변수가 순서가 있거나, 범주 고유값의 개수가 많아 One-hot 적용시 메모리에 이슈가 있을 때 사용합니다. (예시: 학년, 직급 등)\n",
    "- 😀: One-hot 인코딩은 순서가 없으며 고유값의 개수가 많지 않을때 사용합니다. (예시: 과일가게에서 파는 과일의 종류)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 데이터 분할\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/sigirace/page-images/blob/main/kang_lectures/python_preprocessing/data_split.png?raw=true\" width=\"550\" height=\"350\"></p>\n",
    "\n",
    "👀 **Definition**\n",
    "> 분석 모델을 학습하고 성과를 확인하기 위해서 데이터를 Train과 Test 세트로 나누는 것\n",
    "\n",
    "📌 **train_test_split**\n",
    "\n",
    "> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=None, train_size=None, stratify=None)\n",
    "\n",
    "- X: 독립변수(데이터프레임)\n",
    "- y: 종속변수(list)\n",
    "- test_size: 테스트 사이즈 비율\n",
    "- stratify: 동일 비율로 나눌 컬럼\n",
    "\n",
    "📜 **예제**\n",
    "\n",
    "> iris 데이터를 train 0.8, test 0.2 비율로 나누어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris['Class'] = load_iris().target\n",
    "iris['Class'] = iris['Class'].map({0:'Setosa', 1:'Versicolour', 2:'Virginica'})\n",
    "print(\"총 데이터 수: \",len(iris))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.drop(columns='Class'), iris['Class'], test_size = 0.2, random_state=1004)\n",
    "print('X_train :', X_train.shape, ' X_test :', X_test.shape)\n",
    "print('y_train :', y_train.shape, ' y_test :', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ **더 알아보기**\n",
    "- 😗: 데이터 분할이 잘못되어 문제집에 없는 문제가 수능에 나오면 어떻게 되나요?\n",
    "- 😀: 데이터 분할을 적절히 하지 못해 불균형이 일어나게 된다면, 분석모델의 성능평가에 대한 신뢰성이 떨어지게 됩니다. 이럴 경우 데이터 분할을 적절히 수행하여 모든 문제가 골고루 분할되도록 하는 층화추출 기법으로 해결할 수 있습니다.\n",
    "\n",
    "📍 **층화 추출**\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/sigirace/page-images/blob/main/kang_lectures/python_preprocessing/stratify.png?raw=true\" width=\"500\" height=\"250\"></p>\n",
    "\n",
    "- 층화추출을 통해 데이터를 골고루 분배할 수 있음\n",
    "\n",
    "📜 **예제**\n",
    "\n",
    "> 임의로 분할한 데이터셋의 클래스 분포를 확인하고, 층화추출을 통해 target의 불균형 문제를 해결해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.drop(columns='Class'), iris['Class'], test_size = 0.2, random_state=1002)\n",
    "\n",
    "# 클래스 분포 확인\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 층화추출로 iris['Class']를 고루게 분포시킴\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.drop(columns='Class'), \n",
    "                                                    iris['Class'], test_size = 0.2, \n",
    "                                                    stratify =iris['Class'])\n",
    "# 클래스 분포 확인\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 데이터 스케일링\n",
    "\n",
    "현업에서 학습에 사용되는 데이터들은 각 컬럼이 가지는 값의 범위가 다양합니다. 예를들어 태양광 발전량 예측을 위해 사용하는 컬럼 중 온도는 약 -10~35이고 강수량은 0~1,000까지 나타날 수 있습니다. 이렇게 각 컬럼별 단위가 크게 차이가 날 경우 분석모델은 대체로 값이 큰 쪽에 편향되어 학습을 하고, 값이 작은 쪽의 영향은 작다고 판단하기에 좋은 성능을 낼 수 없습니다. 따라서 각 컬럼의 범위를 균일화 하는 스케일링을 통해 이러한 문제를 해결해야 합니다.\n",
    "\n",
    "\n",
    "📍 **데이터 스케일링 수행 시점**\n",
    "\n",
    "데이터 스케일링은 학습 데이터에 대해서만 수행하며, 분석 모델 설계, 학습 시 validation, test 데이터는 아직 보지 못했다고 가정하기 때문에 데이터를 분할한 뒤 사용합니다.\n",
    "\n",
    "### 5.4.1 Standard Scaler\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/sigirace/page-images/blob/main/kang_lectures/python_preprocessing/std_scaler.png?raw=true\" width=\"500\" height=\"150\"></p>\n",
    "\n",
    "- 데이터를 평균이 0, 분산이 1인 정규분포로 스케일링함\n",
    "\n",
    "📜 **예제**\n",
    "\n",
    "> iris 데이터에 대한 데이터 분포를 확인하고 standard scaling을 수행한 뒤 분포를 재확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 분포\n",
    "# 데이터 프레임을 넘파이 배열로 변환\n",
    "# 각 컬럼별이 아닌 모든 데이터의 연산을 수행하기 위함 (dataframe.function()은 컬럼별 연산)\n",
    "print ('Train_scaled (%.2f, %.2f) (%.2f, %.2f)'%(np.array(X_train).min(), \n",
    "                                                 np.array(X_train).max(), \n",
    "                                                 np.array(X_train).mean(), \n",
    "                                                 np.array(X_train).std()))\n",
    "print ('Test_scaled (%.2f, %.2f) (%.2f, %.2f)'%(np.array(X_test).min(), \n",
    "                                                np.array(X_test).max(), \n",
    "                                                np.array(X_test).mean(), \n",
    "                                                np.array(X_test).std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "StdScaler = StandardScaler()\n",
    "# Train 데이터의 fitting과 스케일링\n",
    "StdScaler.fit(X_train)\n",
    "X_train_sc = StdScaler.transform(X_train)\n",
    "# Test 데이터의 스케일링\n",
    "X_test_sc = StdScaler.transform(X_test)\n",
    "# 결과 확인\n",
    "print('\\t\\t(min, max) (mean, std)')\n",
    "print ('Train_scaled (%.2f, %.2f) (%.2f, %.2f)'%(X_train_sc.min(), \n",
    "                                                 X_train_sc.max(), \n",
    "                                                 X_train_sc.mean(), \n",
    "                                                 X_train_sc.std()))\n",
    "print ('Test_scaled (%.2f, %.2f) (%.2f, %.2f)'%(X_test_sc.min(), \n",
    "                                                X_test_sc.max(), \n",
    "                                                X_test_sc.mean(), \n",
    "                                                X_test_sc.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 Min-max Scaler\n",
    "\n",
    "- 컬럼들을 0과 1 사이의 값으로 스케일링 하는 방식\n",
    "- 최솟값이 0 최댓값이 1\n",
    "\n",
    "📜 **예제**\n",
    "\n",
    "> iris 데이터에 min-max scaling을 수행한 뒤 분포를 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "MmScaler = MinMaxScaler()\n",
    "# Train 데이터의 fitting과 스케일링\n",
    "MmScaler.fit(X_train)\n",
    "X_train_sc = MmScaler.transform(X_train)\n",
    "# Test 데이터의 스케일링\n",
    "X_test_sc = MmScaler.transform(X_test)\n",
    "print('\\t\\t(min, max) (mean, std)')\n",
    "print ('Train_scaled (%.2f, %.2f) (%.2f, %.2f)'%(X_train_sc.min(), \n",
    "                                                 X_train_sc.max(), \n",
    "                                                 X_train_sc.mean(), \n",
    "                                                 X_train_sc.std()))\n",
    "print ('Test_scaled (%.2f, %.2f) (%.2f, %.2f)'%(X_test_sc.min(), \n",
    "                                                X_test_sc.max(), \n",
    "                                                X_test_sc.mean(), \n",
    "                                                X_test_sc.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔥 **꿀팁**\n",
    "- scaling은 수치형 컬럼들에 대해 적용\n",
    "- scaling은 이상치에 민감하기에 이상치 처리 후 수행\n",
    "- 분류분석일 경우 standard\n",
    "- 회귀분석일 경우 min-max\n",
    "\n",
    "## 5.5 차원 축소\n",
    "\n",
    "분석 모델을 구축하기 위해 다양한 데이터를 수집하게 됩니다. 이때, 목적이 되는 종속변수를 예측하기 위해 다양한 요인들을 모두 고려한다는 취지로 많은 독립변수를 사용하게되면 오히려 성능이 떨어지는 경우를 확인할 수 있습니다. 이는 차원의 저주라고 불리는 문제로 야기되며, 이를 해결하기위해 여러개의 컬럼을 몇 개로 축소하는 설명변수 선택 혹은 주성분 분석과 같은 차원축소 기법이 사용됩니다.\n",
    "\n",
    "📍 **차원의 저주**\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/sigirace/page-images/blob/main/kang_lectures/python_preprocessing/dimension.png?raw=true\" width=\"500\" height=\"350\"></p>\n",
    "\n",
    "- 컬럼이 늘어날수록 차원도 1차원, 2차원, 3차원으로 늘어남\n",
    "- 차원이 늘어날수록 데이터 간의 거리가 멀어지고, 이는 전체 영역에서 대한 데이터의 설명력이 줄어듦을 뜻함\n",
    "\n",
    "### 5.5.1 설명변수 선택\n",
    "- EDA에서 상관관계가 높았던 설명변수만을 사용 ☞ 해석이 용이함\n",
    "- 그러나 고차원적으로 갈 수록 데이터간의 상관관계를 고려하기 어려움\n",
    "\n",
    "### 5.5.2 주성분 분석(PCA)\n",
    "\n",
    "- 여러 컬럼을 사용하는 대신 이를 잘 나타내는 주성분들로만 선택하여 사용\n",
    "- 주성분 분석은 수치형 데이터에만 사용\n",
    "- 스케일 차이가 주성분에 영향을 주는것을 방지하기 위해 스케일링 수행\n",
    "- 주성분의 개수를 알아내는 방법은 분산 설명력의 누적 합을 보거나 scree plot의 꺾인점(=elbow point)을 확인\n",
    "- 분산 설명력의 누적합이 0.9 이상인 지점\n",
    "- scree plot에서 기울기가 급격히 감소하는 지점 직전\n",
    "\n",
    "📜 **예제**\n",
    "\n",
    "> iris 데이터셋에 대해 주성분 분석을 수행하여 차원을 축소해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris['Class'] = load_iris().target\n",
    "iris['Class'] = iris['Class'].map({0:'Setosa', 1:'Versicolour', 2:'Virginica'})\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치형 데이터만 추출\n",
    "x = iris.drop(columns = 'Class')\n",
    "# 수치형 변수 정규화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x = StandardScaler().fit_transform(x)\n",
    "pd.DataFrame(x).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA( n_components = 4) \n",
    "pca_fit = pca.fit(x)\n",
    "print('고유 값 : ', pca.singular_values_)\n",
    "print('분산 설명력: ', pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔬 **주성분 분석 결과해석**\n",
    "- 분산 설명력이 2번째 주성분까지 누적 0.9를 넘었으므로 주성분을 2개 선택함\n",
    "- 기존 4차원의 데이터를 2차원으로 축소\n",
    "\n",
    "📍 **분산 설명력**\n",
    "- 전체 데이터에서 각 주성분이 설명할 수 있는 분산의 비율\n",
    "- 누적하여 0.9 이상이면 해당 개수까지 주성분으로 선택\n",
    "\n",
    "📜 **예제**\n",
    "\n",
    "> Scree plot으로 주성분 개수 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.plot(pca.explained_variance_ratio_ , 'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔬 **scree plot 결과해석**\n",
    "- 그래프의 기울기가 2.0 지점에서 급격히 줄어들기 때문에 0, 1 두개를 주성분으로 선택할 수 있음\n",
    "\n",
    "📜 **예제**\n",
    "\n",
    "> iris 데이터에 대해 PCA를 수행하여 적절한 주성분 개수를 찾고, 전체 데이터를 주성분에 대해 시각화 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 객체 생성 (주성분 개수 2개 생성)\n",
    "pca = PCA(n_components = 2)\n",
    "# 2개의 주성분을 가진 데이터로 변환\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principal_iris = pd.DataFrame (data = principalComponents, columns =['pc1', 'pc2']) \n",
    "principal_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.title('2 component PCA' )\n",
    "sns.scatterplot (x = 'pc1', y = 'pc2', \n",
    "                 hue = iris.Class, \n",
    "                 data = principal_iris)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📍 **주성분 분석의 시각화 기능**\n",
    "\n",
    "- 주성분 분석은 차원의 저주를 해결하기 위해 컬럼을 줄이는 역할을 수행하는 것 이외로 시각화에 사용됨\n",
    "- 100개의 컬럼이 있는 데이터셋이면 100차원의 데이터 ☞ 우리는 3차원만을 시각화 할 수 있음\n",
    "- 따라서 주성분 분석을 통해 우리가 인지할 수 있는 차원으로 데이터를 줄여 시각화에 사용될 수 있음\n",
    "- 주성분 분석으로 차원이 줄어들었어도 분산 설명력에 의거해 전체 데이터를 잘 설명할 수 있음\n",
    "\n",
    "## 5.6 데이터 불균형 문제 처리\n",
    "\n",
    "데이터 불균형 문제는 클래스 분포가 극도로 불균형하여 일부 클래스가 다른 클래스에 비해 압도적으로 많은 데이터를 가지고 있을 때 발생합니다. 예를 들어, 이진 분류 문제에서 긍정 클래스(예: 사기 거래)가 전체 데이터의 1%를 차지하고, 부정 클래스(예: 정상 거래)가 99%를 차지하는 경우, 모델이 부정 클래스만 예측해도 높은 정확도를 보일 수 있습니다. 이는 모델이 소수 클래스의 패턴을 제대로 학습하지 못하게 하여 예측 성능이 저하됩니다. 즉, 데이터 불균형 문제는 모델의 예측 성능을 저하시킬 수 있는 중요한 문제입니다. 이를 해결하기 위해 다양한 기법을 활용할 수 있으며 이번 섹션에서는 그 중 sampling에 대해 다루어 보겠습니다.\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/sigirace/page-images/blob/main/kang_lectures/python_preprocessing/unbalance.png?raw=true\" width=\"400\" height=\"250\"></p>\n",
    "\n",
    "### 5.6.1 언더샘플링\n",
    "\n",
    "👀 **Definition**\n",
    "\n",
    "> 작은 클래스에 맞추어 전체 데이터를 감소하는 기법으로 불균형은 해결할 수 있으나 데이터가 적은 경우 학습 성능을 떨어뜨릴 수 있음\n",
    "\n",
    "📜 **예제**\n",
    "\n",
    "> 언더샘플링 수행 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# 언더샘플링을 위한 불균형 데이터 생성\n",
    "x, y = make_classification(n_samples=2000, n_features=6, weights=[0.95], flip_y=0)\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언더샘플링 수행\n",
    "undersample = RandomUnderSampler()\n",
    "x_under, y_under = undersample.fit_resample(x, y)\n",
    "print(Counter(y_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 오버샘플링\n",
    "\n",
    "👀 **Definition**\n",
    "\n",
    "> 다수 레이블에 맞춰 소수 레이블의 데이터를 증식시키는 방법으로 일반적으로 언더 샘플링보다 보통 유용함\n",
    "\n",
    "📜 **예제**\n",
    "\n",
    "> 오버샘플링 수행 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# 오버샘플링 수행\n",
    "oversample = RandomOverSampler() \n",
    "x_over, y_over = oversample.fit_resample(x, y)\n",
    "print(Counter(y_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔥 **꿀팁**\n",
    "\n",
    "- 데이터 불균형 문제는 분류 문제에 주로 사용됨 ☞ 종속변수에 대한 처리\n",
    "- 회귀 문제에서 범주형 데이터에 불균형이 있을 경우 사용할 수 있음 ☞ 설명변수에 대한 처리"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
