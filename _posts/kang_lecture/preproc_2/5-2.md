## 5.3 데이터 분할

**5.3.1 데이터 분할의 필요성과 train_test_split**

- 이제 데이터를 어떻게 분할하는지에 대해 알아보겠습니다.
- 결론부터 말씀드리자면 분석 모델을 만들 때 보유한 전체 데이터를 모두 학습에 사용하는 것은 적절하지 않습니다.
- 모델이 데이터를 암기하게 되어 일반화 성능이 저하될 수 있기 때문입니다.
- 따라서 전체 데이터를 학습용, 검증용, 그리고 테스트용으로 나누어 사용합니다.
- 이 과정을 학생의 학습에 비유하면, 학습용 데이터는 연습문제, 검증용 데이터는 모의고사, 테스트 데이터는 수능과 같은 실전 시험에 해당합니다.
- 모델은 학습 데이터를 통해 패턴을 학습하고, 검증 데이터를 통해 성능을 조정하며, 마지막으로 테스트 데이터를 통해 실제 성능을 평가하게 됩니다.
- 일반적으로 전체의 60%를 학습용으로, 20%를 검증 및 테스트용으로 설정하는 경우가 많습니다.
- 이처럼 데이터를 적절히 분할하는 과정은 모델 성능을 객관적으로 검증하는 데 매우 중요한 절차입니다.

**5.3.2 train_test_split 함수로 데이터 나누기**

- 데이터 분할을 수행하기 위해 Python에서 데이터를 분할하는 대표적인 함수인 train_test_split에 대해 알아보겠습니다.

[코드]

- stratify는 동일 비율로 나눌 컬럼이라는 뜻인데 뒤에 층화추출에 대한 내용과 함께 다시 살펴보도록 하겠습니다.

- 다음은 예시입니다.
- 예시로는 앞서 사용하였던 iris 데이터를 학습과 테스트용으로 나누어볼것인데요 검증용 데이터는 고려하지 않고 일단 8대 2의 비율로 나누어 보겠습니다.

[코드]

**더 알아보기**

- 더 알아보기입니다. 앞서 살짝 설명드렸지만, 데이터가 많으면 많을수록 학습에 좋은데 왜 굳이 테스트 데이터를 빼놓고 학습을 수행하냐, 수능 데이터까지 공부하면 좋지 않느냐 라는 질문이 있을 수 있을것 같습니다.
- 이는 과적합, 오버피팅에 대한 문제를 야기할 수 있는데요.

[읽기]

- 비유를 하자면 우리의 학생이 컨닝을 했기 때문에 결국 인생에서 일부분인 수능점수만 높고, 이후 대학교에서 그리고 사회에 나와서 공부를 못하게 된다라고 보시면 됩니다.
- 더하여 언더피팅의 개념은 이와 반대로 어떤 환경에서도 모델의 성능이 나쁜 것으로 생각하시면 됩니다.



**더 알아보기**

- 이제 우리는 데이터를 원하는 비율로 분할하는 방법을 알았습니다.
- 그런데 이때 분할되는 데이터는 랜덤으로 분할되기에 최악, 즉 워스트 케이스가 발생할 수 있습니다.
- 예시로 앞서 범주형 데이터를 보았을 때, 전체 클래스의 비율은 균형을 이루고 있었지만, 무작위로 분할한 결과 특정 클래스가 한쪽에 쏠리거나 심지어 완전히 사라지는 현상이 발생할 수 있습니다.
- 이처럼 데이터가 불균형하게 분할될 경우, 분석 모델은 특정 클래스에 대해 충분히 학습하지 못하거나 성능을 왜곡되게 평가받게 됩니다.
- 이러한 문제를 해결하기 위해 사용하는 방식이 바로 **층화추출(stratified sampling)**입니다.

[5:00]

**5.3.3 데이터 분할 시 주의할 점과 계층 추출**

- 층화추출은 전체 데이터의 클래스 비율을 고려하여, 훈련 데이터와 테스트 데이터 모두에서 **원본과 유사한 클래스 분포를 유지**하도록 데이터를 나누는 방법입니다.
- 다시 말해, 특정 클래스가 전체 데이터의 1/3을 차지하고 있었다면, 분할 이후에도 훈련과 테스트 세트 모두에서 해당 클래스가 1/3 정도 포함되도록 설정하는 것입니다.
- 층화추출에 대한 적용을 위해서는 앞서 살펴본 파라미터중 stratify에 원하는 특성, 즉 컬럼을 인풋으로 작성하면 됩니다.
- 예제를 통해서 알아보도록 하겠습니다.

- 살펴볼 예제는 임의 데이터의 분할과 층화 추출에 대한 분할 두가지에 대한 비교입니다.
  - 코드 읽기

[2:00 / 7:00]

- 정리하자면 데이터 분류는 모델 학습을 위해 데이터를 준비하는 과정으로 생각하시면 됩니다.
- 이때의 꿀팁은 데이터 분할을 수행하기 전에는 전체 데이터의 클래스 분포를 먼저 확인하는 것이 좋습니다.
- 클래스 간 데이터 수가 지나치게 불균형할 경우, 무작위 분할은 결과적으로 학습 성능을 크게 저하시킬 수 있습니다.
- 특히 데이터의 크기가 작거나 일부 클래스가 희귀할 경우에는 계층 추출이 필수적으로 고려되어야 합니다.

- 더하여 한 가지 더 말씀을 드리자면,
- 실제 현업에서는 데이터가 부족한 상황이 매우 자주 발생합니다.
- 원칙적으로는 전체 데이터를 학습용 60%, 검증용 20%, 테스트용 20%로 나누는 것이 이상적입니다.
- 하지만 현실에서는 검증용 데이터 20%조차 아쉬운 경우가 많아, 보다 실용적인 접근을 선택하는 경우가 있습니다.
- 예를 들어, 처음에는 60%의 데이터를 사용해 모델을 학습시키고, 20%의 데이터를 검증용으로 활용하여 성능을 조정합니다.
- 이후 검증 과정이 완료되면, 검증 데이터와 테스트 데이터를 합쳐 총 80%의 데이터를 다시 학습에 활용하는 방식입니다.
- 이렇게 하면 모델이 더 많은 데이터로 학습할 수 있어 성능이 개선되는 효과를 기대할 수 있습니다.
- 따라서 이후 과정에서 60:20:20의 구조 대신, 80:20으로 단순하게 학습과 테스트 데이터만을 나누어 사용하는 경우가 있더라도,
- 이는 **데이터의 양이나 목적에 따라 달라지는 실용적인 판단**으로 이해하시면 되겠습니다.
- 특히 초기에 모델을 실험하거나, 데이터 확보가 어려운 프로젝트에서는 이러한 방식이 더 자주 활용되는 경향이 있습니다.
- 결론적으로 데이터 분할 비율은 상황에 따라 조정할 수 있으며, 중요한 것은 어떤 방식이 **현재 데이터와 문제 상황에 가장 적합한지**를 판단하는 것입니다.

[3:00/ 10:00]

## 5.4 데이터 스케일링

- 이번에는 데이터 스케일링에 대한 내용을 학습해 보겠습니다.
  - 읽기

- 우리는 이번 강의를 통해 데이터 스케일링 중 대표적인 두가지인 스탠다드 스케일러와 민맥스 스케일러에 대해 학습해 볼 것입니다.
- 이때 스케일링을 적용하는 데이터에 대해서도 주의 깊게 살펴보아야 합니다.
- 이 부분은 데이터 분석에 대해 이론적으로 학습한 후, 실제 적용 단계에서 가장 자주 간과되는 부분 중 하나입니다.
- 먼저 스케일링은 분석 모델이 편향을 갖지 않도록 조정하는 데 목적이 있다는 점을 다시 강조드리고자 합니다.
- 따라서 스케일링된 데이터를 사용하여 모델을 학습시킬 경우, 테스트 데이터나 실제 분석 환경에서 사용되는 데이터 역시 **동일한 기준으로 변환된 상태에서** 예측에 사용되어야 합니다.
- 그렇지 않으면 모델이 학습한 기준과 전혀 다른 범위의 데이터를 입력받게 되어, 모델의 해석력을 저하시키게 됩니다.
- 특히 데이터의 평균과 표준편차를 기준으로 스케일링을 수행하는 스탠다드 스케일러의 경우, 테스트 데이터에 대해 별도로 평균이나 표준편차를 다시 계산하는 것은 바람직하지 않습니다.
- 따라서 훈련 데이터를 기준으로 스케일링 수행하고, 그 기준을 **테스트 데이터에 그대로 적용**하는 방식이 정확한 절차입니다.
- 또한 데이터를 분할하는 시점과 스케일링을 적용하는 시점에 있어서도 순서에 유의해야 합니다.
- 보통 데이터를 훈련용과 테스트용으로 나눈 후에 스케일링을 적용해야 하며, 데이터 전체를 한 번에 스케일링한 뒤 분할하게 되면 정보 누수가 발생할 수 있습니다.
- 이는 분석 모델이 테스트 데이터에 대한 일부 정보를 사전에 학습하게 되는 문제로 이어져, 평가의 공정성이 훼손될 수 있습니다.
- 즉, 앞서 데이터셋을 분할할때 설명드린 것처럼 학생이 미리 수능 문제를 보고 학습을 해서는 안되는 것과 동일하다고 생각하시면 됩니다.
- 정리하면, 스케일링은 단순히 수치의 크기를 조정하는 기술적 과정이 아니라, 데이터 분석의 정확성과 신뢰도를 유지하기 위한 전처리 절차로 이해할 필요가 있습니다.

**5.4.1 Standard Scaler**

- 먼저 앞서 말씀드린 **데이터 스케일링 두가지중 하나인 Standard Scaler**에 대해 알아보겠습니다.
- 수식적인 설명은 제외했으나, 스케일러를 적용하면 각 특성(컬럼)의 평균이 0이 되도록 이동시키고, 표준편차가 1이 되도록 크기를 조정하는 연산이 수행됩니다.
- 이 과정을 거치면 변수들의 단위가 달라도 모두 **정규분포와 유사한 형태로 맞추어지며**, 모델이 모든 변수에 대해 동일한 조건에서 학습을 수행할 수 있게 됩니다.

- 예제를 통해 스탠다드 스케일링의 과정을 살펴보도록 하겠습니다.
- 앞서 말씀드렸듯, 스케일링의 적용시점을 고려해서 먼저 학습과 평가 데이터를 8:2 비율로 나눈 뒤 학습데이터에 대해 스케일러를 생성하고, 생성한 스케일러를 테스트 데이터에 적용하는 방식으로 수행하도록 하겠습니다.
  - 코드 읽기

- 코드 수행결과 학습 데이터의 평균과 표준편차가 스케일러를 통해 0과 1로 맞춰진 것을 확인할 수 있습니다.
- 또한 테스트 데이터에는 약간의 차이가 있겠지만 이와 유사한 수준으로 변환된 것을 확인할 수 있습니다.
- 그 이유는 우리가 생성한 스탠다드 스케일러는 학습데이터만을 초점으로 만들어진 모델이기에 새로운 테스트 데이터에 대해서는 약간의 오차가 발생했다고 보시면 됩니다.
- 즉, 스탠다드 스케일러라는 학생이 학습데이터로 열심히 공부한 뒤에 수능 문제인 테스트 데이터에 대해 문제를 풀었기에 정확히 답을 다 맞추지 못했다 라고 보시면 되는데요.
- 학생이 수능문제를 모르는 것처럼 스탠다드 스케일러도 미래에 들어올 데이터를 알 수 없다라는 관점으로 생각하시면 타당한 결과이며 옳은 결과이다 라고 보시면 될 것 같습니다.

[7:30 / 17:30]

**5.4.2 Min-max Scaler**

- 이번에는 **데이터를 0과 1 사이의 범위로 조정하는 스케일링 기법인 Min-max Scaler**에 대해 알아보겠습니다.
- 앞서 스탠다드 스케일러는 값의 범위를 평균이 0 표준편차가 1으로 변경을 했다면
- Min-max 스케일링은 이름 그대로 각 변수의 최소값은 0, 최대값은 1이 되도록 데이터를 변환하는 방식입니다.
- 이 방법은 변수 간 단위가 다르거나, 값의 절대 크기에 민감한 모델을 사용할 때 특히 유용하게 활용됩니다.
- 즉, Min-max Scaler는 **절대적인 최소값과 최대값을 기준으로 데이터를 재조정**합니다.
- Min-max 스케일러도 예시를 통해 살펴보도록 하겠습니다.

[코드]

- 그 결과 최대와 최소가 원본 데이터와는 다르게 1과 0으로 피팅됨을 확인할 수 있습니다. 

**꿀팁**

- Min-max 스케일링을 다시 정리하자면 데이터의 범위를 최대가 1이되게 최소가 0이되게 피팅하는 것입니다.
- 이로인해 Min-max 스케일링은 **이상치(outlier)**에 민감하게 반응하는 성질이 있습니다.
- 따라서 데이터에 이상치가 포함되어 있을 경우, 스케일의 기준이 크게 왜곡될 수 있습니다.
- 이런 경우에는 이상치를 먼저 탐지하고 처리한 후에 스케일링을 적용하는 것이 바람직합니다.
- 만약 이상치가 많거나 분포가 매우 비대칭적인 경우에는, Robust Scaler와 같은 다른 기법을 고려하는 것도 좋은 대안이 될 수 있습니다.

**비교 정리**

- 마지막으로 두 가지 스케일러의 차이와 각각을 언제 사용하는 것이 좋은지 정리하며 마무리하겠습니다.
- 먼저, 사용하는 모델을 기준으로 **선형 모델**과 **비선형 모델**로 나눌 수 있습니다.
- **선형 모델**은 각 변수에 일정한 가중치를 곱해서 더한 값으로 결과를 예측합니다.
- 다시 말해, 변수와 결과 사이의 관계가 **직선처럼 단순하게 연결되는 모델**입니다.
- 이런 모델은 변수마다 크기나 단위가 다르면 결과에 영향을 줄 수 있기 때문에, **모든 변수를 평균 0, 표준편차 1로 맞추는 Standard Scaler**가 잘 어울립니다.
- **비선형 모델**은 변수와 결과 사이의 관계가 직선이 아니라 **곡선이나 복잡한 함수로 연결되는 경우**입니다.
- 이런 모델은 입력값이 지나치게 크거나 작으면 학습이 어려워질 수 있으므로, **입력값을 0에서 1 사이로 정리해주는 Min-max Scaler**가 더 안정적입니다.
- 따라서 **분류 모델**에서는, **Logistic Regression이나 Linear SVM**처럼 계산 방식이 선형적인 모델에는 Standard Scaler가 적합하고,
- **인공신경망**처럼 비선형 구조를 가진 모델은 Min-max Scaler를 사용하는 것이 좋습니다.
- **회귀 분석**의 경우에도, **선형 회귀나 릿지 회귀**처럼 결과를 직선으로 설명하려는 모델에는 Standard Scaler가 잘 맞습니다.
- 반면, **결정트리나 랜덤포레스트처럼 비선형 구조인 회귀 모델**은 스케일링을 하지 않아도 성능에 거의 영향을 받지 않습니다.
- 정리하자면, 어떤 스케일러를 사용할지는 **모델이 데이터를 어떻게 계산하는지**에 따라 달라지며,
- 모델 구조에 맞는 스케일링을 적용하는 것이 모델의 성능을 높이는 데 중요한 역할을 합니다.
- 다만 이번 강의는 선형 모델 비선형 모델에 대한 자세한 얘기는 다루지 않고 단순히 전처리에 대한 내용이기에
- 교재에 적힌 것처럼 분류 분석일때는 일반적으로 스탠다드 스케일러
- 회귀분석일때는 민맥스 스케일러로 단순히 접근하여도 무방합니다.

[5:00 / 22:30]



## 5.5 차원 축소

- 이번에는 분석 모델을 구축할 때 자주 활용되는 기법인 **차원 축소**에 대해 알아보겠습니다.
- 실제 데이터를 수집하다 보면 수많은 변수들을 다뤄야 하는 경우가 생깁니다. 특히 독립변수가 많아질수록, 이 변수들 사이의 상관관계나 데이터의 구조를 파악하기 어려워지며 분석 정확도가 떨어지는 현상이 나타날 수 있습니다.
- 이는 우리가 앞서 배운 원핫 인코딩을 생각하시면 편하실텐데요.
- 우리는 원핫인코딩을 수행하게되면 라벨에 있는 중복을 제거한 데이터의 수만큼 컬럼이 늘어나는 것을 확인했습니다.
- 만약 중복을 제거하고 독립적인 원소가 100개 이상일 경우에는 컬럼이 100개가 늘어나게 되게 되구요.
- 이럴 경우 데이터를 모델의 인풋으로 넣을 수 있도록 바꾸었지만 모델이 해석하기 어려워지게 되는 것이지요.
- 이러한 상황에서 사용하는 것이 바로 차원 축소 기법입니다. 
- 이는 불필요하게 많은 변수를 줄여서, 중요한 정보만 남기는 방법이라고 이해하시면 좋겠습니다.

**차원의 저주란?**

- 여기서 한 가지 용어를 먼저 짚고 넘어가야 하는데요, 바로 **차원의 저주(Curse of Dimensionality)**입니다.
- 데이터의 차원이 증가하면 변수 간의 거리가 멀어지고, 이로 인해 데이터가 희소하게 분포하게 됩니다.
- 그림의 예시를 보면 이해가 더 편하실 거라고 생각이 듭니다.
- 그림을 보자면 데이터가 1차원인 x축만 있을때, 그리고 2차원 3차원으로 늘어나 y, z 축이 생겼을때 데이터가 점점 멀어지는 것을 확인할 수 있습니다.
- 컬럼을 데이터의 특성으로 보자면, 데이터 분석을 위해 기존에는 하나의 컬럼만 존재하였는데, 분석의 정확도를 위해 다른 여러 컬럼을 하나씩 수집하였다고 보겠습니다.
- 그런데 분석 모델에게 더 많은 정보를 전달하기 위해 컬럼을 추가하였음에도 불구하고 분석모델이 봐야할 전체적인 데이터의 공간은 커지게 된 것이죠.
- 이 예시에서는 단순히 3차원으로만 표현했지만 앞서 원핫인코딩의 경우 차원이 100차원이 넘어가게된다거나 하게 되면 모델이 해석해야할 공간의 크기가 매우 커진다 즉, 차원의 저주로 인해 모델의 해석력이 떨어질 수 있다고 볼 수 있습니다.
- 다시 말해, 차원이 많아질수록 데이터 간의 거리가 멀어지고, 거리가 멀어질수록 전체 공간에서의 데이터 밀도는 낮아지게 되기에, 전체 영역에 비해 실질적으로 사용 가능한 정보는 오히려 적어지는 문제가 발생합니다.
- 이 문제로 인해 모델의 성능이 떨어질 수 있기 때문에, 적절한 차원 축소는 매우 중요하게 작용할 수 있습니다.

[2:30 / 25:00]

**5.5.1 설명변수 선택**

- 차원 축소에는 두 가지 큰 방향이 있습니다.
- 그 중 첫 번째는 **설명변수 선택(feature selection)**입니다.
- 이는 원래의 변수들 중에서 중요한 것들만 골라서 사용하는 방법인데요, EDA 단계에서 상관관계를 보면서 유의미한 변수만을 선택하는 것이 대표적인 예입니다.
- 다만 고차원, 즉 변수가 많아질수록 변수 간에 다중공선성이 존재하거나 복잡한 상관관계를 고려해야 할 필요가 있는 경우에는 이 방법이 적절하지 않을 수 있습니다.
- 그래서 더욱 강력한 방식으로 소개되는 것이 **주성분 분석(PCA)**입니다.

**5.5.2 주성분 분석(PCA)**

- 주성분분석, PCA는 기존 변수들을 단순히 제거하는 것이 아니라, **새로운 축을 정의해서 데이터를 투영**하는 방식입니다.
- 원래의 데이터를 회전시켜 가장 많은 분산을 설명할 수 있는 방향으로 새로운 좌표계를 만드는 것이 핵심입니다.
- 이때 생성된 새로운 축들을 **주성분(Principal Components)**라고 부릅니다.
- 말이 참 어려우니 차근차근 알아가보도록 하겠습니다.

- 먼저 분산은 정보량을 나타내는 지표입니다.
- 데이터에서 분산이 크다는 것은, 데이터가 더 많이 퍼져있다는 뜻이겠죠?
- 많이 퍼져있다는건 데이터를 구별짓거나 다양하게 만들수 있는 정보가 많다는 것을 의미합니다.
- 우리가 계속 사용하는 iris 꽃 데이터를 예시를 들어보겠습니다.
  - 만약 꽃잎 길이에 대한 데이터가 모두 4.9에서 5.1cm 로 모여있다면 이 꽃잎 길이는 품종을 구분하는데 거의 도움이 되지 않을 것입니다.
  - 하지만 길이가 1.2부터 6cm 까지 아주 다양하게 퍼져있다면 품종마다 길이가 다를 수 있고 어떤 다른 특성과 연계되어서 품종을 구분할 수 있는 단서가 될 수 있을것입니다.
  - 즉, 어떤 데이터의 분산이 클 경우 데이터를 구분할 수 있는 가능성이 커지게 되는것이지요

- 따라서 PCA는 우리가 가지고 있는 데이터를 대상으로 기존 축이아닌 분산이 가장 커지게 되는 새로운 축을 생성하여 설명력을 높인다는 관점으로 생각하시면 됩니다.
- 이를 영상을 통해 확인해보도록 하겠습니다.

- 먼저 데이터를 쉽게 보기 위해 **데이터의 중심을 원점(0,0)으로 이동시켰습니다**.
- 이 상태에서 각 축을 기준으로 데이터를 **투영**해 보면,
- x축 방향으로는 데이터가 비교적 넓게 퍼져 있는 반면,
- y축 방향으로는 데이터가 오밀조밀하게 모여 있는 것을 확인할 수 있습니다.
- 이 말은 곧, **x축 방향이 분산이 크고**, **y축 방향은 분산이 작다**는 것을 의미합니다.
- 따라서 데이터를 설명할 수 있는 힘, 즉 **설명력은 x축이 더 크다고 볼 수 있습니다.**

- 그렇다면 PCA에서는 어떤 축을 먼저 선택할까요?
- 바로, **데이터의 분산이 가장 큰 방향**,
- 즉 데이터를 가장 넓게 퍼지게 하는 방향을 먼저 선택합니다.
- 이 방향이 바로 첫 번째 주성분인 **PC1**입니다.
- PC1을 기준으로 데이터를 보면, 원래 두 차원이었던 데이터를 하나의 축으로 투영시켜도
- **데이터의 구조나 패턴이 상당 부분 유지**된다는 것을 알 수 있습니다.
- 또한 x축으로 투영시켰을 때보다 더 분산이 크다라는 것도 직관적으로 알 수 있죠.
- 하지만, PC1 하나만으로는 데이터의 모든 정보를 담아내기엔 부족할 수 있습니다.
- 그래서 **두 번째로 중요한 축**, 즉 **PC2**를 선택하게 됩니다.
- 여기서 중요한 점은, **PC2는 반드시 PC1과 직각을 이루어야 한다는 조건이 있다는 것**입니다.
- 왜냐하면 주성분들끼리는 서로 **중복되지 않는 독립적인 정보만 담아야 하기 때문**입니다.
- 이렇게 서로 중복되지 않는 독립적인 정보만 담은 방향을 통계 용어로는 아이겐 벡터라고 보시면 되구요
- 앞서 말씀드린 직교, 직각을 이룬다라는 것이 유닛벡터라고 보시면 됩니다.
- 따라서 PC2는 **PC1에 직교하면서**, 그 조건 하에 **남아 있는 분산을 가장 잘 설명할 수 있는 방향**으로 설정됩니다.
- 이처럼 PCA는 먼저 데이터를 가장 잘 설명하는 방향을 찾아 PC1으로 삼고,
- 그다음에는 PC1과 **독립적이면서도 여전히 많은 정보를 담을 수 있는 방향**을 찾아 PC2로 삼습니다.

- 이 과정을 계속 반복하면서, 필요에 따라 **PC3, PC4…** 같은 다음 주성분도 결정할 수 있습니다.

- 이렇게 결정된 각 주성분들은 원래 데이터 공간에서 **회전된 새로운 좌표계**라고 생각하시면 됩니다.

- 원래의 축(x, y)이 아니라, 데이터의 **구조를 가장 잘 드러낼 수 있는 방향으로 재정렬된 축**이죠.

- 최종적으로 우리는 이 중에서 **몇 개의 주성분만 선택해서** 데이터를 표현할 수 있습니다.

- 예를 들어, PC1과 PC2만으로도 전체 데이터를 90% 이상 설명할 수 있다면,

- **고차원 데이터를 2차원으로 줄이면서도 중요한 정보는 거의 다 유지**할 수 있는 것입니다.

- 이와 같이 PCA는 데이터의 분산을 기준으로, **서로 독립적이고 정보가 풍부한 축들을 순차적으로 선택해 나가며** 

- 차원을 줄이되, 중요한 정보는 최대한 보존하려는 목적을 갖고 있습니다.

  [영상끝]

- 일단 이정도의 설명을 하고 예제를 확인한 후 다시 설명을 이어가도록 하겠습니다.

[7:30 /  32:30]

- 예제는 iris 데이터에 대해 주성분 분석을 수행하고 차원을 축소해보는 것인데요.
- 먼저 계속 봐온 데이터를 다시한번 확인해 보도록 하겠습니다.
  - 코드
- 우리가 가진 수치형 데이터가 4개임을 확인할 수 있죠.
- 뭐 사실 이정도는 큰 문제 없지만 주성분분석을 학습해보자라는 취지로 차원을 줄어보도록 하겠습니다.
- 즉, 피처가되는 컬럼의 개수를 4개에서 더 줄이는 과정을 수행해 보도록 하겠습니다.

[코드]

- 우리가 앞서 배운 스탠다드 스케일러는 평균을 0 표준편차를 1로 만드는 스케일링 기법이었죠
- 그런데 영상에서 보면 데이터를 원점 기준으로 이동시키는 것을 확인할 수 있었습니다.
- 이는 연산시에 축을 찾기 쉽게 하기위해 사용하는 트릭 정도로 보시면 되는데요
- 이 과정을 스탠다드 스케일러로 대신하였다 라고 보시면 됩니다.
- 따라서 PCA를 수행할때는 스탠다드 스케일링 과정이 먼저 수행되어야 한다 정도로 기억해 주시면 좋을 것 같습니다.

[코드]

- 수행 결과로 고유값과 분산 설명력을 나타내고있는데요
- 여기서 이 분산 설명력이 앞서 말씀드린 새로운 축에 대한 데이터의 설명력입니다.
- 축을 1개로 설정했을때는 전체 데이터를 0.72만큼 설명할 수 있고
- 축을 2개로 늘렸을때는 0.72에 0.22를 더한만큼 즉 0.94를 설명할 수 있으며
- 축이 원래와 같은 4개일때는 데이터 전체를 설명할 수 있다라는 의미가 되는 것이지요.
- 이는 즉, 변수를 4개에서 2개로 줄여도 전체 데이터를 90퍼센트 이상 설명할 수 있다는 말로 해석할 수 있고,
- 일반적으로 분산 설명력이 0.9 이상이면 해당 개수까지 주성분으로 선택할 수 있다고 보기에 기존 4차원 데이터를 2차원으로 줄여도 괜찮다라고 보시면 됩니다.
- 물론 차원이 4개정도면 모델을 위해 PCA를 굳이 수행할 필요는 없긴 하지만요.
- 여기서 포인트는 모델을 위해서인데, 이는 바로 뒤에 다시 한번 PCA의 활용과 연계하여 말씀드리도록 하겠습니다.

- PCA를 수행하여 주성분의 개수를 구하는 방법은 앞서 분산설명력을 통해 수치로 구하는 것 외에
- 시각적으로 판단하여 구하는 방법이 존재합니다.
- 이를 scree plot이라고하는데요.
- 이 또한 예시를 통해 살펴보도록 하겠습니다.

[코드]

- 코드는 별건 아니고 그냥 앞서 구한 분산설명력을 꺽은선 그래프로 나타낸 것인데요.
- 여기서 꺽은선의 기울기가 완만해지는 시점이 눈에 보이며, 바로 이 지점을 찾아 해당 이전까지만 주성분을 선택합니다.
- 이를 **elbow point**라고 부르는데, 현 예시에서는 2라고 보이는데요
- 그래프의 기울기가 2.0 지점에서 급격히 줄어들기에 이 직전인 0과 1 두개를 주성분으로 선택할 수 있다라고 해석하시면 됩니다.
- 사실 앞에 수치로 해석하는 것과 크게 차이는 없지만 분석을 시각화하여 보여줄 수 있다는 것은 또 장점이 될 수 있으니 활용하시면 있어빌리티가 생길 것이라고 생각합니다.

[4:00 / 36:30]

**PCA 결과 시각화**

- PCA의 마지막은 시각화입니다.
- PCA는 앞서 모델을 위해 차원을 줄이는 목적 외에도 데이터의 형태를 시각적으로 파악하기 위해서 주로 사용합니다.
- 멀리 갈 것도 없이 우리가 사용하는 iris 데이터에 대해 상상을 해보겠습니다.
- 우리의 데이터 셋에서 수치형 컬럼은 4개였습니다.
- 그런데 3차원 이상의 데이터, 즉, 컬럼이 세개 이상인 데이터를 보기 좋게 시각화하는 방법은 없습니다.
- 사실 4차원도 시각적으로 표현하는 방법이 있긴한데 매우 난해하기에 분석에는 적합하지 않습니다.
- 따라서 가지고 있는 데이터가 4개의 컬럼 이상으로 되어있는데, 이를 시각적으로 좀 파악해서 보고싶을때 PCA를 사용합니다.
- PCA를 사용하면 데이터의 설명력은 유지하면서 이를 3차원 이하로 줄여서 표현할 수 있기 때문이지요.
- 이에 대해 예제를 통해 살펴보도록 하겠습니다.

- 앞서 우리는 iris 데이터 셋에 대해 차원이 2개
- 즉, 2개의 주성분 만으로 전체 데이터의 90%이상을 설명할 수 있음을 확인하였으니
- 데이터를 2차원으로 줄이는 PCA를 수행해 보도록 하겠습니다.

[코드]

- 그리고이 2차원으로 줄인 데이터에 원본 라벨을 인덱스 기준으로 붙여 시각화를 해보면
- 이 그림과 같이 기존에 고차원이라 확인할 수 없었던 데이터들이 눈에 쉽게 들어오는 것을 볼 수 있습니다.
- 게다가 주성분 두개로 데이터가 어느정도 잘 분리되는 모습을 확인할 수 있기에
- 이를 모델링에 활용한다면 아주 잘 데이터가 분류될 수 있음을 기대할 수 있을것 같네요.


**PCA의 활용 가치**

- 마지막으로, PCA의 실제 활용 가치에 대해 정리하겠습니다.
- 첫째, 분석의 효율성을 높일 수 있습니다. 변수 수가 줄어들면 학습 속도도 빨라지고, 과적합 위험도 줄어듭니다.
- 둘째, 시각화가 가능해집니다. 복잡한 고차원 데이터를 2~3차원 공간에 투영하여 시각적으로 이해할 수 있습니다.
- 셋째, 노이즈 제거 효과도 기대할 수 있습니다. 중요한 축을 중심으로 데이터를 표현하다 보니, 의미 없는 노이즈가 자연스럽게 제거되는 효과가 있습니다.
- 다만 주의할 점은, PCA를 수행할때 반드시 스케일링 과정을 거쳐야 하는 것입니다.
- 또한 만들어진 주성분은 원래의 변수와 직접적인 해석이 어렵기에 변수의 해석보다는 패턴 분석과 구조 파악에 초점을 둘 때 더 적합한 기법이라고 볼 수 있습니다.

[3:00 / 40:00]





## 5.6 데이터 불균형 문제 처리

- 이번 시간의 마지막으로는 **데이터 불균형 문제를 처리하는 방법**에 대해 알아보겠습니다.
- 데이터 불균형이란, 특정 클래스가 다른 클래스에 비해 **월등히 많은 데이터 수를 가지는 상황**을 말합니다.
- 예를 들어, 금융 사기 탐지처럼 사기 거래는 전체의 1%만 차지하고, 정상 거래가 99%인 경우를 생각해볼 수 있습니다.
- 이처럼 일부 클래스에 데이터가 편향되면, 모델은 주로 다수 클래스의 패턴만 잘 학습하게 되고, 소수 클래스는 잘 예측하지 못하게 됩니다.
- 즉, 새로운 데이터에 대한 예측을 수행할 때, 모델이 정상거래에 편향되어있기에 실제 사기 거래 데이터가 들어오건, 정상거래가 들어건 정상거래라고 예측 결과를 나타낼 것입니다.
- 하지만 향후 들어올 데이터도 사기거래가 약 1%정도만 차지하고 대부분은 정상거래 데이터가 들어올 것이기에 모델의 전체 성능은 99%의 정확도를 가졌다 라고 잘못 해석될 수가 있습니다.
- 따라서 전체적인 정확도는 높게 나와도 **정작 중요한 예측 성능은 떨어질 수 있는** 문제가 발생하는 것이죠.
- 이런 경우에는 소수 클래스를 더 잘 예측할 수 있도록, **데이터를 재구성하거나 샘플링을 조정하는 작업이 필요**합니다.
- 이 섹션에서는 대표적인 방법인 **언더샘플링과 오버샘플링**을 중심으로 살펴보겠습니다.

[1:30 / 41:30]

**5.6.1 언더샘플링**

- 먼저 **언더샘플링** 기법에 대해 알아보겠습니다.
- 언더샘플링은 말 그대로 **다수 클래스를 줄이는 방식**입니다.
- 즉, 데이터 수가 많은 쪽을 일부만 선택해서, 소수 클래스와 **비슷한 수준으로 맞춰주는 방식**이죠.
- 이렇게 하면 불균형 문제를 어느 정도 완화할 수는 있지만, 반대로 중요한 데이터를 제거하는 경우도 생길 수 있기 때문에 **성능이 저하될 가능성**도 염두에 두어야 합니다.

- 예를 들어, 전체 데이터가 2000개이고, 이 중 95%가 클래스0,  5%가 클래스 1이라고 가정하겠습니다.
- 언더샘플링을 수행하면, 클래스 0에 해당하는 1900개 중에서 임의로 100개만 선택하고, 클래스 1의 100개와 합쳐서 총 200개로 학습을 진행하게 됩니다.
- 이렇게 하면 **균형잡힌 데이터셋이 되기는 하지만, 원래 갖고 있던 정보의 대부분을 버리는 셈**이기 때문에 신중하게 선택해야 합니다.

- 언더샘플링은 다음과 같은 상황에서 고려해볼 수 있습니다:
  - 데이터의 총량이 너무 많아서 처리 비용이 클 때
  - 다수 클래스 내의 정보가 비교적 중복되어 있을 때
  - 빠른 실험이나 가벼운 모델링을 원할 때

- 다만 중요한 건, 학습 성능이 떨어질 수 있다는 점을 충분히 고려하고, **실제 결과를 평가하면서 적절히 조정해주는 것**이 중요합니다.

- 사실 실제 분석 과제에서 데이터는 하나하나가 너무 소중하기 때문에 이를 날려버리는 언더샘플링은 거의 고려가 되지 않고, 뒤에 나올 오버샘플링을 주로 사용하곤 합니다.

- 어쨌든, 예시를 통해 알아보도록 하겠습니다.

[코드]

- 먼저 샘플링을 적용할 데이터셋을 만들어볼건데요
- 데이터의 수는 2000개 컬럼은 6개인 데이터셋이며 클래스의 비율은 0과 1이 약 95:5로 매우 불균형적인 데이터입니다.

[코드]

- 그리고 언더샘플링을 진행해 주는데 정말 쉽습니다.
- 단순히 undersample을 불러와 사용해주면되는데
- 이때 주의할 점은 독립변수와 종속변수를 구분해주어야한다는 점 정도밖에 없습니다.
- 그 결과는 수가 적은 클래스에 많은 클래스가 맞춰진 모습을 볼 수 있네요

[2:30 /  43:00]

**5.6.2 오버샘플링**

- 다음으로는 **오버샘플링** 기법에 대해 알아보겠습니다.
- 오버샘플링은 언더샘플링과 반대로, **소수 클래스의 데이터를 인위적으로 늘리는 방식**입니다.
- 이 방법은 **다수 클래스는 그대로 두고, 소수 클래스 쪽만 복제하거나 생성해서 수를 맞추는 방식**입니다.
- 앞서 본 예시에서 소수 클래스의 데이터가 100개였다면, 오버샘플링을 통해 이 100개를 복제하여 1900개 수준까지 증가시킵니다.
- 이렇게 하면 데이터 손실 없이 균형을 맞출 수 있어, 일반적으로는 언더샘플링보다 더 안정적인 성능을 기대할 수 있습니다.
- 특히 데이터 수가 적은 상황에서는 **모델이 패턴을 학습하기에 부족할 수 있기 때문에**, 오버샘플링은 매우 유용하게 쓰입니다.
- 단점은, 같은 데이터를 복제하다 보니 앞서 잠깐 설명드린 **과적합(overfitting)**이 발생할 수 있다는 점인데요,
- 이를 보완하기 위해 단순 복제 대신, **SMOTE**라는 기법을 사용해 원본 데이터 근처에 새로운 데이터를 생성하는 방식도 사용될 수 있습니다.
- 그래서 오버샘플링은
- 소수 클래스의 데이터 수가 너무 적어 학습이 어려운 경우
- 원본 데이터 손실 없이 불균형만 해결하고 싶은 경우
- 에 적합합니다

- 예시 코드도 언더샘플링과 동일합니다.
- 다만 그 결과가 많은 샘플을 기준으로 데이터가 늘어났다 정도로만 보시면 될 것 같습니다.



**정리하며**

- 마지막으로 정리를 하지면 불균형 데이터 문제는 **모델 성능에 큰 영향을 줄 수 있는 중요한 이슈**입니다.

- 언더샘플링은 빠르게 균형을 맞추는 방법이지만, 데이터 손실 위험이 있고,

- 오버샘플링은 더 많은 학습 데이터를 확보할 수 있지만, 과적합 가능성이 있습니다. 학습이 너무 편향될 수 있다는 거지요.

- 따라서 두 기법 모두 상황에 따라 적절히 활용하는 것이 중요하며,

- 분류 문제의 경우 종속변수의 분포를 보고 샘플링 기법을 적용하고

- 회귀 문제의 경우 독립변수 중 범주형 변수에 대한 분포를 보고 샘플링 기법을 적용하게 됩니다.

  

- 네 여기까지 데이터 전처리에 대한 내용을 마치도록 하구요.

- 마무리멘트

[2:00 / 45:00]

