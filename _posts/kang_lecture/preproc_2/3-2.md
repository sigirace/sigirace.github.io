- pandas 기초 두번째 시간입니다.
- 시작하기전에 판다스를 배우는 이유에 대해 다시한번 말씀드리고자 합니다.
- 우리는 앞서 파이썬에 대해 학습을하고 이를 활용해 판다스를 다뤄보고있는데요
- 그런데 사실 판다스는 파이썬으로 서비스를 개발하는 입장에서는 전혀 쓰일일이 없는 툴입니다.
- 그러나 데이터 분석가의 입장에서 판다스는 핵심이 되는 라이브러리 중 하나입니다.
- 데이터 분석을 위한 데이터는 보통 데이터 웨어하우스, 데이터 베이스 혹은 엑셀의 형태로 존재하기에
- 이러한 데이터를 읽고, 원하는 형태로 가공하고, 분석하는데 있어 판다스는 다른 어떤 툴보다도 빠르게 동작하기 때문인데요
- 따라서 우리는 전처리, 분석 등에 활용하기 위해 데이터를 가공하는 과정을 배우고 있다고 보시면 될 것 같습니다.
- 그럼 이어서 강의를 진행해 보도록 하겠습니다.

[1:30]

**📘 3.4 조건에 맞는 데이터 탐색 및 수정**

- 이번시간의 첫번째 내용은 Pandas로 **조건에 따라 Dataframe을 선택, 탐색, 수정**하는 방법과 
- 전처리의 가장 핵심인 **결측치 처리 및 중복 제거** 방법에 대해 알아보도록 하겠습니다.

- 데이터 프레임에서 조건을 통해 데이터를 탐색하기 위해서는 조건식이 필요합니다.

- 이는 앞서 파이썬 기초에서 배운 연산자를 통해 확인했던 내용이니 아마 쉽게 파악이 가능하실 것입니다.

- 연산자의 대상은 보통 **컬럼을 기준으로 작성**하게 되는데요, 

- 이는 컬럼이 곧 데이터의 특성을 나타내기 때문입니다.

  

**✅ 1. 조건이 하나일 경우**

- 먼저 조건이 하나일 경우에 대해 알어보겠습니다.
- 먼저 학생의 국어점수가 80점 이상인 조건식을 작성해볼까요?

[코드]

- 국어 점수가 80점 초과인 행만 출력되며, 데이터 자체가 아닌 참/거짓으로 구분된 시리즈가 생성되었습니다.

[코드]

- 우리는 이렇게 나온 시리즈를 padnas의 필터링 기능을 통해 원하는 데이터만 추출할 수 있습니다.
- 따라서 이걸 기존 데이터프레임 안에 넣게 되면 True인 행만 출력하게 됩니다.

**✅ 2. 조건이 여러 개인 경우**

- 조건이 두개 이상일 경우 결합이 필요하게 됩니다. 이때 **&(and)** 또는 **|(or)** 사용하며
- 주의할점은 각 조건은 괄호로 감싸야 합니다

[코드]

[3:00]

**🔍 3.4.2 결측값 탐색 및 수정**

- 이제 우리는 조건식을 통해 데이터 프레임에서 원하는 데이터를 가져올 수 있음을 알았습니다.
- 이번에는 그 특수 케이스인 결측치에 대한 조건을 알아보고 사용해 보는 과정을 살펴보겠습니다.
- 잠깐 짚고 넘어가자면, 결측치를 탐색하는 과정은 데이터 분석에서 아주 중요한 과정입니다.
- 우리는 분석을 위해 보유한 데이터에 알고리즘 혹은 연산을 적용하는 일을 매우 빈번히 수행하게 될 것입니다.
- 그런데 특정 행에 결측치가 존재하게 된다면, 연산에서 오류를 발생하거나 알고리즘이 잘못 흘러갈 수 있기 때문에
- 미리 데이터에 결측치가 있는지 확인하고 처리를 해주는 과정이 필요합니다.

**✅ 1. 결측값 탐색**

- 먼저 결측값이 있는지 확인하기 위해서는 조건식이 아닌 함수를 사용하게 되는데요 바로 isna와 isnull이 존재합니다.
- 앞서 설명드렷듯 결측치를 확인하는 일이 매우 빈번하기에 어떤 조건식이 아니라 자주 사용할 수 있게끔 함수로 제공하고 있는 것이 아닌가 싶기도 하네요.
- 어쨌든, 둘의 기능적 차이는 없으니 원하는 걸 사용하셔도 무방하구요.
- 결측치 탐색의 결과는 앞서 우리가 조건을 통해 데이터를 살펴보았을때 와 동일하게 True, False인 형태로 나타나고 이를 데이터프레임에 적용하여 필터링할 수 있습니다.
- 더하여 결측값이 아닌 값을 탐색할때는 natna, notnull을 사용하게 됩니다.

- 실습에 앞서 Null과 NaN의 차이에 대해 한번 짚고 넘어가겠습니다.
- 조금은 헷갈리는 개념인데요, Null은 영어 뜻 그대로 비어있다 라는 의미를 가지구요
- NaN은 Not a Number라는 뜻으로 숫자가 아니니 연산이 불가능하다라는 의미입니다.
- 사실 저희가 배우고 있는 판다스에서는 Null과 NaN을 같은 위상으로 보고 있어서 isna나 isnull 중 어느 것을 써도 무방합니다.
- 그럼 실습을 통해 알아보도록 하겠습니다.

[코드 - 셀 마스킹까지 읽어야함]

[6:00]

**✅ 2. 결측값 개수 확인**

- 다음으로는 결측치에 대한 개수를 확인하기 위한 방법입니다.
- 앞서 함수를 통해 결측치 여서 나타난 True는 1, False는 0으로 집계되어 결측치가 총 몇개있는가 에대한 결측치 합계 정보로 나타낼 수 있습니다.

[1:30]

**✅ 3. 결측값 삭제**

- 다음으로 결측치를 확인했다면 어떤 처리를 해주어야하는데 첫번째로 할 수 있는 수행이 바로 삭제입니다.
- 삭제를 위한 함수는 dropna를 사용하며 그 안의 파라미터는
- [읽기]

- 이제 이에 대한 실습으로 그냥 결측치가 있다면 모두 지워주도록 하겠습니다.
- 이때 inplace 를 사용하지 않았기에 원본 데이터는 여전히 결측치가 있음을 확인해주세요

[1:30]

**✅ 4. 결측값 대체 – fillna()**

- 결측치를 처리하는 또 다른 방법으로는 대체하는 방법이 있습니다.
- 대체를 위해서는 결측된 데이터를 채워 넣는 방식인 fillna 함수가 있으며 파라미터는
- [읽기]

- 데이터 대체하기 위해 값을 자유롭게 넣을 수 있지만 보통 0으로 대체, 평균으로 대체하는 방식이 많이 사용됩니다.

[1;00]

**🧹 3.4.3 중복값 처리**

- 데이터 탐색, 수정의 마지막은 중복값 처리입니다.
- 실제 데이터에서는 수집시 발생한 오류 혹은 가공시 발생한 오류로 인해 데이터가 중복되는 상황이 빈번하게 나타납니다.
- 이럴 경우 분석 알고리즘이 데이터를 잘못 해석하는 경우가 발생할 수 있기에 중복을 확인하고 제거해주는 과정이 필요합니다.
- 중복값 처리는 drop_duplicate 함수를 사용하게 되구요
- 이는 행을 기준으로 동일한 데이터가 있을시 인덱스를 고려하지 않고 항상 먼저있는 행만 남기고 나머지 행들을 지워버립니다.
- 파라미터는 자체에 inplace가 없기에 변수에 재할당 해 줄 필요가 있습니다.
- 바로 실습을 통해 확인해보도록 하겠습니다.

[1:30/ 16:00]



## 3.5 데이터 정렬

[읽기]

- 이렇게 정렬을 수행하게 되면 라벨의 인덱스의 순서가 뒤죽박죽이 됨을 알 수 있습니다.
- 이럴경우 데이터에 대한 확인이 어려워질 수 있기에 인덱스를 재정의 해주도록 하겠습니다.
- 인덱스를 재정의 reset index를 사용한다고 앞 강의에서 말씀드렸었죠
- 바로 적용해보도록 하겠습니다.

[3:00/ 19:00]



## 3.6 데이터 병합

- 이제 우리가 넘어야할 또 하나의 큰 산인 데이터 병합입니다.
- 우리는 다양한 데이터를 예쁘게 하나로 받는 경우가 없고, 여러 데이터 원천에서 데이터를 수집하여 분석을 위한 하나의 데이터로 만들어야 합니다.
- 이를 데이터 분석에서는 데이터 마트라고 표현하곤 하는데요.
- 데이터를 병합하는 과정은 앞서 concat을 사용한 데이터프레임의 연결과 유사하지만 조금 다릅니다.
- 그 차이는 병합 방식은 두 데이터 프레임이 동시에 가지고 있는 특정한 컬럼의 값인 key를 기준으로 수행되기 때문인데요. 
- 이는 엔지니어 분들께서 익숙한 sql에서 join과 유사합니다.
- 다음 그림을 보고 병합의 네가지 방식에 대해 설명을 하겠습니다.

- 먼저 데이터 병합을 위해 두개의 데이터 프레임을 준비하겠습니다.
- 이때 각 데이터프레임에는 공통된 컬럼1이 존재하는데 이렇게 두 데이터 프레임간 공통되는 컬럼이 앞서 말씀드린 key가 됩니다.
- 우리는 이 key를 기준으로 두 데이터 프레임을 하나로 합치는 과정을 merge 즉, 병합이라고 말하게 될 것이구요
- 합치는 방법은 아래와 같이 inner, outer, left, right로 구분됩니다.

- 첫 번째로, inner 방식은 두 데이터프레임의 **교집합**이라고 생각하시면 됩니다.
- 벤다이어그램을 보시면, 두 데이터프레임 모두에 **공통적으로 존재하는 값(Column1 기준)**만 남기고 병합됩니다.
- 예시에서는 Column1에 **A, C**가 양쪽에 모두 있기 때문에 이 두 개만 병합됩니다.

- 두 번째는 outer 방식입니다.
- outer는 말 그대로 **합집합**입니다.
- 즉, **두 데이터프레임 중 어느 한쪽에만 존재하더라도 무조건 포함**됩니다.
- 다만, 어느 한쪽에만 있는 값은 반대편의 값이 NaN으로 채워집니다.
- 예시로는 A, B, C, E 모두 등장하지만 B와 E는 한쪽에만 있기 때문에 **NaN**이 포함됩니다.

- 세 번째는 left 방식은 **왼쪽(left) 데이터프레임을 기준으로** 병합이 이루어집니다.
- 왼쪽의 값은 모두 유지하고, 오른쪽에서 **해당 key가 있으면 병합**, 없으면 NaN이 들어갑니다.
- 예시에서는 A, B, C가 전부 유지 되지만 B는 오른쪽에 없기 때문에 Column3이 NaN이 됩니다.

- 마지막으로 right 방식입니다.
- 이는 left와는 반대로, **오른쪽(right) 데이터프레임을 기준**으로 병합합니다.
- 오른쪽의 값은 모두 유지되며, 왼쪽에서 매칭되는 값이 있으면 병합됩니다.
- 오른쪽의 A, C, E가 기준이 되어 유지되며 E는 왼쪽에 없기 때문에 Column2가 NaN으로 채워집니다.

- 이제 개념은 말씀드렸고 실습을 수행하기 전 정의를 한번 살펴보겠습니다.
- 병합을 위해서는 merge라는 함수 사용하며 파라미터는
- [읽기]
- merge를 사용하실때 sql에 익숙하신 분들은 편하시겠지만 처음이신분들은 이미지를 생각해주시면 될것 같습니다.
- 또한 결측치가 발생할 수 있으니 병합 후에는 결측치 처리가 후속작업으로 따라오게 됩니다.

[코드]

- 먼저 예시를 위해 데이터 프레임 두개를 만들어 보겠습니다.
- key가 되는 컬럼은 product가 되겠네요.

[코드]

[8:30/ 27:30]



## 3.7 데이터 요약

- 다음은 데이터 요약입니다.
- 데이터의 요약은 앞서 info 혹은 Describe를 통해 개괄적으로 확인하였는데
- 이번 장에서는 좀더 세부적으로 데이터를 요약하는 방법을 알아보겠습니다.

### 1. 그룹화 집계

- 먼저 그룹화 집계란 하나 이상의 조건에 따라 데이터를 묶고, 그 묶인 그룹에 대해 평균, 합계, 최대값, 최소값 등의 집계 연산을 수행하는 것입니다.
- 예시로는 어떤 학교의 학생들 데이터가 있을때, 반을 기준으로 그룹화 하여 각 반의 시험점수 평균을 구한다던지
- 매출 데이터를 월 별로 묶어서 평균, 최대, 최소 값을 확인하는 것들이 있겠네요
- 이렇게 그룹화를 수행하게 된다면, 전체 데이터에서는 파악하지 못했던 데이터의 패턴을 파악할 수 있거나, 더 세부적인 단계의 분석을 수행할 수 있는 장점이 존재하게 됩니다.
- 이 그룹화는 Pandas에서는 groupby()라는 함수를 사용하여 쉽게 구현할 수 있습니다.
- 파라미터를 살펴보면
- [읽기]
  - dropna: 만약 그룹화를 사용할 컬럼 데이터에 결측값이 존재할 경우, ~~




- 예시를 위해 다시한번 iris 데이터를 불러오도록 하겠습니다.
- 다만 현재 iris 데이터는 수치 데이터만 포함되어 있기 때문에 그룹화가 불가능합니다.
- 예시로는 학생들의 성적 데이터만 있고 반에 대한 정보가 없기 때문에 반별 평균을 낼 수 없는 것과 동일하겠네요
- 따라서 우리는 그룹화 수행하기 위해 **컬럼을 추가**해야 합니다.
- 저희는 이 컬럼을 꽃의 품종으로 사용할 것이구요, 그에대한 데이터는 load_iris로 부터 가지고온 클래스의 target을 통해 확인할 수 있습니다.
- 확인해본 결과 여기 나타난 0, 1, 2가 각 품종을 의미한다고 보시면 되겠구요, 
- 이를 우리 데이터 프레임의 class라는 컬럼으로 추가해 주도록 하겠습니다.
- 자, 이제 class 컬럼이 추가되었기에 이 컬럼을 기준으로 데이터를 그룹화할 수 있습니다.

[그룹화 코드]

- 그런데 뒤에 집계함수를 주지 않으면, 이는 그냥 어떠한 DataFrameGrouby의 오브젝트만을 알려줍니다.
- 즉, 그룹화는 하였는데 이걸로 어떠한 수치를 나타낼 수 없는 상태이지요.
- 그래서 우리는 결과를 받아보기위해서는 항상 집계함수가 필요하게 되구요
- 이번 실습에서는 평균에 대한 집계를 확인하기 위해 집계함수에 mean을 넣어 주도록 하겠습니다.

[코드]

- 이렇게 그룹바이 함수는 각 클래스로 평균, 분산, 최대값, 최소값 등 다양한 통계 정보를 손쉽게 파악할 수 있기에
- 복잡한 데이터를 간단한 요약으로 정리할 수 있는 매우 유용한 도구입니다.

[4:00/ 16:00]

**3.7.2 도수분포표**

- 다음은 도수분포표입니다.
- 데이터 집계의 첫번쨰로 데이터를 그룹화하여 수치를 중심으로 요약하는 방법에 대해 알아보았습니다.
- 다음으로는 수치가 아닌 빈도를 중심으로 요약하는 방법, 도수분포표에 대해 알아보겠습니다.
- 도수분포표란 데이터를 구간별로 나눈 뒤, 각 구간에 속하는 데이터의 개수를 세어 정리한 표입니다.
- 쉽게 말하면, 데이터가 어떤 범위에 얼마나 몰려 있는지를 보여주는 통계표라고 볼 수 있습니다.
- pandas의 함수로는 value_count를 사용하는것이 가장 일반적입니다.

[코드]

- valuecount를 사용해 iris 데이터의 class의 분포가 어떤지 살펴보도록 하겠습니다.
- 네 전부 50개로 동일한 분포를 가지고 있음을 확인할 수 있네요

[더 알아보기]

- 또한 수치형 데이터의 경우에도 도수분포표를 만들 수 있습니다.
- 예를들어 반, class 등 명확히 구분되어있는 값을 기준으로 빈도수를 세는 것이 아닌
- 학생들의 점수를 기준으로 빈도수를 센다고 가정하면 가장 먼저 떠오른 방식은 0~50점, 60~80점 그리고 90~100 점으로 점수를 구간화하여 세는 것일 것입니다.
- 이렇게 그룹화의 기준이 되는 컬럼이 구분값이 아닌 연속적인 수치를 가지게 될때는 데이터를 구간으로 나누는 작업이 먼저 필요합니다.
- 대표적인 방법으로는 분위수를 기준으로 데이터를 동일한 개수의 구간으로 나누는 방식이 있습니다.

[코드]

- 이를 우리가 사용하는 아이리스 데이터에 적용해 보겠습니다.
- 먼저 아래의 qcut 함수는 Pandas에서 **연속형 수치 데이터를 구간으로 나누기 위해 사용하는 함수**로 통계적으로 말하면, **분위수(quantile)를 기준으로 데이터를 균등하게 나누는** 기능입니다.
- 예시에서는 qcut을 사용해 petal with를 3등분하고 각각의 구간을 low, medium, high로 라벨링하였습니다.

[코드]

- 이렇게 데이터에 대해서 라벨링을 수행하는 것이 곧 데이터를 구간으로 나누는 작업이라고 보시면 되겠구요
- 다음으로는 우리 데이터프레임에 구간에 대한 컬럼을 추가해주는 과정입니다.

[코드]

- 그 결과를 다시 valuecount를 수행해보면
- 네 약간의 차이는 있지만 각 라벨이 동등하게 분포됨을 알 수 있습니다.

- 더하여 이렇게 생성한 도수분포표는 이후 데이터를 시각화 하는 과정에서 그 유명한 히스토그램을 그리기 위한 전단계로 사용합니다.
- 이런식으로 각 구간마다 데이터가 몇개가 있는지 빈도수를 구한다면, 어떤 데이터가 많이 있는지, 어떤 분포로 데이터가 존재하는지 쉽게 시각화로 나타날 수 있겠죠.

- 네 이렇게 여기까지가 데이터를 그룹화 하여 요약하는 방식에 대해알아보았구요
- 다음은 데이터 프레임을 더욱 자유롭게 다루게 해줄 함수에 대해 알아보도록 하겠습니다.

[10:00/37:30]



## 3.8 함수 적용

- 데이터 프레임에 적용할 수 있는 함수는 크게 두가지 apply와 map이 있습니다.
- 이 함수들은 데이터 프레임에 axis를 기준으로 행 또는 열 방향으로 지정한 함수를 수행합니다.
- 둘 사이의 기능 차이는 없구요 저는 개인적으로 apply를 선호합니다.
- 쉬운 개념이니 바로 실습을 통해 알아보겠습니다.

[코드]

- 먼저 하나의 컬럼에 대해서만 진행을 해볼텐데요,
- 우리가 가진 iris 데이터에서 sepal width를 두배하여 double sepal이라는 변수를 만들어 보겠습니다.
- 먼저 새로 만들 컬럼을 지정해주고 변경의 대상이 되기 위한 컬럼명도 지정을 해줍니다.
- 이때 변경의 대상이 되는 컬럼에 함수를 적용하기 위해 apply를 달아주고요, 거기 안에 이제 약간은 거부감이 들 수 있는 lambda라는 표현이 나오게 됩니다.
- 람다는 생긴건 좀 저렇지만 우리가 함수를 쉽게 사용할 수 있도록 해주는 고마운 친구인데요.
- 우리가 함수라고 함은 어떤 값, 파라미터를 받을 수 있어야 겠죠, 이때 그 값을 받아주는 역할을 이 람다가 대신 해주게 됩니다.
- 즉 이 식은 x값을 받을 것이고 그 받은 x 값을 두배해주겠다는 의미가 되겠구요
- 입력되는 x값은 데이터프레임의 sepal wdith 컬럼이 될 것입니다.

[코드]

- 다음으로는 두 컬럼에 대한 변수처리를 apply를 통해 적용해볼 건데요
- 우리가 가진 iris 데이터에서 sepal legnth와 petal length의 합인 sepal petal sum이라는 컬럼를 만들어보겠습니다.
- 동일하게 먼저 새로 만들 컬럼을 지정해주되 여러 컬럼을 다룰 것이기에 변경의 대상이 되는 컬럼은 딱히 지정하지 않고 데이터 프레임 자체를 지정해주겠습니다.
- 이때 lambda 뒤에 오는 x는 데이터 프레임 자체가 되기에 x의 각 컬럼을 선택하여 연산을 진행해준다고 보시면 됩니다.
- 이후 axis로 데이터 프레임을 컬럼단위로 연산할건지 row단위로 연산할 건지에 대해 정해주시면 됩니다.
- 지금까지 확인한 내용으로 axis가 0이면 행 1이면 컬럼임을 알 수 있구요
- 이제 새로 생긴 변수는 아래와 같음을 확인 할 수 있습니다.



- 앞서 말씀드렸듯 apply와 map은 동일한 기능을 수행하기 때문에 map에 대한 것은 따로 다루지 않을 것이구요
- apply의 lambda를 이해하게 되면 쉽게 데이터 프레임에 여러 함수를 열단위 행단위로 수행할 수 있게 됩니다.

[4:00/ 41:30]



## 3.9 문자열 데이터 변환하기

- 이번에는 데이터프레임에 문자열 데이터가 있을때 이를 다뤄보는 방법에 대해 알아보겠습니다.
- 앞서 데이터 프레임에서 컬럼하나를 추출하게 되면 Series가 됨을 말씀드렸습니다.
- 따라서 주요방법으로는 이 시리즈에 str속성을 사용하는 방법이 될 것이구요 이는 데이터 프레임에는 없구 series에만 있는 속성입니다.

### 1. 인덱싱

- 먼저 문자열을 다루기 위한 인덱싱에 대해 알아보기 위해 데이터 프레임을 하나 생성해 보겠습니다.

[코드]

- 이 Location 컬럼에 대해 문자열 처리를하여 대표 지역만을 추출해보도록 하겠습니다.

[코드]

- 그래서 이전 파이썬 기초에서 배웠던 인덱스와 슬라이싱의 개념을 데이터프레임의 컬럼에 대해서도 일괄적으로 수행할 수 있음을 확인해보았습니다.
- 사실 예시로는 이렇게 깔끔하게 데이터가 정제되지만 실제 데이터는 더 많은 잡음이 들어가있기 때문에 다른 데이터보다 문자열 데이터는 전처리에 대해 많은 공수가 필요합니다.



### 2. 분할

- 이번에는 문자열을 나누는 작업, 즉 **분할(split)** 에 대해 알아보겠습니다.
- 분할은 말 그대로 특정 문자를 기준으로 문자열을 **여러 조각으로 나누는 작업**입니다.
- 이 작업은 문장을 여러 개의 단어로 구성된 문자열로 분해할 때 유용합니다.
- 이를 위해 split이라는 함수가 사용되며, 이 split을 사용하면 **리스트 형태의 값이 반환되고**, 그 안에 나눠진 문자열 조각들이 들어갑니다.

[코드]



[3:00/ 44:30]



**3. 문자열 치환**

- 다음 알아볼 기능은 **문자열 치환**, 즉 .replace()입니다.
- replace는 말 그대로 문자열에서 특정 값을 찾아서 **다른 값으로 바꿔주는 함수**입니다.
  - 코드
- 예시로는 '경북'이라는 문자가 들어 있는 경우 이를 '경상북도'로 바꾸는 ㄴ것을 나타내고 있습니다..

[코드]

- 이 replace 코드는 실무에서는 지역 명칭 표준화, 제품 코드 변경, 기호 제거 등 다양한 상황에서 활용되며,
- 특히 **데이터 클렌징 작업에서 빠질 수 없는 필수 기능**입니다.

[1:00 / 45:30]



## 3.10 날짜 데이터 핸들링

- 판다스 데이터프레임 다루기의 마지막은 날짜와 시간 데이터를 다루는 방법입니다.
- Pandas에서 날짜와 시간을 다루기 위해서는 datetime 모듈의 datetime 클래스를 이용합니다.

### 1 현재 날짜 사용하기

- 가장 먼저 알아볼 것은 현재 날짜를 불러오는 방법입니다.
- datetime.today()를 사용하면 **오늘 날짜와 시간**을 포함한 값을 가져올 수 있습니다.

[코드]

- 이 결과는 날짜뿐만 아니라 시간, 분, 초, 마이크로초까지 포함된 **datetime 객체**로 반환됩니다.
- 아무래도 이럴경우 우리가 눈으로 확인하기 쉽지 않기 때문에 이를 다시 가공하여 특정 속성만을 확인해 보도록 하겠습니다.

[코드]



### 2 날짜 형식의 변환

- 다음으로는 날짜 형식의 변환에 대해 알아볼텐데요

**1. 문자열을 날짜로 변환**

- 데이터프레임에서 날짜 정보가 문자열 형식으로 들어 있는 경우가 많습니다.
- 이럴 때는 판다스의 to_datetime 함수를 사용하여 문자열을 날짜 타입으로 변환할 수 있습니다.
  - 코드

- 예시를 위해 문자열 형식의 데이터프레임을 하나 만들어 볼 것이구요.
  - 코드

- 현재 데이트타임 컬럼의 데이터 타입은 오브젝트임을 알 수 있습니다.
  - 코드

- 이후 여기의  '20230101'과 같이 연도, 월, 일이 붙어 있는 숫자 형태의 문자열을 날짜 형식으로 변환하려면
- 데이트타임 함수를 해당 열에 적용해 주면 되는데요
- 이때 format을 원하는 날짜 형식으로 지정해주면 됩니다.
- 여기서는 년 월 일로 구분한 형식을 지정하였네요.
- 그 결과 변환 전에는 dtype이 object였던 것이, 변환 후에는 datetime64 타입으로 변경된 것을 볼 수 있습니다.
- 즉, Pandas가 해당 컬럼의 데이터를 날짜로 인식하였고, 이를 통해 우리는 다양한 날짜 연산이 가능해집니다.

[1:30 / 33:30]

**2. strptime – 문자열을 datetime 객체로 직접 변환**

- 다음으로는 판다스의 함수가 아닌 데이트타임의 함수로 단일 문자열을 데이트타임으로 혹은 데이트타임을 문자로 변경하는 방식을 알아보겠습니다.
- 이를 사용하면 앞서 배운 데이터 프레임의 함수와 응용하여 더 자유롭게 날짜 연산이 가능해지므로 매우 유용하다고 볼 수 있습니다.



- 문자를 날짜로 변환하는 함수는 strptime입니다.
  - 코드
- 이 함수에 필요한 인자로는 문자형식의 날짜 데이터이구 두번째로는 그에 맞는 형식이 들어가게 됩니다.
- 예시로 20230101이라는 문자열의 포맷에는 연월일이 들어가게 되는데 그 결과가 데이트타임형식으로 나오는 것을 알 수 있습니다.



**3. 날짜를 문자열로 변환**

- 반대로 날짜 형식을 문자열로 출력하고 싶을 때는 strftime()을 사용합니다.
- 이 함수는 날짜를 우리가 원하는 형식의 문자열로 가공하여 보여줍니다.



- 예시는 다음과 같아요.
  - 코드

- 앞서 데이트타임의 투데이트 함수를 사용하면  **datetime 객체**로 반환됨을 알 수 있었습니다.
- 이 타임이라는 데이트타임 객체를 다시 문자열 형식으로 변경하기 위해 strftime을 적용하구요
- 출력되길 원하는 포맷을 작성해주시면 되는데
- 예를 들어 년 월 일 시 분 초와 같이 지정하면 날짜와 시간을 사람이 읽기 좋은 포맷으로 바꿔줍니다.
- 이 기능은 레포트 출력, 파일명 생성, 로그 기록 등 실무에서 매우 자주 사용됩니다.



- 아래에는 주로 사용하는 날짜 포맷을 담았습니다.



[37:00]



**날짜 포맷**

- 아래에는 날짜 포맷에 대한 표기법들이 적혀져있습니다.
- 외울 필요는 없다고 생각합니다.



**3.10.3 날짜 데이터의 연산**

- 마지막은 날짜 데이터의 연산입니다.
- 날짜끼리의 연산은 datetime 모듈의 timedelta 패키지를 사용하여 수행합니다.
- timedelta는 날짜 또는 시간 간의 간격을 나타내는 객체로, 더하거나 빼면 새로운 날짜를 생성할 수 있습니다.
- 예를 들어, 오늘 날짜에 timedelta(days=1)을 더하면 **내일 날짜**가 되고, timedelta(weeks=1)을 더하면 **1주일 뒤 날짜**가 됩니다.
- days, hours, minutes 등 다양한 단위로 시간 연산이 가능하며,
- 이는 일정 주기의 데이터 생성, 유효 기간 계산, 기간 필터링 등에 널리 활용됩니다.



- 날짜를 다룰 땐 항상 **형식 변환, 포맷 지정, 시간 연산** 이 세 가지 축을 기억하셔야 합니다.
- 날짜가 문자열인지 datetime인지에 따라 사용 가능한 연산이나 함수가 달라지므로
- 데이터를 다루기 전에 먼저 타입 확인이 중요합니다.



[38:00]



## 3.11 실습

- 읽기



1. 문자열 -> 날짜 변환

```
products["launch_date"] = pd.to_datetime(products["launch_date"])
orders["order_date"] = pd.to_datetime(orders["order_date"])
```



2. inner merge로 두 데이터 프레임 병합

```
df_merge = pd.merge(products, orders, on="product_id", how="inner")
df_merge
```



3. 결측치 확인

```
df_merge.isna().sum()
```



4. 판매가에서 할인율 적용한 최종 가격 컬럼 생성

```
df_merge["final_price"] = df_merge.apply(lambda x: x.price * (1 - x.discount / 100), axis=1)
df_merge
```



5. 카테고리별 평균 판매 가격

```
df_merge[["category", "final_price"]].groupby(by="category").mean()
```



6. 2023-02-01 이후의 데이터만 추출

```
recent = df_merge[df_merge["order_date"] >= pd.to_datetime("2023-02-16")]
recent
```





- 네 이상으로 판다스로 데이터를 다루는 과정을 전반적으로 한번 살펴보았는데요.
- 다음시간부터는 이를 활용하여 시각화 부터 머신러닝까지 진행해 보도록 하겠습니다.
- 모두 고생많으셨습니다.


