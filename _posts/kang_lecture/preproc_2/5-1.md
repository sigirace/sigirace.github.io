**Warm Up – 데이터 전처리의 중요성**

- 이번에는 **분석 전 반드시 선행되어야 하는 데이터 전처리(Data Preprocessing)**에 대해 간단히 알아보겠습니다.
- 사실 데이터를 분석하거나 모델링을 하기 전에, **그 데이터를 분석에 적합하도록 잘 다듬어놓는 과정**이 먼저 필요합니다.
- 이걸 무시하고 진행하면 아무리 좋은 알고리즘을 써도 결과가 왜곡되거나, 정확도가 낮게 나오는 경우가 정말 많아요.

**좋은 재료가 좋은 결과를 만든다**

- 첫 번째 그림을 보시면, **냉동 상태의 고기, 패티, 빵**을 재료로 햄버거를 만든 뒤 **최종적으로는 고급 스테이크 수준으로 품질을 높이는 과정**이 표현되어 있어요.
- 이건 데이터를 전처리해서 **더 좋은 형태로 바꾸는 과정**과 매우 비슷합니다.
- 즉, 원재료가 아무리 많아도 **가공되지 않으면 쓸모 없듯**, 데이터도 **깨끗하고 구조화된 형태여야 제대로 된 분석이 가능**하다는 거죠.

**목적을 향한 정확한 준비**

- 두 번째 이미지는 **현실에서 데이터를 정리하지 않고 분석하려는 경우**를 보여줍니다.

- 여자 아이가 ATM기를 사용하려하는데, 동전과 지폐를 고민하고 있습니다.

- 아마 동전을 가지고 가면 ATM기기는 동작하지 않겠죠.

- 이때 ATM기는 모델이라고 생각하시면 되고, 동전과 지폐는 데이터라고 볼 수 있습니다.

- 즉, 사용할 모델에 맞는 데이터를 **잘 준비해두는 것이 전체 분석의 출발점**이라는 메시지를 담고 있어요.

  

**데이터 전처리란 무엇인가?**

- 데이터 전처리는 쉽게 말하면 **분석에 적합하도록 데이터를 정리하고 다듬는 작업**이에요.
- 우리가 직접 데이터를 수집하지 않고, **현장에서 생성된 원본 데이터를 그대로 사용할 경우**엔 이상치, 결측치, 오류가 섞여 있을 가능성이 높아요.
- 이런 데이터를 정제하지 않고 분석에 바로 쓰게 되면, **분석 모델이 왜곡되거나, 비효율적인 결과가 나올 수밖에 없습니다.**

**데이터 전처리의 주요 종류**

- 아래는 전처리 과정에서 흔히 수행하는 작업들을 정리해놓은 것입니다.

| **구분**          | **설명**                                                     |
| ----------------- | ------------------------------------------------------------ |
| **데이터 클리닝** | 결측치 채우기, 이상치 제거, 중복 제거 등 기본적인 정리 작업  |
| **데이터 통합**   | 서로 다른 소스의 데이터를 하나로 합치는 작업                 |
| **데이터 변환**   | 스케일 조정, 정규화, 파생 변수 생성 등                       |
| **데이터 축소**   | 필요한 변수만 선택하거나, 차원을 축소하는 작업               |
| **샘플링**        | 너무 많은 데이터가 있을 경우 일부만 추출하거나 샘플을 균형 있게 조정 |
| **데이터 분할**   | 분석이나 모델링을 위해 train/test 셋으로 나누는 작업         |

**결론**

- 결국 데이터 전처리는 **분석의 시작을 위한 준비운동이자, 전체 성과의 50% 이상을 좌우하는 핵심 과정**이에요.
- **좋은 모델을 쓰는 것보다 더 중요한 건, 좋은 데이터를 제공하는 것**이라는 점 꼭 기억하셔야 합니다.
- 이 강의에서도 앞으로 실습이나 분석을 할 때마다 항상 전처리 단계를 먼저 진행하게 될 텐데요,
- **그만큼 분석에 있어 가장 필수적인 기초 체력 훈련**이라고 생각하시면 됩니다.







**5. 데이터 전처리**

- 그럼 데이터 전처리에 대한 강의를 시작해보도록 하겠습니다.

**5.1 이상치 확인 및 정제**

**5.1.1 이상치란?**

- 처음으로는 데이터 전처리 중에서도 **이상치(outlier)**를 어떻게 정의하고, 또 어떻게 찾아내는지에 대해 알아보겠습니다.
- 우리가 데이터를 다룰 때 종종 어떤 값이 유난히 튀는 경우를 보게 되는데요,
- 이런 값들은 대개 **측정 오류, 입력 실수, 혹은 극단적인 상황에서 발생한 값**일 가능성이 높습니다.
- 문제는 이런 이상치가 분석 결과에 **불필요한 영향을 주거나 왜곡된 해석을 유발할 수 있다**는 점이에요.
- 따라서 분석 전 반드시 **이상치를 탐지하고 필요한 경우 정제하거나 제거하는 작업**이 필요합니다.



- 그럼 **이상치는 왜 문제가 될까요?**
- 이상치는 단순히 보기만 튀는 게 아니고, 실제로 분석 모델에도 **큰 영향을 줍니다.**
- 특히 **평균 기반 계산 예를들어 선형 회귀, PCA 등**을 할 때는 소수의 이상치가 평균값을 크게 왜곡할 수 있기때문에 전체 결과를 왜곡시킬 수 있어요.
- 그래서 사전에 **이상치를 탐지하고, 필요하면 제외하거나 적절히 처리**해야 합니다.



- **이상치의 예시를 한번 볼까요?**
- 예를 들어 어떤 항목의 대부분 값이 1~10 사이에 분포하고 있는데, 거기서 갑자기 300이나 500 같은 값이 나온다면?
- 이건 분명 뭔가 문제가 있는 값일 가능성이 높겠죠.
- 또는 예를 들어 혈압 데이터인데, 값이 0으로 되어 있다면 이건 입력 오류일 가능성도 있습니다.



- 그럼 **어떻게 이상치를 찾을 수 있을까요?**
- 이상치를 찾는 방법은 여러 가지가 있지만, 여기서는 가장 널리 쓰이는 방법인 **IQR 방식**을 사용해볼게요.
- IQR은 사분위수 범위를 의미합니다.
- 1사분위수(Q1)와 3사분위수(Q3)를 기준으로
- **Q1 - 1.5×IQR보다 작거나 Q3 + 1.5×IQR보다 큰 값**은 이상치로 간주합니다.
- 이 방법은 **데이터의 분포에 민감하지 않고, 전형적인 극단값을 잘 걸러주는 장점**이 있어요.



**예제 설명**

- 예시로는 와인 데이터셋을 이용해서 color_intensity 변수에 이상치가 있는지 확인해보도록 하겠습니다.
- 데이터를 먼저 불러오도록 할게요
  - 코드 읽기



- 그리고 앞서 이상치를 파악하는 EDA 과정인 boxplot을 그려보도록 하겠습니다.
- boxplot 결과를 확인해보면, 대부분의 값이 일정한 범위 안에 몰려 있고,
- 위쪽에 몇 개 점으로 찍힌 값들이 보이죠? 바로 저 값들이 **수염 밖에 있는 이상치**입니다.
- 이런 시각화만으로도 이상치를 파악할 수 있지만,
- 실제로 어떤 값들이 이상치인지 구체적으로 알고 싶다면 **IQR 수식**을 이용해서 계산하는 것이 더 정확합니다.



**IQR 방식으로 이상치 추출**

- IQR 수식을 통해 이상치를 확인하는 함수를 코드로 만들어보겠습니다.
  - 코드해석
    - 앞서 Q1과 Q3를 통해 IQR을 계산한다고 설명드렸습니다.
    - 따라서 percentile 함수를 통해 이를 구할 수 있구요



- 다음으로 구한 이상치 함수를 통해 실제 이상치를 확인해보면
- color_intensity 값이 **10.8, 11.75, 13.0**처럼 일반적인 범위를 벗어난 것을 확인할 수 있습니다.







**5.1.2 이상치 처리 – 제거 또는 대체**

- 다음으로는 구한 이상치를 **제거하거나 대체하는 방식**으로 데이터를 정제하는 방법에 대해 알아보겠습니다.
- 앞에서 이상치를 탐지하는 방법, 특히 **IQR 방식으로 이상치를 찾는 방법**까지는 살펴봤죠.
- 그렇다면 이제 실제 분석을 할 때, **이상치를 발견한 후엔 어떤 방식으로 처리할 수 있는지** 살펴볼 차례입니다.



**1. 이상치 제거 방식**

- 첫 번째 방법은 이상치를 **아예 데이터셋에서 제거하는 방법**이입니다.

- 코드 해석
  - 보통 이상치를 제거하기 위해서는 IQR 기준으로 이상치라고 판단된 인덱스 번호를 추출하고,
  - 그 인덱스에 해당하는 데이터를 통째로 삭제하도록 합니다.



- 이상치 제거 방식은 단순하고 직관적인 방식인데, 주의할 점이 하나 있습니다.
- 데이터 양이 많고 이상치가 극소수일 때는 유용하지만,
- **데이터 자체가 적거나 이상치가 많을 경우엔 정보 손실이 클 수 있습니다.**
- 실제로 데이터를 삭제하고 shape을 비교해보면,
- 총 178개의 샘플 중 4개가 삭제되어 174개가 남은 걸 확인할 수 있네요.
- 저는 이상치가 약 10% 미만이면 그냥 제거를 해버리는 편인데 너무 많은 이상치가 발생한다면 두가지를 고려해봐야합니다.
- 도메인 지식으로 보아 이상치가 아닐 수 있는지
- 너무 많이 제거를 할 수 없으니 대체가 가능한지 입니다.





**2. 이상치 대체 방식**

- 그래서 살펴볼 두 번째 방식은 데이터를 아예 제거하지 않고, **값을 대체하는 방식**입니다.
- 이는 보통 이상치를 **결측치(NaN)**로 먼저 바꿔주고,
- 그다음에 원하는 값으로 대체하는 방식인데요, 대표적으로 **평균값, 중앙값, 혹은 예측 모델 기반 값**으로 채우는 경우가 많습니다.
- 여기서는 간단하게 평균값으로 대체해보도록 하겠습니다.
- 코드해석
  - 이상치를 NaN으로 바꾸고 나서, 평균값을 구해 그 자리 채워주면 됩니다.



- 결과적으로 이상치였던 인덱스들에는 **동일한 평균값이 채워졌고**, 데이터셋은 그대로 유지됨을 확인할 수 있습니다.





- **그렇다면 이상치 제거/ 대체 중 어떤 방식을 써야 할까요?**
- 이건 사실 데이터의 특성과 분석 목적에 따라 달라요.
- **모델이 민감한 경우에는 제거**,
- **데이터가 희소하거나 결측치로 처리하고 싶은 경우엔 대체**가 적절하겠죠.
- 그리고 무엇보다 중요한 건, 무작정 제거하거나 대체하는 게 아니라,
- **왜 이 데이터를 제거했는지, 왜 이렇게 채웠는지에 대한 근거를 명확히 남기는 것**이에요.
- 이는 분석의 신뢰도를 높이고, 이후 결과 해석에도 중요한 영향을 줍니다.



**정리하며 마무리**

- 다시한번 정리하자면 이상치 처리에는 대표적으로 **제거**와 **대체** 두 가지 방법이 있습니다.
- 제거는 깔끔하지만 데이터 손실 가능성이 있고, 대체는 유연하지만 대체 값 선정 기준에 따라 분석 결과가 달라질 수 있죠.
- 결국 중요한 건 **데이터에 따라 가장 합리적인 방식으로 처리**하고, 그 과정을 **투명하게 기록**하는 거예요.







## 5.2 범주형 변수 처리

- 우리는 앞서 변수의 종류에 대한 내용을 알아보았습니다.
- 다시 한 번 복습을 하자면, 변수의 종류에는 크게 **범주형과 수치형**이 있는데요,
- **범주형 변수**들은 말 그대로 **‘그룹이나 분류를 나타내는 값’**이라고 보시면 됩니다.
- 예를 들어 ‘성별’, ‘지역’, ‘등급’, ‘제품명’ 같은 것들이죠.
- 그런데 이 범주형 변수는 **컴퓨터 입장에서 그대로는 사용할 수 없는 데이터**입니다.
- 왜냐하면 대부분의 머신러닝 모델들은 **수치 연산 기반의 알고리즘**으로 작동하기 때문입니다.
- 즉, 숫자 계산을 통해 패턴을 찾아내고 예측을 수행하는 구조이기 때문에,
- 문자열로 되어 있는 범주형 데이터는 모델이 이해할 수 없게 됩니다.
- 그래서 반드시 필요한 과정이 바로 **범주형 변수를 수치형으로 변환해주는 ‘인코딩’ 과정**입니다.
- 따라서 우리는 인코딩의 대표적인 두가지 라벨 인코딩과 원핫인코딩에 대해 배워볼것입니다.





### 1. 라벨 인코딩(Label Encoding)

- 첫번쨰로는 **범주형 변수**, 그 중에서도 **라벨 인코딩** 방식에 대해 알아보겠습니다.
- 범주형 변수는 ‘a’, ‘b’, ‘c’처럼 문자의 형태를 가진 값들이 많기 때문에,
- 앞서 설명하였듯 모델에 입력하기 전에 **숫자로 변환해주는 작업**이 필요합니다.
- 이때 사용할 수 있는 가장 기본적인 방법이 바로 **라벨 인코딩(Label Encoding)**입니다.



**라벨 인코딩이란?**

- 라벨 인코딩은 **각 범주형 값을 정수로 치환해주는 방식**입니다.
- 예를 들어 ‘a’, ‘b’, ‘c’라는 범주가 있다면 각각을 0, 1, 2처럼 숫자로 바꿔주는 방식이죠.
- 중요한 건 **변환의 결과인 숫자는 크기나 순서를 나타낼 수 있기에**
- **변환의 대상이 이에 합당한 경우에만 사용하는 것이 바람직하다**는 점입니다.
- 다시 말해, 범주 간에 **실제로 순서가 존재하는 경우**,
- 예를 들어 ‘낮음’, ‘보통’, ‘높음’이나, ‘초급’, ‘중급’, ‘고급’과 같이
- **서열(ordinality)이 있는 변수라면 라벨 인코딩이 적절한 선택**이 될 수 있습니다.



- 예시를  통해 알아보도록 하겠습니다.

  - 코드설명

    - 우선 ‘b’, ‘a’, ‘c’, ‘d’, ‘a’, ‘b’와 같은 범주형 문자열이 담긴 리스트가 있습니다.

    - 이 리스트를 sklearn의 LabelEncoder를 통해 변환하면,

    - 각 고유 값이 사전순 기준으로 자동 인코딩됩니다.

    - 인코딩된 라벨의 결과를 살펴보면 중복이 제거되며 각 개별의 문자가 클래스로 정의되었음을 확인할 수 있습니다.

      

    - 다음으로는 이 인코딩 모델을 활용하여 라벨을 숫자로 변환하는 작업을 살펴보겠습니다.

    - 이떄, transform() 메서드를 사용하면 해당 리스트를 숫자 배열로 바꿀 수 있습니다.

    - 또, inverse_transform() 메서드를 사용하면 **숫자 값을 다시 원래 문자열로 복원**할 수도 있어요.

    - 이처럼 라벨 인코딩은 **간단하게 문자열 데이터를 정수로 매핑하고 다시 되돌리는 기능**까지 제공합니다.

  

- 라벨 인코딩은 **범주형 데이터를 정수로 바꿔주는 가장 간단한 인코딩 방식**입니다.

- 하지만 모델이 숫자를 크기또는 순서로 오해할 수 있는 위험이 있기에 순서 정보가 있는 변수에 적용할 경우에만 사용하는 것이 적합합니다.







**5.2.2 원핫 인코딩 (One-Hot Encoding)**

- 이번에는 범주형 변수를 수치형으로 바꾸는 또 다른 대표적인 기법인 **원핫 인코딩**에 대해 알아보겠습니다.
- 앞에서 살펴봤던 라벨 인코딩은 범주형 값을 0, 1, 2 같은 정수로 단순히 치환하는 방식이었다면,
- 원핫 인코딩은 **각 범주를 열(column)로 분리해서 0과 1의 값으로 표현**하는 방식입니다.
- 그렇다면 **왜 원핫 인코딩이 필요할까요?**
- 라벨 인코딩은 간단하지만, 숫자 간의 **순서나 크기 의미를 포함하고 있는 것처럼 오해될 수 있는 문제**가 있습니다.
- 예를 들어 ‘Red’=0, ‘Green’=1, ‘Blue’=2처럼 인코딩된 경우, 기계는 마치 ‘Blue’가 ‘Red’보다 2배 크다고 해석할 수 있죠.
- 이런 잘못된 해석을 막기 위해서는, **범주 간에 순서가 없을 때**는 원핫 인코딩을 적용하는 것이 더 안전합니다.



- **원핫 인코딩의 동작 방식**은 각 범주를 개별적인 열로 분리한 뒤, **해당하는 위치에만 1을 부여하고 나머지는 0으로 채웁니다.**
- 예를 들어 색상이라는 컬럼에 해당하는 값이 ‘Red’, ‘Green’, ‘Blue’라고 하면,
- 원핫 이코딩의 수행 결과는 순서대로 1, 0, 0 / 0, 1, 0 / 0, 0, 1과 같이 각 행마다 **고유한 이진 벡터**로 표현될 수 있습니다.
- 여기서 어떤 값에 1이 들어가는지는 사실 중요하지 않으며 다만, 이 세 범주가 구분이 된다라는 점에 유의하여 보시길 바랍니다.



- 실제 방법은 예시를 통해 알아보도록 하겠습니다.

  - 코드 해석
  - 사과 바나나 배 망고라는 유일한 값을 가진 라벨이라는 컬럼이 있다고 가정해보겠습니다.
  - 이때 원핫인코딩은 sklearn의 원핫인코더를 통해 변환하게 되면 데이터를 학습하는 개념으로 인코딩을 수행하게 됩니다.

  

  - 인코딩의 결과를 보면 여러가지 속성이 있는 것을 볼 수 있는데요

  - 주요하게 볼 것은 여기 피처 네임과 카테고리입니다.

  - 카테고리가 앞서 말씀드린 유일한 값을 가지는 데이터라고 보시면 됩니다.

    

  - 그 이후 우리는 이 원핫인코딩 모델을 사용해서 가지고 있는 데이터를 변환시키는 과정을 수행합니다.

  - 그 결과는 배열이 됨을 확인할 수 있구요

  - 우리는 이 배열을 원본 데이터 프레임에 추가해주어야되기 때문에 다시 데이터 프레임의 형태로 변환을 해주어야합니다.

    

  - 변환의 결과는 다음과 같은 데이터 프레임이 될 것이구요

  - 이를 원본 데이터프레임의 인덱스 기반으로 재생성한 것이기 때문에 앞서 배운 concat 방식을 통해 쉽게 변환된 데이터 프레임을 생성할 수 있습니다.

    

  - 마지막으로 라벨인코딩 방식과 동일하게 원래의 범주형 데이터로 변환하는 것도 가능합니다.



- 다시한번 정리하자면, **원핫 인코딩은** 각 범주 간의 **순서와 거리 개념이 사라지기 때문에**, 잘못된 수치적 해석을 피할 수 있습니다.
- 반면 **원핫 인코딩의 단점** **범주의 개수가 너무 많을 경우 열이 폭발적으로 늘어날 수 있다는 단점**이 존재합니다.
- 예를 들어 1000개의 고유한 직업군이 있다면, 1000개의 열이 생기는 셈이죠.
- 이런 상황에서는 **차원 축소 기법**이나 **빈도 기반 필터링** 등의 후처리를 고려해볼 수 있습니다.
- 따라서 보유한 데이터 중 범주형데이터의 속성에서 **범주의 개수가 많지 않고**, **순서가 없는 명목형 데이터(nominal)**의 경우에는 원핫 인코딩을 우선 고려하시는 것이 좋습니다.
- 반면 범주 간의 **순위나 크기 관계가 존재하는 경우(예: 학력 수준, 등급 등)**에는 라벨 인코딩이 더 적합할 수 있습니다.
- 항상 모델이 범주형 값을 어떻게 해석할지 고려하면서, **해당 인코딩 기법이 문제에 적절한지 판단하는 것이 중요**합니다.



- 우리는 이번 섹션에서 데이터 인코딩 방법 중 대표적인 두 가지, **라벨 인코딩과 원핫 인코딩**에 대해 배워보았는데요.
- 언제 어떤 인코딩을 사용하는지는 **데이터의 속성**, 즉 **범주 간의 순서 유무**에 따라 결정된다고 말씀드렸습니다.
- 그런데 여기에는 단순히 데이터 해석의 문제를 넘어서, **우리가 사용하는 모델의 구조와 특성**도 함께 고려되어야 합니다.
- 왜냐하면 이 인코딩 방식이 **수치 기반 모델**, 그리고 **트리 기반 모델이나 딥러닝 모델**에서 **어떻게 받아들여지고 어떤 영향을 미치는지**에 따라 성능이 달라질 수 있기 때문입니다.
- 예를 들어, 숫자의 크기나 거리 개념을 그대로 해석하는 **선형 회귀나 로지스틱 회귀 같은 수치 모델**에서는 라벨 인코딩이 더 직관적이고 간결한 방식이 될 수 있고요,
- 반대로, **분기 기준만을 따지는 트리 기반 모델**이나 **벡터를 처리하는 데 익숙한 딥러닝 모델**에서는 **범주 간 순서를 인식할 필요가 없기 때문에 원핫 인코딩이 더 적합한 경우가 많습니다.**
- 결국 중요한 건, **인코딩을 수행하는 목적과 모델의 방향성에 따라 데이터를 준비해야 한다는 점**입니다.
- 따라서 앞으로는 단순히 범주형이니까 라벨로 할까, 원핫으로 할까 고민하는 수준을 넘어서서, 
- **이 데이터를 어떤 모델에 넣을 건지 그리고 어떤 관점으로 해석되어야 하는지를 함께 고려하시는 것이 훨씬 더 효과적인 접근**이 될 수 있습니다.









## 5.3 데이터 분할



**5.3.1 데이터 분할의 필요성과 train_test_split**

- 이번에는 데이터를 어떻게 분할하는지에 대해 알아보겠습니다.
- 결론부터 말씀드리자면 분석 모델을 만들 때 전체 데이터를 모두 학습에 사용하는 것은 적절하지 않습니다.
- 모델이 데이터를 암기하게 되어 일반화 성능이 저하될 수 있기 때문입니다.
- 따라서 전체 데이터를 학습용, 검증용, 그리고 테스트용으로 나누어 사용합니다.
- 이 과정을 학습에 비유하면, 학습용 데이터는 연습문제, 검증용 데이터는 모의고사, 테스트 데이터는 수능과 같은 실전 시험에 해당합니다.
- 모델은 학습 데이터를 통해 패턴을 학습하고, 검증 데이터를 통해 성능을 조정하며, 마지막으로 테스트 데이터를 통해 실제 성능을 평가하게 됩니다.
- 일반적으로 전체의 60%를 학습용으로, 20%를 검증 및 테스트용으로 설정하는 경우가 많습니다.
- 이처럼 데이터를 적절히 분할하는 과정은 모델 성능을 객관적으로 검증하는 데 매우 중요한 절차입니다.



**5.3.2 train_test_split 함수로 데이터 나누기**

- 데이터 분할을 수행하기 위해 Python에서 데이터를 분할하는 대표적인 함수인 train_test_split에 대해 알아보겠습니다.
  - 읽기
    - stratify는 동일 비율로 나눌 컬럼이라는 뜻인데 뒤에 층화추출에 대한 내용과 함께 다시 살펴보도록 하겠습니다.



- 다음은 예시입니다.
- 예시로는 앞서 사용하였던 iris 데이터를 학습과 테스트용으로 나누어볼것인데요 검증용 데이터는 고려하지 않고 일단 8대 2의 비율로 나누어 보겠습니다.
  - 코드해석



**더 알아보기**

- 이제 우리는 데이터를 원하는 비율로 분할하는 방법을 알았습니다.
- 그런데 이때 분할되는 데이터는 랜덤으로 분할되기에 최악, 즉 워스트 케이스가 발생할 수 있습니다.
- 예시로 앞서 범주형 데이터를 보았을 때, 전체 클래스의 비율은 균형을 이루고 있었지만, 무작위로 분할한 결과 특정 클래스가 한쪽에 쏠리거나 심지어 완전히 사라지는 현상이 발생할 수 있습니다.
- 이처럼 데이터가 불균형하게 분할될 경우, 분석 모델은 특정 클래스에 대해 충분히 학습하지 못하거나 성능을 왜곡되게 평가받게 됩니다.
- 이러한 문제를 해결하기 위해 사용하는 방식이 바로 **층화추출(stratified sampling)**입니다.







**5.3.3 데이터 분할 시 주의할 점과 계층 추출**

- 층화추출은 전체 데이터의 클래스 비율을 고려하여, 훈련 데이터와 테스트 데이터 모두에서 **원본과 유사한 클래스 분포를 유지**하도록 데이터를 나누는 방법입니다.
- 다시 말해, 특정 클래스가 전체 데이터의 1/3을 차지하고 있었다면, 분할 이후에도 훈련과 테스트 세트 모두에서 해당 클래스가 1/3 정도 포함되도록 설정하는 것입니다.
- 층화추출에 대한 적용을 위해서는 앞서 살펴본 파라미터중 stratify에 원하는 특성, 즉 컬럼을 인풋으로 작성하면 됩니다.
- 예제를 통해서 알아보도록 하겠습니다.





- 살펴볼 예제는 임의 데이터의 분할과 층화 추출에 대한 분할 두가지에 대한 비교입니다.
  - 코드 읽기



- 정리하자면 데이터 분류는 모델 학습을 위해 데이터를 준비하는 과정으로 생각하시면 됩니다.
- 이때의 꿀팁은 데이터 분할을 수행하기 전에는 전체 데이터의 클래스 분포를 먼저 확인하는 것이 좋습니다.
- 클래스 간 데이터 수가 지나치게 불균형할 경우, 무작위 분할은 결과적으로 학습 성능을 크게 저하시킬 수 있습니다.
- 특히 데이터의 크기가 작거나 일부 클래스가 희귀할 경우에는 계층 추출이 필수적으로 고려되어야 합니다.



- 더하여 한 가지 더 말씀을 드리자면,
- 실제 현업에서는 데이터가 부족한 상황이 매우 자주 발생합니다.
- 원칙적으로는 전체 데이터를 학습용 60%, 검증용 20%, 테스트용 20%로 나누는 것이 이상적입니다.
- 하지만 현실에서는 검증용 데이터 20%조차 아쉬운 경우가 많아, 보다 실용적인 접근을 선택하는 경우가 있습니다.
- 예를 들어, 처음에는 60%의 데이터를 사용해 모델을 학습시키고, 20%의 데이터를 검증용으로 활용하여 성능을 조정합니다.
- 이후 검증 과정이 완료되면, 검증 데이터와 테스트 데이터를 합쳐 총 80%의 데이터를 다시 학습에 활용하는 방식입니다.
- 이렇게 하면 모델이 더 많은 데이터로 학습할 수 있어 성능이 개선되는 효과를 기대할 수 있습니다.
- 따라서 이후 과정에서 60:20:20의 구조 대신, 80:20으로 단순하게 학습과 테스트 데이터만을 나누어 사용하는 경우가 있더라도,
- 이는 **데이터의 양이나 목적에 따라 달라지는 실용적인 판단**으로 이해하시면 되겠습니다.
- 특히 초기에 모델을 실험하거나, 데이터 확보가 어려운 프로젝트에서는 이러한 방식이 더 자주 활용되는 경향이 있습니다.
- 결론적으로 데이터 분할 비율은 상황에 따라 조정할 수 있으며, 중요한 것은 어떤 방식이 **현재 데이터와 문제 상황에 가장 적합한지**를 판단하는 것입니다.













## 5.4 데이터 스케일링

- 이번에는 데이터 스케일링에 대한 내용을 학습해 보겠습니다.
- 읽기



- 우리는 이번 강의를 통해 데이터 스케일링 중 대표적인 스탠다드 스케일러와 민맥스 스케일러에 대해 학습해 볼 것입니다.
- 이때 스케일링을 적용하는 데이터에 대해서도 주의 깊게 살펴보아야 합니다.
- 이 부분은 데이터 분석에 대해 이론적으로 학습한 후, 실제 적용 단계에서 가장 자주 간과되는 부분 중 하나입니다.
- 먼저 스케일링은 분석 모델이 편향을 갖지 않도록 조정하는 데 목적이 있다는 점을 다시 강조드리고자 합니다.
- 따라서 스케일링된 데이터를 사용하여 모델을 학습시킬 경우, 테스트 데이터나 실전 데이터 역시 **동일한 기준으로 변환된 상태에서** 예측에 사용되어야 합니다.
- 그렇지 않으면 모델이 학습한 기준과 전혀 다른 범위의 데이터를 입력받게 되어, 성능이 급격히 저하될 수 있습니다.
- 특히 학습 데이터의 평균과 표준편차를 기준으로 스케일링을 수행하는 스탠다드 스케일러의 경우, 테스트 데이터에 대해 별도로 평균이나 표준편차를 다시 계산하는 것은 바람직하지 않습니다.
- 따라서 훈련 데이터를 기준으로 스케일링 기준을 **한 번만 학습**하고, 그 기준을 **테스트 데이터에 그대로 적용**하는 방식이 정확한 절차입니다.
- 또한 데이터를 분할하는 시점과 스케일링을 적용하는 시점에 있어서도 순서에 유의해야 합니다.
- 보통 데이터를 훈련용과 테스트용으로 나눈 후에 스케일링을 적용해야 하며, 데이터 전체를 한 번에 스케일링한 뒤 분할하게 되면 정보 누수가 발생할 수 있습니다.
- 이는 분석 모델이 테스트 데이터에 대한 일부 정보를 사전에 학습하게 되는 문제로 이어져, 평가의 공정성이 훼손될 수 있습니다.
- 정리하면, 스케일링은 단순히 수치의 크기를 조정하는 기술적 과정이 아니라, 데이터 분석의 정확성과 신뢰도를 유지하기 위한 전처리 절차로 이해할 필요가 있습니다.





**5.4.1 Standard Scaler**

- 먼저 **데이터 스케일링 중 하나인 Standard Scaler**에 대해 알아보겠습니다.
- 수식적인 설명은 제외했으나,
- 스탠다드 스케일링은 Standard Scaler는 각 특성(컬럼)의 평균이 0이 되도록 이동시키고, 표준편차가 1이 되도록 크기를 조정하는 연산이 수행됩니다.
- 이 과정을 거치면 변수들의 단위가 달라도 모두 **정규분포와 유사한 형태로 맞추어지며**, 모델이 모든 변수에 대해 동일한 조건에서 학습을 수행할 수 있게 됩니다.



- 예제를 통해 스탠다드 스케일링의 과정을 살펴보도록 하겠습니다.
- 앞서 말씀드렸듯, 스케일링의 적용시점을 고려해서 먼저 학습과 평가 데이터를 8:2 비율로 나눈 뒤 학습데이터에 대해 스케일러를 생성하고,
- 생성한 스케일러를 테스트 데이터에 적용하는 방식으로 수행하도록 하겠습니다.
  - 코드 읽기



- 코드 수행결과 학습 데이터의 평균과 표준편차가 스케일러를 통해 0과 1로 맞춰진 것을 확인할 수 있습니다.
- 또한 테스트 데이터에는 약간의 차이가 있겠지만 이와 유사한 수준으로 변환된 것을 확인할 수 있습니다.





**5.4.2 Min-max Scaler**

- 이번에는 **데이터를 0과 1 사이의 범위로 조정하는 스케일링 기법인 Min-max Scaler**에 대해 알아보겠습니다.

- Min-max 스케일링은 이름 그대로 각 변수의 최소값은 0, 최대값은 1이 되도록 데이터를 선형적으로 변환하는 방식입니다.

- 이 방법은 변수 간 단위가 다르거나, 값의 절대 크기에 민감한 모델을 사용할 때 특히 유용하게 활용됩니다.

- 앞서 살펴본 Standard Scaler가 평균과 표준편차를 기준으로 스케일을 맞췄다면,

- Min-max Scaler는 **절대적인 최소값과 최대값을 기준으로 데이터를 재조정**합니다.

  

- Min-max 스케일러도 예시를 통해 살펴보도록 하겠습니다.
  - 코드읽기
    - 예시는 동일하게 iris 데이터에 대해 학습과 테스트 데이터로 나눈 뒤 스케일러를 적용하는 것이구요
    - 그 결과 최대와 최소가 원본 데이터와는 다르게 1과 0으로 피팅됨을 확인할 수 있습니다. 



**꿀팁**

- Min-max 스케일링을 다시 정리하자면 데이터의 범위를 최대가 1이되게 최소가 0이되게 피팅하는 것입니다.
- 이로인해 Min-max 스케일링은 **이상치(outlier)**에 민감하게 반응하는 성질이 있습니다.
- 따라서 데이터에 이상치가 포함되어 있을 경우, 스케일의 기준이 크게 왜곡될 수 있습니다.
- 이런 경우에는 이상치를 먼저 탐지하고 처리한 후에 스케일링을 적용하는 것이 바람직합니다.
- 만약 이상치가 많거나 분포가 매우 비대칭적인 경우에는, Robust Scaler와 같은 다른 기법을 고려하는 것도 좋은 대안이 될 수 있습니다.





**비교 정리**

- 마지막으로 두 가지 스케일러의 차이와 각각을 언제 사용하는 것이 좋은지 정리하며 마무리하겠습니다.
- 먼저, 사용하는 모델을 기준으로 **선형 모델**과 **비선형 모델**로 나눌 수 있습니다.
- **선형 모델**은 각 변수에 일정한 가중치를 곱해서 더한 값으로 결과를 예측합니다.
- 다시 말해, 변수와 결과 사이의 관계가 **직선처럼 단순하게 연결되는 모델**입니다.
- 이런 모델은 변수마다 크기나 단위가 다르면 결과에 영향을 줄 수 있기 때문에, **모든 변수를 평균 0, 표준편차 1로 맞추는 Standard Scaler**가 잘 어울립니다.
- **비선형 모델**은 변수와 결과 사이의 관계가 직선이 아니라 **곡선이나 복잡한 함수로 연결되는 경우**입니다.
- 이런 모델은 입력값이 지나치게 크거나 작으면 학습이 어려워질 수 있으므로, **입력값을 0에서 1 사이로 정리해주는 Min-max Scaler**가 더 안정적입니다.
- 따라서 **분류 모델**에서는, **Logistic Regression이나 Linear SVM**처럼 계산 방식이 선형적인 모델에는 Standard Scaler가 적합하고,
- **KNN, RBF SVM, 인공신경망**처럼 비선형 구조를 가진 모델은 Min-max Scaler를 사용하는 것이 좋습니다.
- **회귀 분석**의 경우에도, **선형 회귀나 릿지 회귀**처럼 결과를 직선으로 설명하려는 모델에는 Standard Scaler가 잘 맞습니다.
- 반면, **결정트리나 랜덤포레스트처럼 비선형 구조인 회귀 모델**은 스케일링을 하지 않아도 성능에 거의 영향을 받지 않습니다.
- 정리하자면, 어떤 스케일러를 사용할지는 **모델이 데이터를 어떻게 계산하는지**에 따라 달라지며,
- 모델 구조에 맞는 스케일링을 적용하는 것이 모델의 성능을 높이는 데 중요한 역할을 합니다.
- 다만 이번 강의는 선형 모델 비선형 모델에 대한 자세한 얘기는 다루지 않고 단순히 전처리에 대한 내용이기에
- 교재에 적힌 것처럼 분류 분석일때는 일반적으로 스탠다드 스케일러
- 회귀분석일때는 민맥스 스케일러를 단순히 접근하여도 무방합니다.











## 5.5 차원 축소

- 이번에는 분석 모델을 구축할 때 자주 활용되는 기법인 **차원 축소**에 대해 알아보겠습니다.
- 실제 데이터를 수집하다 보면 수많은 변수들을 다뤄야 하는 경우가 생깁니다. 특히 독립변수가 많아질수록, 이 변수들 사이의 상관관계나 데이터의 구조를 파악하기 어려워지며 분석 정확도가 떨어지는 현상이 나타날 수 있습니다.
- 이는 우리가 앞서 배운 원핫 인코딩을 생각하시면 편하실텐데요.
- 우리는 원핫인코딩을 수행하게되면 라벨에 있는 중복을 제거한 데이터의 수만큼 컬럼이 늘어나는 것을 확인했습니다.
- 만약 중복을 제거하고 독립적인 원소가 100개 이상일 경우에는 컬럼이 100개가 늘어나게 되게 되구요.
- 이럴 경우 데이터를 모델의 인풋으로 넣을 수 있도록 바꾸었지만 모델이 해석하기 어려워지게 되는 것이지요.
- 이러한 상황에서 사용하는 것이 바로 차원 축소 기법입니다. 이는 불필요하게 많은 변수를 줄여서, 중요한 정보만 남기는 방법이라고 이해하시면 좋겠습니다.
- 예를 들어 3차원 공간에 있는 데이터를 2차원 평면으로 투영시키는 식으로, 주요한 구조를 유지한 채 데이터의 복잡성을 줄이는 것이 핵심입니다.



**차원의 저주란?**

- 여기서 한 가지 용어를 먼저 짚고 넘어가야 하는데요, 바로 **차원의 저주(Curse of Dimensionality)**입니다.
- 데이터의 차원이 증가하면 변수 간의 거리가 멀어지고, 이로 인해 데이터가 희소하게 분포하게 됩니다.
- 그림의 예시를 보면 이해가 더 편하실 거라고 생각이 듭니다.
- 그림을 보자면 데이터가 1차원인 x축만 있을때, 그리고 2차원 3차원으로 늘어나 y, z 축이 생겼을때 데이터가 점점 멀어지는 것을 확인할 수 있습니다.
- 다시 말해, 차원이 많아질수록 데이터 간의 밀도가 줄어들고, 전체 영역에 비해 실질적으로 사용 가능한 정보는 오히려 적어지는 문제가 발생합니다.
- 이 문제로 인해 모델의 성능이 떨어질 수 있기 때문에, 적절한 차원 축소는 매우 중요합니다.



**5.5.1 설명변수 선택**

- 차원 축소에는 두 가지 큰 방향이 있습니다. 그 중 첫 번째는 **설명변수 선택(feature selection)**입니다.
- 이는 원래의 변수들 중에서 중요한 것들만 골라서 사용하는 방법인데요, EDA 단계에서 상관관계를 보면서 유의미한 변수만을 선택하는 것이 대표적인 예입니다.
- 다만 변수 간에 다중공선성이 존재하거나 복잡한 상관관계가 있는 경우에는 이 방법이 적절하지 않을 수 있습니다.
- 그래서 더욱 강력한 방식으로 소개되는 것이 **주성분 분석(PCA)**입니다.



**5.5.2 주성분 분석(PCA)**

- 이번에는 대표적인 차원 축소 기법 중 하나인 **주성분 분석(PCA)**에 대해 알아보겠습니다.
- PCA는 기존 변수들을 단순히 제거하는 것이 아니라, **새로운 축을 정의해서 데이터를 투영**하는 방식입니다.
- 원래의 데이터를 회전시켜 가장 많은 분산을 설명할 수 있는 방향으로 새로운 좌표계를 만드는 것이 핵심입니다.
- 이때 생성된 새로운 축들을 **주성분(Principal Components)**라고 부릅니다.
- 말이 참 어려우니 차근차근 알아가보도록 하겠습니다.



- 먼저 분산은 정보량을 나타내는 지표입니다.
- 데이터에서 분산이 크다는 것은, 데이터가 더 많이 퍼져있다는 뜻이겠죠?
- 많이 퍼져있다는건 데이터를 구별짓거나 다양하게 만들수 있는 정보가 많다는 것을 의미합니다.
- 우리가 계속 사용하는 iris 꽃 데이터를 예시를 들어보겠습니다.
  - 만약 꽃잎 길이에 대한 데이터가 모두 4.9에서 5.1cm 로 모여있다면 이 꽃잎 길이는 품종을 구분하는데 거의 도움이 되지 않을 것입니다.
  - 하지만 길이가 1.2부터 6cm 까지 아주 다양하게 퍼져있다면 품종마다 길이가 다를 수 있고 어떤 다른 특성과 연계되어서 품종을 구분할 수 있는 단서가 될 수 있을것입니다.
  - 즉, 어떤 데이터의 분산이 클 경우 데이터를 구분할 수 있는 가능성이 커지게 되는것이지요

- 따라서 PCA는 우리가 가지고 있는 데이터를 대상으로 기존 축이아닌 분산이 가장 커지게 되는 새로운 축을 생성하여 설명력을 높인다는 관점으로 생각하시면 됩니다.
- 이를 영상을 통해 확인해보도록 하겠습니다.



```
https://www.youtube.com/shorts/bv9agba7blc
```



- 먼저 데이터를 쉽게 보기 위해 **데이터의 중심을 원점(0,0)으로 이동시켰습니다**.
- 이 상태에서 각 축을 기준으로 데이터를 **투영**해 보면,
- x축 방향으로는 데이터가 비교적 넓게 퍼져 있는 반면,
- y축 방향으로는 데이터가 오밀조밀하게 모여 있는 것을 확인할 수 있습니다.
- 이 말은 곧, **x축 방향이 분산이 크고**, **y축 방향은 분산이 작다**는 것을 의미합니다.
- 따라서 데이터를 설명할 수 있는 힘, 즉 **설명력은 x축이 더 크다고 볼 수 있습니다.**



- 그렇다면 PCA에서는 어떤 축을 먼저 선택할까요?

- 바로, **데이터의 분산이 가장 큰 방향**,

- 즉 데이터를 가장 넓게 퍼지게 하는 방향을 먼저 선택합니다.

- 이 방향이 바로 첫 번째 주성분인 **PC1**입니다.

- PC1을 기준으로 데이터를 보면, 원래 두 차원이었던 데이터를 하나의 축으로 투영시켜도

- **데이터의 구조나 패턴이 상당 부분 유지**된다는 것을 알 수 있습니다.

- 하지만, PC1 하나만으로는 데이터의 모든 정보를 담아내기엔 부족할 수 있습니다.

- 그래서 **두 번째로 중요한 축**, 즉 **PC2**를 선택하게 됩니다.

- 여기서 중요한 점은, **PC2는 반드시 PC1과 직각을 이루어야 한다는 조건이 있다는 것**입니다.

- 왜냐하면 주성분들끼리는 서로 **중복되지 않는 독립적인 정보만 담아야 하기 때문**입니다.

- 따라서 PC2는 **PC1에 직교하면서**, 그 조건 하에 **남아 있는 분산을 가장 잘 설명할 수 있는 방향**으로 설정됩니다.

- 이처럼 PCA는 먼저 데이터를 가장 잘 설명하는 방향을 찾아 PC1으로 삼고,

- 그다음에는 PC1과 **독립적이면서도 여전히 많은 정보를 담을 수 있는 방향**을 찾아 PC2로 삼습니다.

  

- 이 과정을 계속 반복하면서, 필요에 따라 **PC3, PC4…** 같은 다음 주성분도 결정할 수 있습니다.

- 이렇게 결정된 각 주성분들은 원래 데이터 공간에서 **회전된 새로운 좌표계**라고 생각하시면 됩니다.

- 원래의 축(x, y)이 아니라, 데이터의 **구조를 가장 잘 드러낼 수 있는 방향으로 재정렬된 축**이죠.

- 최종적으로 우리는 이 중에서 **몇 개의 주성분만 선택해서** 데이터를 표현할 수 있습니다.

- 예를 들어, PC1과 PC2만으로도 전체 데이터를 90% 이상 설명할 수 있다면,

- **고차원 데이터를 2차원으로 줄이면서도 중요한 정보는 거의 다 유지**할 수 있는 것입니다.

- 이와 같이 PCA는 데이터의 분산을 기준으로, **서로 독립적이고 정보가 풍부한 축들을 순차적으로 선택해 나가며** 

- 차원을 줄이되, 중요한 정보는 최대한 보존하려는 목적을 갖고 있습니다.

  [영상끝]

- 일단 이정도의 설명을 하고 예제를 확인한 후 다시 설명을 이어가도록 하겠습니다.



- 예제는 iris 데이터에 대해 주성분 분석을 수행하고 차원을 축소해보는 것인데요.
- 먼저 계속 봐온 데이터를 다시한번 확인해 보도록 하겠습니다.
  - 코드
- 우리가 가진 수치형 데이터가 4개임을 확인할 수 있죠.
- 뭐 사실 이정도는 큰 문제 없지만 주성분분석을 학습해보자라는 취지로 차원을 줄어보도록 하겠습니다.
- 즉, 피처가되는 컬럼의 개수를 4개에서 더 줄이는 과정을 수행해 보도록 하겠습니다.
  - 코드
  - 코드
- 수행 결과로 고유값과 분산 설명력을 나타내고있는데요
- 여기서 이 분산 설명력이 앞서 말씀드린 새로운 축에 대한 데이터의 설명력입니다.
- 축을 1개로 설정했을때는 전체 데이터를 0.72만큼 설명할 수 있고
- 축을 2개로 늘렸을때는 0.72에 0.22를 더한만큼 즉 0.94를 설명할 수 있으며
- 축이 원래와 같은 4개일때는 데이터 전체를 설명할 수 있다라는 의미가 되는 것이지요.
- 이는 즉, 변수를 4개에서 2개로 줄여도 전체 데이터를 90퍼센트 이상 설명할 수 있다는 말로 해석할 수 있고,
- 일반적으로 분산 설명력이 0.9 이상이면 해당 개수까지 주성분으로 선택할 수 있다고 보기에 기존 4차원 데이터를 2차원으로 줄여도 괜찮다라고 보시면 됩니다.
- 물론 차원이 4개정도면 모델을 위해 PCA를 굳이 수행할 필요는 없긴 하지만요.
- 여기서 포인트는 모델을 위해서인데, 이는 바로 뒤에 다시 한번 PCA의 활용과 연계하여 말씀드리도록 하겠습니다.



- PCA를 수행하여 주성분의 개수를 구하는 방법은 앞서 분산설명력을 통해 수치로 구하는 것 외에
- 시각적으로 판단하여 구하는 방법이 존재합니다.
- 이를 scree plot이라고하는데요.
- 이 또한 예시를 통해 살펴보도록 하겠습니다.
  - 코드
  - 코드는 별건 아니고 그냥 앞서 구한 분산설명력을 꺽은선 그래프로 나타낸 것인데요.
  - 여기서 꺽은선의 기울기가 완만해지는 시점이 눈에 보이며, 바로 이 지점을 찾아 해당 이전까지만 주성분을 선택합니다.
  - 이를 **elbow point**라고 부르는데, 현 예시에서는 2라고 보이는데요
  - 그래프의 기울기가 2.0 지점에서 급격히 줄어들기에 이 직전인 0과 1 두개를 주성분으로 선택할 수 있다라고 해석하시면 됩니다.
  - 사실 앞에 수치로 해석하는 것과 크게 차이는 없지만 분석을 시각화하여 보여줄 수 있다는 것은 또 장점이 될 수 있으니 활용하시면 있어빌리티가 생길 것이라고 생각합니다.





**PCA 결과 시각화**

- PCA의 마지막은 시각화입니다.
- PCA는 앞서 모델을 위해 차원을 줄이는 목적 외에도 데이터의 형태를 시각적으로 파악하기 위해서 주로 사용합니다.
- 멀리 갈 것도 없이 우리가 사용하는 iris 데이터에 대해 상상을 해보겠습니다.
- 우리의 데이터 셋에서 수치형 컬럼은 4개였습니다.
- 그런데 3차원 이상의 데이터, 즉, 컬럼이 세개 이상인 데이터를 보기 좋게 시각화하는 방법은 없습니다.
- 사실 4차원도 시각적으로 표현하는 방법이 있긴한데 매우 난해하기에 분석에는 적합하지 않습니다.
- 따라서 가지고 있는 데이터가 4개의 컬럼 이상으로 되어있는데, 이를 시각적으로 좀 파악해서 보고싶을때 PCA를 사용합니다.
- PCA를 사용하면 데이터의 설명력은 유지하면서 이를 3차원 이하로 줄여서 표현할 수 있기 때문이지요.
- 이에 대해 예제를 통해 살펴보도록 하겠습니다.
  - 코드
    - 앞서 우리는 iris 데이터 셋에 대해 차원이 2개
    - 즉, 2개의 주성분 만으로 전체 데이터의 90%이상을 설명할 수 있음을 확인하였으니
    - 데이터를 2차원으로 줄이는 PCA를 수행해 보도록 하겠습니다.

  - 코드
    - 그리고이 2차원으로 줄인 데이터에 원본 라벨을 인덱스 기준으로 붙여 시각화를 해보면
    - 이 그림과 같이 기존에 고차원이라 확인할 수 없었던 데이터들이 눈에 쉽게 들어오는 것을 볼 수 있습니다.
    - 게다가 주성분 두개로 데이터가 어느정도 잘 분리되는 모습을 확인할 수 있기에
    - 이를 모델링에 활용한다면 아주 잘 데이터가 분류될 수 있음을 기대할 수 있을것 같네요.






**PCA의 활용 가치**

- 마지막으로, PCA의 실제 활용 가치에 대해 정리하겠습니다.
- 첫째, 분석의 효율성을 높일 수 있습니다. 변수 수가 줄어들면 학습 속도도 빨라지고, 과적합 위험도 줄어듭니다.
- 둘째, 시각화가 가능해집니다. 복잡한 고차원 데이터를 2~3차원 공간에 투영하여 시각적으로 이해할 수 있습니다.
- 셋째, 노이즈 제거 효과도 기대할 수 있습니다. 중요한 축을 중심으로 데이터를 표현하다 보니, 의미 없는 노이즈가 자연스럽게 제거되는 효과가 있습니다.
- 다만 주의할 점은, PCA를 수행할때 반드시 스케일링 과정을 거쳐야 하는 것입니다.
- 또한 만들어진 주성분은 원래의 변수와 직접적인 해석이 어렵기에 변수의 해석보다는 패턴 분석과 구조 파악에 초점을 둘 때 더 적합한 기법이라고 볼 수 있습니다.







## 5.6 데이터 불균형 문제 처리

- 이번 시간의 마지막으로는 **데이터 불균형 문제를 처리하는 방법**에 대해 알아보겠습니다.
- 데이터 불균형이란, 특정 클래스가 다른 클래스에 비해 **월등히 많은 데이터 수를 가지는 상황**을 말합니다.
- 예를 들어, 금융 사기 탐지처럼 사기 거래는 전체의 1%만 차지하고, 정상 거래가 99%인 경우를 생각해볼 수 있습니다.
- 이처럼 일부 클래스에 데이터가 편향되면, 모델은 주로 다수 클래스의 패턴만 잘 학습하게 되고, 소수 클래스는 잘 예측하지 못하게 됩니다.
- 따라서 전체적인 정확도는 높게 나와도 **정작 중요한 예측 성능은 떨어질 수 있는** 문제가 발생하는 것이죠.
- 이런 경우에는 소수 클래스를 더 잘 예측할 수 있도록, **데이터를 재구성하거나 샘플링을 조정하는 작업이 필요**합니다.
- 이 섹션에서는 대표적인 방법인 **언더샘플링과 오버샘플링**을 중심으로 살펴보겠습니다.



**5.6.1 언더샘플링**

- 먼저 **언더샘플링** 기법에 대해 알아보겠습니다.
- 언더샘플링은 말 그대로 **다수 클래스를 줄이는 방식**입니다.
- 즉, 데이터 수가 많은 쪽을 일부만 선택해서, 소수 클래스와 **비슷한 수준으로 맞춰주는 방식**이죠.
- 이렇게 하면 불균형 문제를 어느 정도 완화할 수는 있지만, 반대로 중요한 데이터를 제거하는 경우도 생길 수 있기 때문에 **성능이 저하될 가능성**도 염두에 두어야 합니다.

- 예를 들어, 전체 데이터가 2000개이고, 이 중 95%가 클래스 0, 5%가 클래스 1이라고 가정하겠습니다.
- 언더샘플링을 수행하면, 클래스 0에 해당하는 1900개 중에서 임의로 100개만 선택하고, 클래스 1의 100개와 합쳐서 총 200개로 학습을 진행하게 됩니다.
- 이렇게 하면 **균형잡힌 데이터셋이 되기는 하지만, 원래 갖고 있던 정보의 대부분을 버리는 셈**이기 때문에 신중하게 선택해야 합니다.

- 언더샘플링은 다음과 같은 상황에서 고려해볼 수 있습니다:
  - 데이터의 총량이 너무 많아서 처리 비용이 클 때
  - 다수 클래스 내의 정보가 비교적 중복되어 있을 때
  - 빠른 실험이나 가벼운 모델링을 원할 때

- 다만 중요한 건, 학습 성능이 떨어질 수 있다는 점을 충분히 고려하고, **실제 결과를 평가하면서 적절히 조정해주는 것**이 중요합니다.



- 이제 예시를 통해 알아보도록 하겠습니다.
  - 코드
    - 먼저 샘플링을 적용할 데이터셋을 만들어볼건데요
    - 데이터의 수는 2000개 컬럼은 6개인 데이터셍이며 클래스의 비율은 0과 1이 약 95:5로 매우 불균형적인 데이터입니다.
  - 코드
    - 그리고 언더샘플링을 진행해 주는데 정말 쉽습니다.
    - 단순히 undersample api를 사용해주면되는데
    - 이때 주의할 점은 독립변수와 종속변수를 구분해주어야한다는 점 정도밖에 없습니다.
- 그 결과는 수가 적은 클래스에 많은 클래스가 맞춰진 모습을 볼 수 있네요





**5.6.2 오버샘플링**

- 다음으로는 **오버샘플링** 기법에 대해 알아보겠습니다.
- 오버샘플링은 언더샘플링과 반대로, **소수 클래스의 데이터를 인위적으로 늘리는 방식**입니다.
- 이 방법은 **다수 클래스는 그대로 두고, 소수 클래스 쪽만 복제하거나 생성해서 수를 맞추는 방식**입니다.

- 앞서 본 예시에서 소수 클래스의 데이터가 100개였다면, 오버샘플링을 통해 이 100개를 복제하여 1900개 수준까지 증가시킵니다.
- 이렇게 하면 데이터 손실 없이 균형을 맞출 수 있어, 일반적으로는 언더샘플링보다 더 안정적인 성능을 기대할 수 있습니다.

- 특히 데이터 수가 적은 상황에서는 **모델이 패턴을 학습하기에 부족할 수 있기 때문에**, 오버샘플링은 매우 유용하게 쓰입니다.
- 단점은, 같은 데이터를 복제하다 보니 **과적합(overfitting)**이 발생할 수 있다는 점인데요,
- 이를 보완하기 위해 최근에는 단순 복제 대신, **SMOTE**처럼 새로운 데이터를 생성하는 방식도 많이 사용됩니다.

- 오버샘플링은 다음과 같은 경우에 적합합니다:
  - 소수 클래스의 데이터 수가 너무 적어 학습이 어려운 경우
  - 원본 데이터 손실 없이 불균형만 해결하고 싶은 경우
  - 더 복잡한 비선형 모델을 적용할 계획이 있는 경우

- 다만, 오버샘플링을 했다고 해서 무조건 좋은 결과가 나오는 것은 아니기 때문에,
  - **학습 후 평가 지표를 통해 성능을 지속적으로 확인하는 것**이 필요합니다.



- 예시 코드도 언더샘플링과 동일합니다.
- 다만 그 결과가 많은 샘플을 기준으로 데이터가 늘어났다 정도로만 보시면 될 것 같습니다.



**정리하며**

- 마지막으로 정리를 하지면 불균형 데이터 문제는 **모델 성능에 큰 영향을 줄 수 있는 중요한 이슈**입니다.

- 언더샘플링은 빠르게 균형을 맞추는 방법이지만, 데이터 손실 위험이 있고,

- 오버샘플링은 더 많은 학습 데이터를 확보할 수 있지만, 과적합 가능성이 있습니다.

- 따라서 두 기법 모두 상황에 따라 적절히 활용하는 것이 중요하며,

- 분류 문제의 경우 종속변수의 분포를 보고 샘플링 기법을 적용하고

- 회귀 문제의 경우 독립변수 중 범주형 변수에 대한 분포를 보고 샘플링 기법을 적용하게 됩니다.

  

- 네 여기까지 데이터 전처리에 대한 내용을 마치도록 하구요.

- 마무리멘트

