🗣️ 강의: [ / ]

🐥 실습: [ / ]

🔑 해석 : [ / ]

- 결측치나 이상치가 없는 데이터

### Feature Engineering

- 실제 예시 - 김민성 차장님
- 휴일, 프로모션

**1. 시간**

🗣️ 강의: [ 3:00/ 3:00 ]

- 어려운 것이 아니니 한 2분정도 기다리도록 하겠습니다.

🐥 실습: [ 2:00 / 5:00 ]

````python
# 시간 변환
solar["DATE_TIME"] = pd.to_datetime(
    solar["DATE"].astype(str) + " " + solar["HOUR"].astype(str) + ":00:00"
)
solar.drop(columns=["DATE", "HOUR"], inplace=True)
````

🔑 해석 : [ 2:00/ 7:00]

**2. 풍속**

🗣️ 강의: [ 2:00/ 9:00]

**3. 비, 눈**

🗣️ 강의: [ 2:00/ 11:00]

- 미션 수행 시간은 5분정도면 충분할 것 같습니다.

🐥 실습: [ 5:00 / 16:00]

🔑 해석 : [ 2:00 / 18:00]

````python
# Feature engineering
solar["RAIN_SNOW"] = solar["RAIN"] + solar["SNOW"]
solar.drop(columns=["RAIN", "SNOW"], inplace=True)
````

````python
# 'DATE_TIME'을 인덱스로 설정
solar.set_index("DATE_TIME", inplace=True)

# 날짜별, 시간별로 SNOW와 RAIN의 합계 계산
df_sum = solar.resample("H").mean(numeric_only=True)
solar.reset_index(inplace=True)

# 시계열 그래프 생성
plt.figure(figsize=(15, 4))
df_sum["RAIN_SNOW"].plot(label="RAIN_SNOW", color="orange")
plt.title("Hourly Sum of RAIN_SNOW")
plt.xlabel("Date Time")
plt.ylabel("Sum")
plt.legend()
plt.show()
````



**4. Feature Engine library** 

- 이렇게 피쳐 엔지니어링을 소개했는데, 강의를 준비하다보니 재밌는 라이브러리가 있어서 소개를 드리고자 합니다.
- **Feature-engine**은 머신러닝 모델을 위해 특성(Feature)을 생성·선택할 수 있는 여러 변환기(Transformer)를 제공하는 파이썬 라이브러리입니다. **Scikit-learn**과 동일하게 fit()으로 학습하고 transform()으로 데이터를 변환합니다.

- 주기 특성들에 대한 생성
  - 일부 특성은 본질적으로 주기적이라고 합니다.
  - 시간 데이터 뿐만아니라, 자연 현상들도 주기성을 띈다고하는데요
  - 여기서 우리의 분석 목표인 태양광 발전량도 예시로 들고 있네요
  - 그리고 아래 부분에 주기적 변수를 원의 좌표로 변환하는 아이디어를 논문으로 발표한 사례를 말하고 있고
  - 이를 위해 사인 코사인 변환을 수행했다고 합니다.
  - 저희 예시에서 시간, 바람 등을 변환한 것과 같네요
- 수학적 파생 변수
  - 이건 상대적으로 쉬운데요 총 결제 횟수와 평균 결제 횟수등의 내용이 있을때 이를 합하거나 곱하는 과정을 소개하고있습니다.
  - 이것 또한 눈, 비를 더하여 파생변수를 만든 과정과 같구요
- 트리기반의 파생변수
  - 마지막으로는 트리기반의 파생변수인데
  - 1개 이상의 변수을 모델링을 통해서 종속변수를 예측하고, 이를 최종 목적에 활용하기 위해 추가한다는 개념입니다.
  - 이 부분이 조금 재밌다고 느껴지는데요.
  - 모델에 사용하기 위한 변수를 다른 모델을 통해 생성한다 라는 개념이 앙상블의 개념과 유사한 것 같습니다.
- 실제 프로젝트를 수행하던 시점에서는 개념적으로 알고만있어서 

🗣️ 강의: [ 4:30/ 22:30]

**4. 시계열 데이터 분할**

🗣️ 강의: [ 1:00/ 23:30]

- 실습 시간은 5분으로 하겠습니다.

🐥 실습: [ 5:00 / 28:30 ]

🔑 해석 : [ 3:30 / 32:00 ]

````python
# 전체 데이터 날짜
date_array = solar["DATE_TIME"].unique()

# 전체 데이터 길이 계산
total_dates = len(date_array)

# 70%와 90% 위치 계산
index_70 = int(total_dates * 0.7)
index_90 = int(total_dates * 0.9)

# 해당 위치의 날짜 추출
date_70 = date_array[index_70]
date_90 = date_array[index_90]

print(f"70%에 해당하는 날짜: {date_70}")
print(f"90%에 해당하는 날짜: {date_90}")
````

````python
# 데이터 셋 분할
train_data = solar[solar["DATE_TIME"] < date_70].reset_index(drop=True)
validation_data = solar[
    (solar["DATE_TIME"] >= date_70) & (solar["DATE_TIME"] < date_90)
].reset_index(drop=True)
test_data = solar[solar["DATE_TIME"] >= date_90].reset_index(drop=True)

print("train data:", train_data.shape)
print("validation data:", validation_data.shape)
print("test data:", test_data.shape)
````

````
💥 **MISSION**

> 각 데이터셋의 발전소 분포 시각화 해석

- 각 발전소 별로 모두 동일한 데이터 양을 가지고 있음
- 즉, 모든 데이터셋에서 비율이 동일함
- 7:2:1의 비율로 데이터가 분할 되었음
````



**5. 스케일링**

🗣️ 강의: [ 1:00 / 33:00]

- 실습 시간은 5분으로 하겠습니다.

🐥 실습: [ 5:00 / 38:00 ]

🔑 해석 : [ 3:00 / 41:00 ]

````Python
# 회귀 분석이기에 MinMaxScaler 사용
from sklearn.preprocessing import MinMaxScaler

scale_columns = [
    "CAPACITY",
    "GHI",
    "DNI",
    "DHI",
    "TEMP",
    "CLOUDS",
    "DEWPT",
    "PRES",
    "RH",
    "SLP",
    "UV",
    "VIS",
    "HUMIDITY",
    "Day sin",
    "Day cos",
    "Year sin",
    "Year cos",
    "Wx",
    "Wy",
    "RAIN_SNOW",
]

scaler = MinMaxScaler()
# Train 데이터의 fitting과 스케일링
scaler.fit(train_data[scale_columns])
scaled_data = scaler.transform(train_data[scale_columns])

# 데이터프레임 변환
scaled_df = pd.DataFrame(scaled_data, columns=scale_columns)

scaled_train_data = pd.concat(
    [train_data.drop(scale_columns, axis=1), scaled_df], axis=1
)
scaled_train_data.head()
````



````python
# 검증용 데이터 스케일링 적용
scaled_data = scaler.transform(validation_data[scale_columns])
scaled_df = pd.DataFrame(scaled_data, columns=scale_columns)
scaled_val_df = pd.concat(
    [validation_data.drop(scale_columns, axis=1), scaled_df], axis=1
)

scaled_val_df.head()
````



````python
# 테스트 데이터 스케일링 적용
scaled_data = scaler.transform(test_data[scale_columns])
scaled_df = pd.DataFrame(scaled_data, columns=scale_columns)
scaled_test_df = pd.concat([test_data.drop(scale_columns, axis=1), scaled_df], axis=1)

scaled_test_df.head()
````



**6. 예측 모델 생성**

[2:30]
