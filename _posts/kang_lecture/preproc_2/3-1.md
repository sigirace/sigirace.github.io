### warm up

- 이번 수업의 시작은 가볍게 “Pandas가 뭐예요?“라는 질문으로 열어보겠습니다.
- 판다스라고 하면 저는 처음에 이렇게 귀여운 팬더 이미지가 생각이 났었는데요 그래서 더 정이 가는 것일 수도 있습니다. 
- ‘Pandas’는 동물이 아닌, Python에서 데이터를 조작하고 분석하는 데 아주 강력한 **Python의 데이터 분석 라이브러리**입니다.
- 특히 대용량 데이터를 쉽고 빠르게 다룰 수 있도록 설계된 라이브러리예요.
- 우리가 흔히 사용하는 엑셀처럼 **표 형태의 데이터**, 즉 테이블 데이터를 다루는 데 최적화되어 있고요,
- 그래서 엑셀과 유사한 기능도 많이 갖추고 있습니다.



- 그럼 본격적으로 Pandas와 Excel을 비교해 보도록 하겠습니다.
- [읽기]
- 이처럼 Pandas는 단순히 데이터를 다루는 수준을 넘어서, **자동화**, **대규모 처리**, **시각화**, **확장성**까지 모두 고려할 수 있는 도구입니다.



- pandas 세트로 같이 묶여서 나오는 라이브러리 중 하나는 넘파이가 있습니다.
- Numpy는 숫자나 배열 계산에 특화된 Python 라이브러이며
- Pandas는 이 Numpy 위에서 만들어졌고, 덕분에 Pandas도 수치 계산이 빠르고 효율적입니다.
- 즉, Numpy는 **숫자 중심 계산**에 강하고, Pandas는 **표 형태 데이터(행/열 데이터)** 분석에 특화되어 있다고 보시면 됩니다.



- 이번 웜업은 Pandas의 개념을 아주 가볍게 맛보는 시간이었습니다.
- 앞으로 Pandas를 통해 여러분이 얼마나 쉽게, 또 강력하게 데이터를 다룰 수 있는지 직접 체험하시게 될 거예요.
- 그럼 본격적인 실습으로 들어가 보겠습니다! 🙌



[3:30]



### 강의

```
https://colab.research.google.com/drive/1_fpLsEtMKbvXHNBi4EPJRBfx0fhL0Res?usp=copy
```



-  첫번째로 **Pandas의 기본 구조**에 대해 배워보겠습니다.

- Pandas에서 데이터를 다룰 때는 크게 두 가지 구조가 있습니다.

- 바로 **Series**와 **DataFrame**입니다.

- 먼저 **Series**부터 살펴볼게요.

- Series는 **1차원 배열** 구조입니다.

- 근데 단순한 배열이 아니라, 각 값에 **라벨**이 붙어 있다는 게 특징이에요.

  

- 예를 들어, 사과 대 400, 4 대 달러

- 이런 식으로, 데이터와 함께 그에 대한 라벨이 같이 있는 구조입니다.

- 이는 라벨이 단순히 순서가 아니라, **지정한 라벨을 기준으로 데이터에 접근할 수 있다**는 장점이 있어요.

- 앞서 배운 Python의 딕셔너리랑 비슷하게 생겼죠?



- 다음은 **DataFrame**입니다.

- DataFrame은 행과 열, 즉 **2차원 배열 구조**예요.

- 우리가 흔히 아는 **엑셀 테이블**과 거의 동일한 형태라고 보시면 됩니다.

  

- 시리즈에서는 데이터를 기준으로 라벨이 하나만 존재했다면 데이터프레임은 라벨이 행과 열에 모두 붙어있다고 볼 수 있습니다.

- 따라서 이 라벨을 통해 데이터를 빠르게 찾을 수 있게 됩니다.

- 또한 각 **열(column)**은 서로 다른 데이터 타입을 가질 수도 있고요,



- 화면에 보이시는 표를 예로 들어 설명드리면,
- 김철수, 김영희라는 학생이 행의 라벨에 붙어있고 각각 **서울대, 고려대, 연세대**가 열의 라벨에 붙어 지원 결과가 들어있습니다.
- 행 인덱스는 사람 이름, 열 라벨은 대학 이름이 되는 구조죠.
- 여기서 눈여겨 볼 점은 라벨에는 동일한 데이터가 들어갈 수 있으며 각 셀에는 타입의 제한이 없는 다양한 데이터가 포함될 수 있다는 점입니다.



- 그럼 이제 **DataFrame을 어떻게 다루는지** 간단히 설명드릴게요.
- DataFrame은 분석을 시작하기 전, **가장 먼저 데이터를 불러오고** 그 데이터를 **요약하거나**, **형태를 확인하거나**, **통계 정보를 확인**할 때 자주 사용됩니다.
- Pandas에서는 이 작업이 API 아주 쉽게 가능하고요,
- 데이터 분석을 하다 보면 **데이터의 타입**도 바뀌는 경우가 있는데,
- Pandas는 이걸 자동으로 처리해주기도 하고, 우리가 직접 수정할 수도 있습니다.

[3:00 / 7:00]



**📘 3.2.2 DataFrame 선언하기**

- 이번에는 Pandas에서 **DataFrame을 생성하는 방법**에 대해 알아보겠습니다.
- DataFrame을 만드는 방법은 여러 가지가 있지만, 오늘은 **가장 기본적인 두 가지 방식**을 소개해드릴게요.

**✅ Case 1. 배열 사용**

- 먼저 **배열을 사용해서 DataFrame을 생성하는 방법**입니다.

```
import pandas as pd

array = [['kor', 80], ['math', 70]]
```

- 여기서 array는 2차원 리스트 형태입니다.
- 배열의 형태를 시각화하면 아래와 같구요
- 이 배열을 데이터 프레임 형태로 선언하기 위해서는 아래와 같은 명령어를 사용하면 됩니다.

```
df = pd.DataFrame(array, columns=['subject', 'score'])
df
```

- 이때 각 내부 리스트는 한 행(row)을 의미하고, columns 인자는 열 이름을 지정하는 데 사용됩니다.
- 이처럼, **배열 기반**으로 생성할 때는 **행 중심**으로 데이터가 구성된다고 기억하면 좋을 것 같습니다.



**✅ Case 2. 딕셔너리 사용**

- 두 번째 방법은 **딕셔너리를 이용하는 방식**입니다.

```
df2 = pd.DataFrame({
    'subject': ['kor', 'math'],
    'score': [80, 70]
})
df2
```

- 이 방식은 **열(column) 중심의 구성**입니다.
- 딕셔너리의 **key**는 열 이름, **value**는 각 열의 데이터 목록이 됩니다.
- 결과는 앞서와 동일하게 출력됩니다.
- 이 방식은 코드가 좀 더 직관적이고 읽기 쉬워서 실무에서 많이 사용됩니다.



- 하나만 더 확인을 해보도록 할까요?
- 만약 데이터가 가진 길이가 서로 다르다면 어떻게 될까요?
- [코드 작성]
- 아 에러가 나게 되네요, 기본적으로 판다스에서 자동으로 길이를 맞춰서 데이터 프레임을 맞춰주는 방식은 없습니다.
- 그래서 이럴 경우 보통 먼저 데이터의 길이를 선행적으로 파악하여야 하는 경우가 발생합니다.
- 만약 이것도 자동으로 맞춰줘! 라는 기능이 필요하다면
- zip_longest라는 방식을 사용할 수 있는데요 코드는 이러합니다.

```python
from itertools import zip_longest

data = list(zip_longest(['kor', 'math'], [80, 70, 100]))
df3 = pd.DataFrame(data, columns=['subject', 'score'])
df3
```

- 그런데 데이터들은 어떠한 인덱스를 기준으로 맞춰주는 경우가 대부분이고
- 이런식으로 단순 데이터의 나열만 가진 데이터는 거의 없기 때문에 잘 사용하지는 않는 것 같습니다.



- 정리를하자면 **리스트 기반** 방식은 행 중심, **딕셔너리 기반** 방식은 열 중심입니다.
- 두 방식 모두 같은 결과를 만들 수 있지만, 상황에 따라 더 편리한 방법을 선택해서 사용할 수 있어요.
- 물론 딕셔너리 기반이 훨씬 가시성이 높긴 합니다.



[3:00 / 10:00]



**📘 3.2.3 DataFrame 출력하기 – 강의 스크립트**

- 이번에는 실제 데이터를 불러와서 **DataFrame 형태로 출력**해보는 실습을 진행하겠습니다.



- 이번 실습에서는 머신러닝 예제에서 자주 쓰이는 **붓꽃(iris)** 데이터를 사용하겠습니다.
- 이 데이터는 Python의 scikit-learn 라이브러리, 줄여서 sklearn에서 기본으로 제공하는 예제 데이터예요.
- 머신러닝 연습용으로 많이 사용되며, 다양한 **꽃잎의 길이, 너비 등 측정값**이 포함되어 있습니다.

**🔧 실습 코드**

```
from sklearn.datasets import import load_iris

iris = load_iris()
iris = pd.DataFrame(iris.data, columns=iris.feature_names)
iris
```

- load_iris()는 데이터를 불러오는 함수입니다.
- iris.data에는 데이터 값들이 들어 있고, 
- iris.feature_names는 각 열의 이름(컬럼명)을 의미합니다.
- 마지막 줄에서 Pandas의 DataFrame으로 변환해주고 있습니다.



- 이제 이 iris DataFrame을 화면에 출력해보면,
- 꽃잎의 길이와 너비 같은 여러 특성 값들이 테이블 형태로 나타납니다.



- 전체 데이터를 다 보기엔 너무 많기 때문에,
- 앞부분 또는 뒷부분 일부만 미리 보고 싶을 땐 head() 또는 tail() 함수를 사용합니다.
- 이때, head()나 tail()에 인자값을 지정하지 않으면 기본값은 **5개**를 출력하여 보여줍니다.
- 더하여 뭐 내가 어느 범위만 보고싶다 하면 우리가 배운 슬라이스를 사용하시면 됩니다.

```
iris[10:30]
```

- 이렇게 상세 데이터를 쿼리해서 보는 내용은 뒤에 다시 한번 다뤄보도록 하겠습니다.



[3:00 / 13:00]



**📘 3.2.4 DataFrame 요약 및 통계 확인하기 – 강의 스크립트**



- 다음으로는 불러온 DataFrame에 담긴 iris 데이터를 요약하고, **기초 통계 정보를 확인**하는 방법에 대해 알아보겠습니다.
- 여기서 우리가 중점적으로 확인할 내용은
- 데이터에 어떤 **열(column)**이 있고, 각 열에 **몇 개의 값**이 있으며, 그 값들의 **분포와 요약 통계**가 어떤지 확인하는 것이 목적입니다.

```
iris.info()
```

- 먼저 info() 함수부터 확인해볼게요.
- 이 코드는 DataFrame의 **전체적인 구조를 요약**해 보여줍니다.
- 확인해 보면 총 150개의 데이터가 있고
- 4개의 열: 'sepal length', 'sepal width', 'petal length', 'petal width'이 존재하며
- 모두 빠짐없이 150개의 값이 존재 (→ Non-Null Count 전부 150)하는 것을 알 수 있습니다.
- 결측치가 없다는 정보는 매우 중요하고 유용하기에 자주 사용하는 명령어가 될 것입니다.
- 마지막으로 데이터 타입은 모두 **float64**, 즉 실수형 데이터이네요.

```
iris.info()
```

총 150개의 데이터가 있고

​	•	4개의 열: 'sepal length', 'sepal width', 'petal length', 'petal width'

​	•	모두 빠짐없이 150개의 값이 존재 (→ Non-Null Count 전부 150)

- 이렇게 info()는 데이터 수, 결측치 여부, 데이터 타입을 빠르게 확인할 때 유용하며 저는 데이터 분석 과제의 첫 수행을 항상 이 함수로 시작하고 있습니다.
- 그리고 제가 말씀드렸던 해석 포인트들은 이런식으로 아래에 담겨있습니다.



- 이번에는 describe() 함수를 실행해봅니다.

```
iris.describe()
```

- 이 함수는 **각 열의 주요 통계 요약**을 보여줍니다.
- **count**: 값의 개수
- **mean**: 평균
- **std**: 표준편차
- **min / max**: 최소값 / 최대값
- **25%, 50%, 75%**: 분위수 (중앙값 포함)



- 여기서 제가 주로 보는 데이터의 특성은 평균, 최소값, 최대값인데요.
- 최소값 최대값은 주로 이상 데이터가 들어있을 경우 눈에 확 띄기 때문에 가장 첫번째로 확인합니다.



- 다음으로는 평균과 표준편차를 비교해서 확인하는 편입니다.
- 아시다싶이 평균은 데이터의 대표값으로 주로 어디에 데이터가 몰려있는가를 판단할 수 있는 지표입니다.
- 그리고 표준편차는 데이터의 퍼짐 정도를 주로 파악하는데, 정확히는 평균에서부터 얼마나 데이터가 멀어져있냐 라는 것이죠.
- 저는 이 표준편차를 통해 보통 평균에 비해서 얼마나 값이 크냐, 작냐로 데이터가 sparse한지 dense한지를 파악합니다.
- 이를 수치로 확인하기 위해서 데이터의 범위와 비교할 수가 있는데 코드를 한번 작성해 볼까요?
- 이 데이터에서는 petal length가 다른것에 비해 표준편차가 좀 큰 것으로 나오니 한번 확인해 보겠습니다.

```python
_range = iris["petal length (cm)"].max() - iris["petal length (cm)"].min()
_percent = iris["petal length (cm)"].std() / _range
print(_percent)
```

- 비율이 약 0.3 정도 나오는 것을 볼 수 있는데요
- 보통 0.1 이하면 매우 몰려있고, 0.3에서 0.5면 적당히 퍼져있으며, 0.6 이상이면 매우 넓게 퍼져있다고 볼 수 있습니다.
- 이 데이터는 적당히 분포되어있음을 볼 수 있네요



- 이 외로도 데이터가 퍼져있음을 확인하는 방법은 정규분포라는 가정하에 z-score를 통해 볼 수 있구요
- 실제 업무에서는 도메인 지식을 기준으로 판단하곤 합니다.
- 예를들어 학생 키의 표준편차가 10인 것은 density하다고 볼 수 있지만
- 은행 금리의 표준편차가 0.5%라는 것은 또 매우 Sparse하다고 볼 수 있죠





- 아무튼 다시 iris 데이터로 돌아오게되면
- sepal length는 4.3 ~ 7.9 사이의 값을 가집니다. petal width는 0.1 ~ 2.5로, 범위가 매우 작다는 특징을 볼 수 있습니다.
  - 생각해보니 이게 범위에 비해 편차가 큰것 같네요?
  - 앞서 비율을 확인하는 코드로 다시 한번 확인해 보도록 하겠습니다.
- 이처럼 describe는 **값의 분포가 어떤지**, **어떤 열이 스케일이 다른지** 빠르게 파악하는 데 유용합니다.



- 다음으로 더 알아보기라고 적어놓으면서 다시한번 질문을 던질 수 있을 것 같습니다.
- 🤔 “열마다 값의 범위가 다르면 분석에 영향이 있을까요?”
- 맞습니다.
- 예를 들어 어떤 컬럼, 특성은 은 0-1 사이, 어떤 컬럼은 100-1000 사이 값을 가진다면
- 모델이 큰 값을 가지는 범위의 특성을을 더 중요하다고 오해할 수 있습니다.
- 따라서 이럴 경우에는 **정규화나 표준화**와 같은 전처리로 각 컬럼의 스케일을 맞춰줄 필요가 있습니다.
- 이 과정은 뒤에서 자세히 다뤄보도록 하겠습니다.



- 이렇게 info()와 describe()는 데이터 분석에서 꼭 필요한 **기초 탐색 함수**입니다.
- 아마 pandas 강의에서 가장 중요한 부분이 아닐까 싶어요.
- 앞으로 분석할 데이터를 처음 접했을 때는 **무조건 이 두 함수부터 실행**하는 습관을 들이시면 좋습니다!

[08:00 / 21:00]





**📘 3.2.5 DataFrame 인덱스 핸들링 – 강의 스크립트**



- 이번에는 Pandas에서 **DataFrame의 인덱스를 자유롭게 다루는 방법**에 대해 알아보겠습니다.
- Pandas에서 인덱스는 **각 행(row)의 고유한 위치를 나타내는 값**입니다.
- 기본적으로는 0부터 시작하는 정수형 인덱스가 자동 부여됩니다.
- 앞서 생성했던 데이터 프레임을 다시 한번 확인해 볼까요?
- 여기서 왼쪽의 0, 1이 **기본 인덱스**입니다.

```
df = pd.DataFrame({'subject': ['kor', 'math'], 'score': [80, 70]})
print(df)
```



**📌 인덱스 확인 및 변경**

- 이제 인덱스만 확인해보도록 하겠습니다.
- 인덱스만 추출하는 방법은 데이터프레임 변수에 .index 명령어를 사용하시면 되구요

```
list(df.index)
```

- 일반적으로 list 형식으로 출력하지 않으면 range 형태로 나오기에 
- list를 사용하여 인덱스가 배열 형태로 예쁘게 보이게 할 수 있습니다.

**🧪 인덱스 직접 지정**

- 다음으로는 인덱스를 원하는 형태로 바꾸는 방법입니다.
- 앞서 인덱스를 리스트로 뽑아낸 것처럼 새로운 인덱스를 리스트로 재지정하면 기존 인덱스를 우리가 직접 정한 값으로 바꿀 수 있게 됩니다.

```
df.index = ['A', 'B']
print(df)
```



**🛠️ set_index() 함수로 인덱스 설정**

- 특정 **열(column)**을 인덱스로 사용하고 싶을 땐 set_index() 함수를 사용합니다.
- 주로 cdate와 같은 순서를 나타내는, 인덱스 성격의 데이터가 컬럼으로 속해있을때 많이 사용되곤 합니다.
- 해당 api의 주요 파라미터를 살펴보자면
- 보고 읽기

```
df.set_index('subject', drop=True, append=False, inplace=True)
```

- 그래서 우리는 원본 데이터 프레임에서 과목에 대한 컬럼을 인덱스로 사용하며,
- 기존의 컬럼은 인덱스로 내림과 동시에 없앨 것이고
- 기존의 인덱스 또한 유지하지 않으며
- 원본 자체를 변경하겠다 라는 코드로 해석 될 수 있습니다.



🔄 reset_index() 함수로 인덱스 초기화

- 그런데 우리가 인덱스를 설정하고 나니 조금 이상하다.. 라고 생각되어 기존 인덱스를 **다시 숫자 인덱스로 초기화**하고 싶을 땐 reset_index()를 사용합니다.
- 해당 api에서 drop은 인덱스 열을 컬럼으로 복원할지 여부

```
df.reset_index(drop=False, inplace=True)
```

- 이처럼 인덱스를 다시 **일반 열로 되돌릴 수 있습니다.**



[3:00 /24:00]



**📘 3.2.6 DataFrame의 열(column) 핸들링 – 강의 스크립트**



- 이번에는 **DataFrame의 열, 즉 컬럼을 다루는 방법**에 대해 알아보겠습니다.

**✅ 1. 컬럼 이름 확인하기**

- 먼저 현재 DataFrame에 어떤 컬럼들이 있는지 확인해봅시다.

```
iris.columns
```

- 출력 결과는 Index 객체로, 열 이름들이 리스트처럼 출력됩니다.

```
Index(['sepal length (cm)', 'sepal width (cm)', ...])
```

이처럼, columns 속성은 **열 이름 전체를 확인**할 수 있게 해줍니다.





**✅ 2. 컬럼 이름 변경하기**

- 기존 컬럼 이름이 너무 길거나 공백이 많을 경우, 우리가 원하는 형태로 **일괄 변경**할 수 있습니다.
- 우리가 가진 아이리스 데이터의 컬럼은 뒤에 단위도 붙어있고, 공백도 있기에 데이터를 처리하기 쉽지 않습니다.
- 아까처럼 매번 복사 붙여넣기를 하지않으면 오류가 날 수 있죠.

```
iris.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
iris
```

- 따라서 기존의 공백과 괄호 등을 제거하고 가독성 좋게 snake_case로 수정한 예시입니다.
- python에는 이렇게 snake_case를 사용하는 것이 규칙이며, 이런 식으로 컬럼명을 일괄 수정하면 이후 데이터 다루기가 훨씬 편해집니다.



- 다음으로는 컬럼의 데이터 타입 확인 및 변경을 수행해보도록 하겠습니다.
- 데이터 타입은 분석 전 반드시 확인해야 할 중요한 정보입니다.
- 앞으로 전처리 과정에서 다양한 연산을 수행하게 될텐데, 형이 맞지 않는다면 오류가나기 때문이죠.
- Pandas에서는 dtypes 속성으로 데이터 타입을 확인할 수 있습니다.

```
iris.dtypes
```

| **타입** | **의미**             |
| -------- | -------------------- |
| int      | 정수형               |
| float    | 실수형 (소수점 있음) |
| bool     | True/False           |
| datetime | 날짜/시간 데이터     |
| object   | 문자열 등 기타       |

- 다시한번 파이썬의 데이터 타입을 말씀드리자면
- int는 정수 float은 실수 bool은 True/False datetime은 날짜/시간 그리고 그외 스트링을 포함한 데이터 자료형들은 object로 표현됩니다.



**✅ 2. 타입 변경하기: astype()**

- 다음은 타입 변경입니다.
- 앞서 파이썬 과정에서 단일 자료형의 타입을 변경하려면 앞에 변경하고 싶은 타입을 적어주었습니다.
- 하지만 컬럼 자체를 변경하고 싶으면 pandas의 astype api를 사용하면 됩니다.

```
iris['sepal_length'] = iris['sepal_length'].astype('int')
iris.dtypes
```

- 이렇게 하면 **소수점이 제거**되고, 타입이 정수형으로 바뀐 걸 확인할 수 있습니다.
- ⚠️ 단, astype으로 바꿀 때는 **데이터 손실 여부**에 주의해야 합니다.
- 예시로 실수 → 정수로 바꿀 경우 소수점이 잘려 나가기 때문이죠.

[3:00 / 27:00]





**📘 3.3 row / column 핸들링 – 강의 스크립트**

- 다음으로 행과 열을 선택하여 데이터를 핸들링하는 방법을 알아보겠습니다.

  

**🔹 3.3.1 row / column 선택 조회**

**✅ 1. 행(row) 선택 – 슬라이싱**

- 먼저 DataFrame에서 행만 선택하고 싶을 땐 슬라이싱을 사용합니다.
- 슬라이싱을 확인하기위해 다시 원본데이터를 불러오구요.
- 데이터를 불러온 다음 슬라이싱을 통해 특정 행을 가져오도록 해보겠습니다.
- 슬라이싱의 개념은 처음부터 끝 직전까지라고 말씀을 드렸습니다.
- 따라서 여기서는 1번~3번 인덱스 행을 선택합니다.

```
iris[1:4]
```



**✅ 2. 열(column) 선택 – 열 이름 사용**

- 다음으로는 특정 컬럼을 선택해서 조회해보도록 하겠습니다.
- 이는 컬럼명 또는 컬럼명의 리스트를 사용하는데요
- 키 포인트로 컬럼명은 시리즈를 반환하며 컬럼명 리스트는 데이터 프레임을 반환한다 라고 적어놓았습니다.
- 이를 유의하며 먼저 먼저 단일 열을 조회해보도록 하겠습니다.

```
iris['petal length (cm)'].head()
```

- 이런식으로 데이터 프레임처럼 나오고있는데 컬럼명 단일로 조회하여 일차원 데이터가 나온다 하면 **Series** 형태로 보시면 됩니다.
- 이전에는 가독성이 좋지 못했는데 판다스 버전이 업데이트되면서 가독성이 좋게 바뀌었네요
- 타입을 확인하면 시리즈인것을 알 수 있습니다.



- 다음으로는 복수 열을 조회해 보도록 하겠습니다.

```
iris[['petal length (cm)', 'petal width (cm)']].head()
```

- 이런식으로 리스트로 여러 열을 넘기면 결과는 **DataFrame**임을 확인 할 수 있습니다.



[2:00/ 29:00]



**🔹 3. row와 column 동시 선택**

- 다음으로는 Row와 컬럼 둘다 선택해서 쿼리하는 방식입니다.
- 지금까지는 단일 행 또는 열만을 가지고 데이터를 가지고 왔으나, 이를 통해서 특정 데이터 한 셀을 가지고 오는 쿼리 혹은 연속적이지 않은 데이터 구간을 가지고 오는 쿼리는 수행하기 어렵습니다.
- 이를 위해 Pandas에서 데이터를 다룰 때 **행(row)** 또는 **열(column)**을 자유롭게 선택하거나 수정할 수 있도록 iloc과 loc이 사용됩니다.
- row와 컬럼이 쉼표 기준으로 구분되며 인수를 하나만 설정한 경우에는 행만을 사용하게 됩니다.
- 인수로는 정수, 리스트, 슬라이싱 모두 사용할 수 있으며 loc에서의 슬라이싱은 우리가 배운 슬라이싱이 아닌 진짜 시작부터 끝을 나타냅니다.
- 실습을 통해 그 이유를 알아보겠습니다.



**✅ 3-1. .iloc[] – 인덱스를 기준으로 선택**

- 첫번쨰로 위치를 기준으로 선택하는 iloc입니다.
- 이때 I가 붙었죠, 이 i는 index이지만 실제 데이터 프레임의 인덱스가 아닌 위치로 기억해주세요
- 우리는 set_index로 인덱스를 숫자가아닌 다른것으로 바꿀 수 있었습니다.
- 하지만 이런 경우에도 데이터프레임은 행의 위치를 또 다른 인덱스로 삼아 우리에게 행의 위치에 따른 데이터를 제공해 줄 수 있습니다.

```
iris.iloc[1:4]
```

- 행 번호 기준으로 슬라이싱을 수행했고 iloc이기에 우리가 알고있는 일반적인 슬라이싱 끝 직전까지가 적용되어있습니다.

```
iris.iloc[[1, 3, 5], 2:4]
```

- 이거는 [row, column] 형식으로 행에는 특정 인덱스 번호가 열에는 슬라이싱이 적용되어있네요
- 이를 수행하면 1,3, 5번째 행과 2에서 3번 열이 출력됨을 알 수 있습니다.





**✅ 3-2. .loc[] – 레이블(이름)을 기준으로 선택**

- 다음은 loc입니다.
- 대부분의 내용은 iloc과 같으나 차이점은 loc은 위치가 아닌 라벨 기준으로 선택이됩니다.
- 라벨은 처음 우리가 데이터프레임을 배울때 확인했던 이름표와 같습니다.

**▶ 레이블 변경 실습 예시**

```
# loc에 대한 이해를 위해 인덱스를 변경
iris_index = list(iris.index)
iris_index[0] = '파이썬'
iris.index = iris_index
iris.head()
```

- 먼저 라벨 기준으로 데이터를 가져오는 것을 확인하기 위해 인덱스를 변경해보도록 하겠습니다.
- 앞서 리스트 형식으로 인덱스를 변경하는 것과 동일한 코드로 첫번째 인덱스를 파이썬으로 변경하였구요.

```
iris.loc['파이썬':3]
```

- 이제 파이썬 라벨 행부터 3번째 행까지 슬라이싱을 수행해 보겠습니다.
- 여기서 알 수 있는 점은 라벨 기준으로 슬라이싱을 수행할때, 기존 파이썬의 슬라이싱처럼 끝 직전까지 라는 의미가 어렵다는 것을 알 수 있습니다.
- 왜냐하면 라벨은 스트링 문자열이든 인트형이든 어느것이나 될 수 있기 때문이지요.
- 그래서 loc을 사용하는 인덱싱은 시작부터 끝까지라는 의미를 가질 수 밖에 없습니다.

```
iris.loc[['피어슨', 3, 5], 'sepal width (cm)':'petal width (cm)']
```

- 컬러명도 슬라이스로 지정할 수 있습니다.
- 쉼표를 기준으로 왼쪽은 행 오른쪽은 열이되며 열에대한 슬라이싱이 세팔 위드부터 페탈 위드까지 수행한 된 것을 확인할 수 있습니다.



**✅ 5. 선택한 값 변경하기**

- 이런식으로 행과 열을 모두 사용하게 된다면 특정 위치의 값, 엑셀에서는 셀을 지정할 수 있고
- 해당 값만을 변경할 수 있습니다.
- 여기에서는 라벨 기준으로 데이터를 지정하는 loc을 사용해 파이썬 행의 세팔 렝스 열을 100으로 값 변경을 수행해 보겠습니다.
- 네 값이 잘 바뀐 것을 확인할 수 있습니다.



[5:00 / 34:00]





**📘 3.3.2 row / column Concat – 강의 스크립트**



- 이제 데이터를 가져오는 방법을 알았으니 데이터를 조합하는 방법을 알아볼 텐데요
- 첫번째로 두 개 이상의 DataFrame을 **붙이는 방법**, 즉 concat()을 사용하는 방법을 배워보겠습니다.
- concat은 **행(row)** 또는 **열(column)** 방향으로 데이터를 연결할 수 있도록 해주는 함수입니다.

**🔧 concat() 함수 구조**

```
pd.concat(objs, axis=0, ignore_index=False)
```

- 컨켓 api의 파라미터는 다음과 같습니다.
- 읽기



**✅ 실습 1 – 행(row) 추가**

```
score = pd.DataFrame({
    '국어': [80, 90, 70],
    '수학': [90, 70, 60]
}, index=['황길동', '김철수', '이영희'])
```

- 여기 이런식으로 학생 이름을 인덱스로해서 과목이 열이며 각 데이터는 과목점수를 가진 데이터 프레임이 있습니다.

```
new_student = pd.DataFrame({
    '국어': [100], '수학': [100]
}, index=['엄정아'])
```

- 다음으로 이와 동일한 구조의 새로운 데이터프레임을 생성하겠습니다.
- 생성한 데이터 프레임이 한 행 기준으로 작성된 모습을 볼 수 있습니다.



- 이제 새로 만든 데이터프레임을 기존 데이터 프레임에 붙여 볼 건데요
- 행 기준으로 붙일 것이기에 axis를 0으로 지정하였습니다.
- 그럼 가장 마지막 행에 새로운 데이터 프레임이 컨캣된 것을 볼 수 있습니다.



✅ 실습 2 – 열(column) 추가

- 이번에는 새로운 과목 과학 점수를 추가해보겠습니다.

```
new_subject = pd.DataFrame({
    '과학': [100, 90, 80, 100]
}, index=['황길동', '김철수', '이영희', '엄정아'])
```

- 데이터를 확인해 보면 기존 데이터 프레임과 동일한 행을 가지며 과학 점수에 대한 열이 추가된 것을 확인할 수 있는데요.



- 이제 이를 열 방향으로 기존 score 데이터프레임에 추가해보도록 하겠습니다.
- 열 기준으로 붙일 것이기에 axis를 1로 지정하였습니다.



- 이렇게 컨캣을 수행해보았는데요. 
- 컨캣을 수행할때 인덱스와 열 이름이 **정확히 맞아야** 자연스럽게 결합됩니다.
- 만약 다를 경우 NaN(결측값)이 들어갈 수 있기에 항상 주의해야 합니다.



[37]



**📘 3.3.3 row / column 삭제 – 강의 스크립트**



- 이제 데이터를 붙여보았으니 떼보기도 해야겠죠 이번에는 Pandas의 drop() 함수를 사용하여
- **특정 행(row) 또는 열(column)** 을 삭제하는 방법을 배워보겠습니다.

**✅ 1. drop() 함수 기본 설명**

```
DataFrame.drop(index=None, columns=None, inplace=False)
```

- drop api의 파라미터는 다음과 같습니다.
- 읽기



**✅ 2. 행(row) 삭제 – index 기준**

- 먼저 행을 삭제하기 위해서는 인덱스를 기준으로 삼아야 합니다.
- '엄친아'라는 인덱스를 가진 행을 삭제해 보겠습니다.

```
score = score.drop('엄정아', inplace=False)
```

- inplace=False이므로 원본은 그대로이고, 결과만 출력됩니다.



**✅ 3. 열(column) 삭제 – columns 기준**

```
score = score.drop(columns=['과학'], inplace=False)
```

- 다음으로 '과학' 열 전체를 삭제하는 예시입니다.



- 데이터 삭제는 간단히 drop이라는 api를 사용해 쉽게 진행할 수 있음을 확인했습니다.



[38]



**📘 3.4 조건에 맞는 데이터 탐색 및 수정 – 강의 스크립트**



- 다음으로는 Pandas로 **조건에 따라 Dataframe을ㅈ 선택, 탐색, 수정**하는 방법과 
- 전처리의 가장 핵심인 **결측치 처리 및 중복 제거**까지 실습을 통해 경험해 보겠습니다.



- 데이터 프레임에서 조건을 통해 데이터를 탐색하기 위해서는 조건식이 필요합니다.
- 이는 앞서 데이터에 대한 연산자를 통해 확인했던 내용이니 아마 쉽게 파악이 가능하실 것입니다.
- 조건은 보통 **컬럼을 기준으로 작성**하게 되는데요, 컬럼에 대한 데이터를 확인하는 것은 앞서 단일 컬럼, 컬럼 리스트를 통해 할 수 있다고 말씀드렸습니다.예를 들어 국어 점수가 80점을 초과한 학생만 보고 싶다면 다음과 같이 작성할 수 있습니다:



**✅ 1. 조건이 하나일 경우**

- 먼저 조건이 하나일 경우에 대해 알어보겠습니다.
- 먼저 학생의 국어점수가 80점 이상인 조건식을 작성해볼까요?

```
score['국어'] > 80
```

- 국어 점수가 80점 초과인 행만 출력되며, 데이터 자체가 아닌 참/거짓으로 구분된 시리즈가 생성되었습니다.
- 우리는 이렇게 나온 시리즈를 padnas의 필터링 기능을 통해 원하는 데이터만 추출할 수 있습니다.
- 따라서 이걸 기존 데이터프레임 안에 넣게 되면 True인 행만 출력하게 됩니다.



**✅ 2. 조건이 여러 개인 경우**

- 조건을 결합할 때는 **&(and)** 또는 **|(or)** 사용하며
- 주의점은 각 조건은 괄호로 감싸야 합니다

```
# 두 조건 모두 만족 (and)
score[(score['국어'] > 70) & (score['수학'] > 80)]

# 둘 중 하나만 만족 (or)
score[(score['국어'] > 85) | (score['수학'] > 85)]
```

- 각각 and 연산과 Or 연산을 통해 데이터를 추출함을 볼 수 있고
- 이 과정 안에는 조건에 대한 필터링이 포함되어있음을 설명드렸습니다.



[41]



**🔍 3.4.2 결측값 탐색 및 수정**

- 이제 우리는 데이터 프레임에서 원하는 데이터를 지정해서 가져오거나 조건에 따라 가져올 수 있습니다.
- 또한 더 특정한 조건인 결측치가 있는가를 조건으로 데이터를 필터링하여 가져올 수 있는데,
- 이번 섹션에서는 이에 대해 간단히 실습해보도록 하겠습니다.



**✅ 1. 결측값 탐색**

- 먼저 결측값 탐색 함수는 두개 isna와 isnull이 존재합니다.
- 반대로 결측값이 아닌 탐색은 Notna와 notnull이 있습니다.
- 둘의 기능적 차이는 없으니 원하는 걸 사용하셔도 무방하구요.
- 결측치 탐색의 결과는 앞서 우리가 조건을 통해 데이터를 살펴보았을때 와 동일하게 True, False인 형태로 나타나고 이를 데이터프레임에 적용하여 필터링할 수 있습니다.



- 먼저 데이터에 일부로 결측치를 포함해보도록 하겠습니다.
- 데이터 프레임을 생성할때는 서로간의 길이가 맞지않으면 오류가났었는데, 컨캣을 수행할때는 없는데이터는 결측인 Nan으로 표기됩니다.
- Nan은 저희가 지금 사용하고있는 pandas, numpy 라이브러리에서 제공하는 것으로 일반적인 Null과는 다릅니다.
- Null은 연산 자체가 불가능하지만 Nan은 결측임을 나타내기 위한 표현으로 float 값을 가지며 연산 또한 가능합니다.
- 다만 그 연산 결과가 Nan임으로 한번 결측이면 계속 결측인 것이 문제지만 말이죠.
- 아무튼 이런식으로 결측에 대한 내용을 탐색하면 앞서 말씀드렸듯, True/ False 형태로 필터링 가능한 형태로 나오게됩니다.



**✅ 2. 결측값 개수 확인**

- 이때 True는 1 False는 0으로 집계되어 결측치가 총 몇개있는가 에대한 결측치 합계 정보로 나타낼 수 있습니다.

- .sum()을 사용하면 True는 1로 계산되어 **몇 개가 결측인지 바로 확인**할 수 있습니다.

  

**✅ 3. 결측값 삭제**

- 다음으로는 결측치를 확인했다면 어떤 처리를 해주어야하는데 첫번째로 할 수 있는 수행이 바로 삭제입니다.
- 삭제를 위한 api는 dropna를 사용하며 그 안의 파라미터는
- [읽기]



- 이제 이에 대한 실습으로 그냥 결측치가 있다면 모두 지워주도록 하겠습니다.
- 이때 inplace 를 사용하지 않았기에 원본 데이터는 여전히 결측치가 있음을 확인해주세요



**✅ 4. 결측값 대체 – fillna()**

- 결측치를 처리하는 또 다른 방법으로는 대체하는 방법이 있습니다.
- 대체를 위해서는 결측된 데이터를 채워 넣는 방식인 fillna 함수가 있으며 파라미터는
- [읽기]



- 데이터를 결측하는 방법으로는 0으로 대체, 평균으로 대체, 이전값 대체, 다음값 대체 정도가 있으며
- 0과 평균은 초기 모델 구축시 전처리 없이 빠르게 값을 확인할 때 많이 사용하며
- 이전값, 다음값 대체는 시계열 데이터에서 유용하게 사용합니다.



- 베이스모델 결과 확인 후 모델의 성능을 올리기 위해 전처리를 수행하게 되는데
- 이 결측치를 가지고 있는 컬럼의 특성에 대해 파악한 후 지울지, 채울지 그리고 어떤 방식으로 채울지를 결정하는 것이 매우 중요합니다.
- 이는 데이터 전처리 파트에서 확인해보도록 하고, 각각을 한번 수행해 보겠습니다.



**🧹 3.4.3 중복값 처리**

- 다음으로는 중복값 처리입니다.
- 이는 우리가 앞서 데이터를 컨캣하거나 뒤에 나올 병합하는 방식을 사용할 시,
- 조건이 잘못되어 중복된 데이터가 들어가는 경우가 빈번하게 발생합니다.
- 또는 데이터 자체가 잘못되어있어 중복이 많은 경우도 존재하게 되는데요 이때 중복값 처리를 통해 데이터를 제거해줄 수 있습니다.
- 중복값 처리는 drop_duplicate라는 api를 사용하게 되구요
- 이는 행 단위를 기준으로 동일한 데이터가 있을시 인덱스를 고려하지 않고 항상 먼저있는 행만 남기고 나머지 행들을 지워버립니다.
- 파라미터는 자체에 inplace가 없기에 변수에 재할당 해 줄 필요가 있습니다.
- 바로 실습을 통해 확인해보도록 하겠습니다.



**✅ 중복값 생성 예시**

```
score.loc['이영희', '수학'] = 70  # 수학 점수가 같은 값으로 만들어 중복 상황
```

- 먼저 중복값을 생성해주도록하구요
- 그런데 타입이 서로 안맞아보이니까 일단 타입을 먼저 맞춰주도록 하겠습니다.

```
score['수학'] = score['수학'].astype(float)
```

- 네 서로 float 타입으로 맞춰주었구요
- 데이터를 확인해보면 이영희의 국어와 수학 점수는 동일해서 중복처럼 보입니다.
- 하지만 전체 데이터를 보았을때 둘다 70점인 데이터는 없으므로 컬럼을 지정하지 않게 되면 아무런 중복제거도 일어나지 않습니다.



- 반면 컬럼을 지정하여 중복값을 삭제한다면
- 수학만 있는 컬럼에서 70인 행이 두개 존재하기에 나중에 나온 이영희의 수학 점수는 제거되게 되는 것이지요



[45]



- 네 이렇게 일단 pandas 기초 첫 시간은 마무리하도록 하구요
- 잠시 쉬었다가 다음시간부터 이어나가도록 하겠습니다.



