🗣️ 강의: [ / ]

🐥 실습: [ / ]

🔑 해석 : [ / ]



- 네 이전시간은 데이터를 확인하고 시각화, EDA 과정을 진행해 보았다면
- 이번 시간은 데이터 전처리부터 모델링 그리고 평가 까지를 실습해 보도록 하겠습니다

### 2.2 전처리

**2.2.1 결측치 탐색 및 처리**

- 먼저 전처리입니다.
- 앞서 데이터를 어느정도 확인해 보았으니 확인한 내용을 바탕으로 전처리를 수행해 보도록 하겠습니다.
- 우리는 데이터를 확인하는 과정에서 결측치가 있음을 알았습니다.
- 따라서 결측치를 탐색하고 처리해 볼것인데요.
- 미션 수행 가이드
- 실습 시간은 3분 드리도록 하겠습니다.

🗣️ 강의: [ 1:00/ 1:00]

🐥 실습: [ 3:00/ 4:00]

````
💥 **MISSION**

> **결측치 탐색 결과 해석**

- lead_time 결측치 비율이 작기에 제거해도 무방해 보임
- is_repeated_guest는 시각화를 통해 대부분이 0이 었으니 0으로 대체하여도 무방해 보임
- adr은 수치형이기 때문에 평균값으로 대체
````

````python
# lead_time : 결측치의 비율이 적기에 삭제
hotel.dropna(subset=["lead_time"], axis=0, inplace=True)

# is_repeated_guest : 대부분 첫 방문 고객이기에 0으로 대체
hotel["is_repeated_guest"].fillna(0, inplace=True)

# adr : 객실 요금의 평균값으로 대체
hotel["adr"].fillna(hotel["adr"].mean(), inplace=True)
````

🔑 해석 : [ 4:00 / 8:00 ]



**2.2.2 이상치 탐색 및 처리**

- 결측치를 처리한 다음에는 이상치를 처리해볼 것인데요
- 이전 강의에서 이상치를 탐색하기 위해 두가지 방법에 대해 소개드렸습니다.
- 먼저 IQR 방식을 통해 이상치를 제거하는 것은 통계적인 근거를 가지고 수행하는 방법이라고 말씀드렸습니다.
- 그런데 이 통계는 약간 고지식하기에 IQR을 사용하면 너무 많은 데이터가 삭제 되는 단점이 있었죠
- 그래서 다른 방식으로 상위, 하위 몇퍼센트의 데이터만을 삭제하는 방식을 사용할 수 있다고 말씀드렸습니다.
- 그에 대한 내용을 한번 확인해 보도록 하겠습니다.
- 코드수행

🗣️ 강의: [ 1:00/ 9:00]

- 시간은 1분정도면 충분할 것 같으니 잠시 기다리도록 하겠습니다.

🐥 실습: [ 1:00/ 10:00]

````Python
# 이상치 비율 확인
print("이상치 비율 iqr:", len(iqr_outliers) / len(hotel) * 100)
print("이상치 비율 n%:", len(outliers) / len(hotel) * 100)
````

````
💥 **MISSION**

> **이상치 탐색 결과 해석**

- 시각화를 통해 이상치로 보여도 데이터들이 연속적으로 존재하여 업무적으로는 이상치가 아닐 수 있음을 확인
- iqr 방식을 수행하면 너무 많은 데이터가 제거됨
- 비즈니스 로직을 모르기 때문에 보수적으로 이상치 제거
````

🔑 해석 : [ 2:00 / 12:00 ]



**2.2.3 범주형 변수 처리**

- 다음으로는 범주형 변수를 처리하는 과정입니다.
- 시간은 1분정도면 충분할 것 같으니 잠시 기다리도록 하겠습니다.

🗣️ 강의: [ 1:00/ 13:00]

🐥 실습: [ 1:00/ 14:00]

````Python
from sklearn.preprocessing import OneHotEncoder

# step2. 인코딩 객체 생성 및 학습
encoder = OneHotEncoder(sparse_output=False)
````

````
💥 **MISSION**

> 인코딩 모델 선택 이유

- deposit_type은 범주형 변수 중 순서가 없는 명목형이기에 원-핫 인코딩 수행
````

```python
# deposit_type 제거
drop_deposit_type = hotel_clean.drop("deposit_type", axis=1)

# 열 기준 concat 수행
encoded_hotel_concat = pd.concat(
    [drop_deposit_type, encoded_hotel], axis=1
)
encoded_hotel_concat
```

🔑 해석 : [ 2:00 / 16:00 ]



**2.2.4 데이터 분할**

- 이제 결측치 이상치에 대한 처리가 끝났으니 모델 학습을 위해 데이터 분할 과정을 수행해야 합니다.
- 분할을 수행하는 데이터는 지금까지 처리를 해오던 encoded_hotel_concat 데이터 프레임으로 진행을 해주시구요
- train, valid, test 데이터 셋에 대해 8:1:1로 수행을 해 주시면 됩니다.
- 다만 여기서 하나의 트릭이 존재하게 되는데요
- 우리가 사용하는 train_test_split은 데이터프레임을 두개로 쪼개는 역할만 수행합니다.
- 그런데 세개로 쪼개기 위해서는 어떠한 트릭을 써야하는지 고민해보시구요
- 앞서 설명드렸던 분할의 이유와 어떤 분할을 사용할지에 대한 내용을 고민하여 적어보시길 바랍니다.
- 시간은 3분 드리도록 하겠습니다.

🗣️ 강의: [ 1:00/ 17:00]

🐥 실습: [ 3:00/ 20:00]

````python
from sklearn.model_selection import train_test_split

# train / temp_test (80 : 20)
train_data, temp_test_data = train_test_split(
    encoded_hotel_concat,
    test_size=0.2,
    random_state=1004,
    stratify=encoded_hotel_concat["is_canceled"],
)

# temp_test → validation / test (50 : 50 → 전체 비중으로는 각 10%)
validation_data, test_data = train_test_split(
    temp_test_data,
    test_size=0.5,
    random_state=1004,
    stratify=temp_test_data["is_canceled"],
)

# 인덱스 정리
train_data.reset_index(drop=True, inplace=True)
validation_data.reset_index(drop=True, inplace=True)
test_data.reset_index(drop=True, inplace=True)

# 결과 확인
print("train data :", train_data.shape)
print("validation data :", validation_data.shape)
print("test data :", test_data.shape)
````

```
💥 **MISSION**

> 데이터셋 분할 이유와 방법에 대해 설명

- 전체 데이터를 **학습용(train), 검증용(validation), 테스트용(test)**으로 분할하여 모델 학습시 과적합 방지 및 일반화 성능 확보 가능
- 최종적으로 train : validation : test = 8 : 1 : 1의 비율로 구성
- stratify 옵션을 적용하여 타깃 변수의 클래스 비율을 각 데이터셋에 동일하게 유지
```

🔑 해석 : [ 3:00 / 23:00 ]



**2.2.5 스케일링**

- 스케일링 전체에 대한 실습 가이드 전달
- 이번에는 미션이 좀 많기 때문에 5분정도 시간을 드리도록 하겠습니다.

🗣️ 강의: [ 2:00/ 25:00]

🐥 실습: [ 5:00/ 30:00]

````
💥 **MISSION**

> **데이터 범위 확인 결과 해석**

- lead_time, days_in_waiting_list, adr 변수가 다른 값에 비해 큰값을 가지고 있음을 확인
````

````python
# 분석 모델에 어울리는 스케일링 모델 임포트
from sklearn.preprocessing import StandardScaler

excluded = ["is_canceled", "is_repeated_guest", "deposit_type_No Deposit", 
            "deposit_type_Non Refund", "deposit_type_Refundable"]
scale_columns = [col for col in train_data.columns if col not in excluded]

scaler = StandardScaler()

# Train 데이터의 fitting과 스케일링
scaler.fit(train_data[scale_columns])
scaled_data = scaler.transform(train_data[scale_columns])

# 데이터프레임 변환
scaled_df = pd.DataFrame(scaled_data, columns=scale_columns)
scaled_train_data = pd.concat([train_data.drop(scale_columns, axis=1), scaled_df], axis=1)
scaled_train_data.head()
````

```
💥 **MISSION**

> 스케일링 모델 선정 이유 작성

- StandardScaler는 분류 모델의 학습 안정성과 성능 향상에 도움을 줌
```

```python
scaled_data = scaler.transform(validation_data[scale_columns])
scaled_df = pd.DataFrame(scaled_data, columns=scale_columns)
scaled_val_df = pd.concat(
    [validation_data.drop(scale_columns, axis=1), scaled_df], axis=1
)
scaled_val_df.head()
```

```python
scaled_data = scaler.transform(test_data[scale_columns])
scaled_df = pd.DataFrame(scaled_data, columns=scale_columns)
scaled_test_df = pd.concat([test_data.drop(scale_columns, axis=1), scaled_df], axis=1)

scaled_test_df.head()
```

🔑 해석 :  [ 4:00/ 34:00]



**2.2.6 데이터 불균형 처리**

🗣️ 강의: [ 1:00/ 35:00]

- 수행 시간은 3분정도 드리도록 하겠습니다.

🐥 실습: [ 3:00/ 38:00]

````python
# 조건식을 활용하여 타겟의 비율 확인

ratio0 = round(
    len(train_data[train_data["is_canceled"] == 0]) / len(train_data) * 100, 2
)
ratio1 = round(
    len(train_data[train_data["is_canceled"] == 1]) / len(train_data) * 100, 2
)
print("0 비율: {}%".format(ratio0))
print("1 비율: {}%".format(ratio1))
````

````python
from imblearn.over_sampling import RandomOverSampler
from collections import Counter

# 오버샘플링 수행
oversample = RandomOverSampler()

# 종속변수, 독립변수 분할
x = scaled_train_data.drop("is_canceled", axis=1)
y = scaled_train_data["is_canceled"]

# 오버샘플링 적용
x_over, y_over = oversample.fit_resample(x, y)
print(Counter(y_over))
````

```
💥 **MISSION**

> **오버샘플링 수행의 이유와 결과 해석**

- 다수 클래스(예: 취소되지 않음)에 비해 소수 클래스(예: 취소됨)가 지나치게 적을 경우, 모델이 소수 클래스를 무시하고 다수 클래스만 예측하는 경향 발생
- 이를 보완하기 위해 소수 클래스의 샘플을 복제하여 학습 데이터의 클래스 비율을 균형 있게 조정하는 방식인 **오버샘플링(RandomOverSampler)**을 적용
- Counter(y_over) 결과를 보면, 두 클래스의 샘플 수가 동일하게 조정된 것을 확인할 수 있음
- 이를 통해 모델이 학습 시 양 클래스 모두에 대한 예측 성능을 고르게 반영할 수 있는 기반을 마련하게 됨
```

🔑 해석 :  [ 3:00/ 41:00]



## 2.3 예측 모델 생성

- 이번시간에는 전처리를 위주로 학습을 해보았고 예측모델 생성 및 평가는 다음시간부터 미션을 수행해 보도록 하겠습니다.
- 먼저 모델을 생성해 보겠습니다.
- 이번에 사용할 모델은 트리기반 알고리즘에서 유명한 랜덤포레스트 유형의 모델입니다.
- 모델을 불러와서 학습시키는 과정은 이전에 수행한 스케일링, 인코딩 과정과 동일합니다.
- 결과를 확인해보면 정학도가 0.99로 정말 높은 정확도를 나타냄을 알 수 있습니다.
- 그런데 과연 정말 그런지 밑의 성능 평가를 통해 다시한번 확인해 보도록 하겠습니다.

## 2.4 성능 평가

- 이제 우리는 학습데이터로 만든 모델에 대해 성능을 평가해야 합니다.
- 원래라면 이제 학습데이터로 모델을 만든 뒤 검증 데이터를 사용해 모델의 파라미터를 조정하고 최종으로 테스트 데이터를 사용해 성능을 평가하는 것이 정석입니다.
- 하지만 이번 실습에서는 튜닝은 생략할 것이기 때문에 검증데이터와, 테스트데이터를 합쳐서 모두 성능을 평가하는데 사용할 것이고
- 이를 위해 하나의 데이터 프레임으로 합쳐주는 결합과정을 수행합니다.

[코드]

- 그리구 이렇게 만든 데이터에서 종속변수와 독립변수를 구분지어주고
- 학습데이터로 만든 모델에 테스트데이터의 독립변수들을 넣어 예측 결과를 추출한 뒤, 실제 종속변수와의 비교를 통해 성능 검증을 수행합니다.

[코드]

- 분류 분석의 성능 검증은 혼동행렬, 컨퓨전 매트릭스를 사용한다고 말씀 드렸죠
- 따라서 이를 수행해보면 다음과 같은 결과가 나옵니다.
- 우리가 알고있는 혼동행렬과는 조금 다른 모습인데요
- 이는 우리의 종속변수가 0과 1로되어있는 이진 변수이기 때문입니다.
- 혼동행렬은 어느것을 positve로 두냐에 따라 그 결과가 달라지게 되는데요
- 쉽게 생각해서 우리 암환자 예시를 생각해봤을때, 예측의 목적인 양성환자를 Positive로 설정했던 것을 생각해주시면 됩니다.
- 따라서 우리는 객실 취소를 예측하는 즉 is canceled가 1인 것이 목적이기에 아래 것을 기준으로 보면 되는데요
- 정확도 accuracy에 비해 정밀도, 재형률 f1-score 모두 결과가 매우 낮은 것을 알 수 있습니다.
- 즉 정확도에 속아 이 모델이 좋겠구나라고만 생각하면 안되고 혼동행렬을 통해 실제 원하는 목적에 맞는 성능 지표를 파악해야 함을 다시한번 볼 수 있었습니다.

- 이때 우리는 비즈니스 로직과 연관하여 볼 수 가 있는데요
- 앞서 배운 정밀도와 재현율의 관점으로 비교를 해보자면 아래와 같은 표로 확인할 수 있습니다.
- 이런경우 저는 모델이 취소라고 예측했는데 실제 취소가 아닌 경우가 더 크리티컬하다고 생각합니다.
- 정말 최악의 경우 고객에게 객실을 제공하지 못하는 경우가 생기기 때문인데요.
- 즉, 정밀도를 모델 성능 지표로 삼는 것이 이 과제의 핵심이라고 생각이 들구요
- 이를 기준으로 모델을 조정해 나가는 과정을 후속 과정으로 삼을 것입니다.

- 마지막으로는 **분류 모델의 또 다른 성능 지표**인 **ROC 커브와 AUC 스코어**를 구해보겠습니다.
- ROC 커브는 **모델이 클래스 1(객실 취소)을 얼마나 잘 구분하는지**를 시각화한 곡선으로,
- 이때 곡선 아래 면적인 **AUC (Area Under the Curve)** 스코어는 **모델이 클래스 간을 구분할 수 있는 능력의 평균적인 성능**을 나타내며, 0.5에 가까우면 랜덤 수준, 1.0에 가까우면 완벽한 분류기를 의미합니다.
- 그런데 이 AUC 스코어가 **0.76**으로, 꽤 괜찮은 수치처럼 보이지만,
- 앞서 확인한 **정밀도나 재현율은 0.5도 채 되지 못했던 것**을 생각해보면, AUC만 보고 **모델이 좋다고 판단하기엔 무리가 있습니다**.
- **왜냐하면 AUC는 클래스 불균형이 있는 상황에서 과도하게 낙관적인 해석을 유도할 수 있기 때문입니다.**
- 특히 우리는 “객실 취소”라는 **희귀 이벤트를 예측**하는 것이 목적이고, 이 경우 **정밀도**와 **재현율**이 훨씬 더 의미 있는 지표가 됩니다.

🔑 해석 :  [ 5:00/ 46:00]

