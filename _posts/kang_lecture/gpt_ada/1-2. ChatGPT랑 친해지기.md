---
layout: single
title: "ChatGPT랑 친해지기"
toc: true
categories: [GPT Advanced Data Analysis]
tags: [Lecture, LLM]
---

# 🎩 ChatGPT랑 친해지기

## 1. 언어모델 (Language Model, LM)

### 1.1 언어모델이란?

👀 **정의**

```
주어진 단어들을 보고 다음으로 올 확률이 높은 단어를 예측하는 도구
```

✏️ **언어모델의 발전**

언어 모델은 빈도수 기반의 확률 추론을 하는 `통계적 언어모델`부터 복잡한 연산을 수행하는 `신경망 언어모델`까지 단점을 극복하며 발전을 이루어왔습니다. 최근 가장 많이 활용되는 **Transformer** 기반의 언어모델들 역시 `신경망 언어모델`이며, 어텐션 메커니즘을 통해 문맥을 효과적으로 이해하고 처리 속도를 대폭 향상시키는 혁신적인 구조를 갖추고 있습니다.

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/kang_lectures/gpt_ada/transformer.png?raw=true" height="300"></p>

🌈 **예시: 언어모델**

> 오늘 점심은 [다음 나올 단어] <br>구내식당-90%, 외식-9.9%, 휴지-0.0001%, ...

- 상황: 언어 모델에게 `오늘 점심은`이라는 단어를 입력하였을 때, 다음 단어에 대한 예측
- 🤖: 오늘 점심은 구내식당

📍 **다음 단어를 어떻게 예측할 수 있을까?**

- 다음 나올 단어에 대한 확률을 계산하기 위해 많은 문장들을 `학습`시킵니다.
- 학습을 통해 기존 문장들에서 `오늘` 과 `점심은` 이라는 단어 뒤에는 `구내식당`이라는 단어가 나올 확률이 높음을 알 수 있습니다.
- 즉, 언어 모델은 학습 데이터에 의존적이며, 이에 따라 다른 문장을 도출할 수 있습니다.

<br>
<br>

📍 **언어모델의 학습은 어떻게 시킬까?**

앞서 최신 언어 모델은 `Transformer` 구조로 되어 있다고 언급했습니다. 이는 변신하는 로봇과 같은 막연한 개념이 아니라, 실제로는 단순한 `행렬` 연산을 통해 문장 간의 관계를 계산하고 `parameter`를 `update`하는 방식입니다. 즉, 데이터를 사용해 `행렬`연산을 반복적으로 수행하고, 최적의 `parameter`를 찾는 과정을 학습이라고 부릅니다.

- `Transformer` <-> `뇌`
- `Parmeter Update` <-> `학습`
- `예시`

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/kang_lectures/gpt_ada/parameter_update.png?raw=true" height="300"></p>

📌 **더 알아보기**

- NLP 분야에서 학습 데이터는 `corpus`라 불립니다.
- 보유한 학습 데이터는 `도메인`에 의존적이기에 언어 모델은 `도메인`에 따라 다르게 구축할 수 있습니다.

### 1.2 대규모 언어모델이란?

👀 **Definition**

```
기존 언어 모델보다 훨~~씬 많은 데이터를 학습시켜 다양한 언어 예측 작업을 수행할 수 있는 도구
```

📍 **vs 언어모델**

대규모 언어모델은 기존 언어모델보다 훨씬 더 많은 데이터를 학습했기 때문에, 풍부한 지식을 기반으로 다양한 일반적인 상황에서 더 정확하고 포괄적인 답변을 생성할 수 있습니다. 즉, 이전 언어모델은 작은 데이터로 학습되어 특정 도메인에 한정된 답변만을 제공하는 경향이 있었던 반면, 대규모 언어모델은 광범위한 데이터를 학습하여 다양한 주제와 맥락에서 유연하고 다각적인 답변을 제공할 수 있습니다.

<br>
<br>
<br>
<br>
<br>

🌈 **예시: 언어모델과 대규모 언어모델의 차이**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/kang_lectures/gpt_ada/lm_llm.png?raw=true" height="200"></p>

📍 **더 많은 데이터를 어떻게 학습시킬까?**

앞서 언어모델에서 학습은 `Transformer` 의 `Parameter`를 `update`하는 것이라고 하였습니다. 이는 현실의 학습의 개념으로 보자면 `Transfomer`라는 `뇌`에 문서들을 `저장`시키는 것과 동일합니다. 따라서 훨~~씬 많은 데이터를 언어모델에 학습시키려면, 더 `큰 뇌`와 데이터를 이해하기 위한 `연산 능력`이 필요합니다. 이때 `큰 뇌`는 컴퓨터의 저장 공간으로 `메모리 크기`를 나타내고, `연산 능력`은 컴퓨터의 `프로세싱 파워`를 의미합니다. 즉, 고성능의 컴퓨터가 필수적이며, 이러한 컴퓨터는 방대한 데이터를 효율적으로 처리하고 학습하는 데 중요한 역할을 합니다.

🏷️ **참조**

- 📜 [Naver Cloud Platform-GPU Price](https://www.ncloud.com/product/compute/gpuServer#pricing)

---

## 2. 할루시네이션

### 2.1 할루시네이션이란?

👀 **정의**

```
모델이 사실이 아닌 정보를 생성하거나, 실제로 존재하지 않는 데이터를 만들어내는 현상
```

📍 **왜 할루시네이션이 나타날까?**

1. **모델의 학습 방식**: 모델의 학습은 대규모 텍스트 데이터를 기반으로 이루어집니다. 이 과정에서 모델은 다양한 정보들이 함께 존재하는 텍스트에서 단어와 문장 간의 `패턴`을 학습하게 되며, 이로인해 특정 입력에 대해 가능성이 높은 출력을 생성합니다. 즉, 모델이 질문을 이해하거나 기억하는 것이 아니라 단지 `패턴`을 따르는 것이기 때문에, 잘못된 또는 존재하지 않는 정보를 생성할 수 있습니다.
2. **모델의 목표**: 언어 모델은 정확한 정보를 생성하는 것을 목표로 하기보다는, 주어진 텍스트 입력에 `가장 그럴듯한 출력`을 생성하는 것을 목표로 합니다. 이로인해, 모델은 사실과 일치하지 않지만 `언어적`으로는 타당해 보이는 출력을 생성할 수 있습니다.
3. **데이터의 시점**: 언어 모델은 `학습된 시점`까지의 데이터를 바탕으로 정보를 생성합니다. 이후 발생한 사실이나 `최신 정보`를 반영하지 못하기 때문에, `최신 정보`와 관련된 질문에 대해 잘못된 정보를 제공할 수 있습니다.

<br>
<br>

🎹 **실습: 할루시네이션 경험하기**

🧑‍💻: 첫번째 실습입니다. [ChatGPT](https://chatgpt.com/)에 접속하여 아래와 같이 상장 기업에 대한 주가를 물어보는 질문을 해보겠습니다.

```
삼성전자 주가 알려줘
```

📍 **어떻게 할루시네이션을 방지할 수 있을까?**

- **최신 데이터 사용**: 모델을 `최신 데이터`로 주기적으로 `학습`하여 최신 정보를 반영하도록 합니다.
- **프롬프트 엔지니어링**: 명확하고 구체적인 질문을 통해 잘못된 정보를 생성할 가능성을 줄입니다.

---

## 3. 프롬프트

### 3.1 프롬프트란?

👀 **정의**

```
AI 모델에게 작업을 지시하거나 질문을 던질 때 사용함
```

✏️ **프롬프트 엔지니어링**

**프롬프트 엔지니어링**은 AI 모델이 원하는 결과를 생성하도록 입력을 설계하는 과정입니다. 효과적인 프롬프트는 명확하고 구체적이며, 원하는 결과를 도출하기 위한 정보와 제약 조건을 포함합니다. 프롬프트의 구조와 언어적 표현을 잘 설계하면 모델의 성능을 극대화할 수 있습니다

🎹 **실습: 할루시네이션 개선하기**

🧑‍💻: 앞서 주가 정보에 대해 잘못된 답을 주었던 것을 개선해보도록 하겠습니다.

```
삼성전자 주가알려줘, 단 최신 정보를 모른다면 대답하지마
```

### 3.2 Few-Shot Learning

👀 **정의**

```
모델에 질문시 예시를 더하여 향상된 답변을 얻는 방법으로 다양한 작업에 적용될 수 있음
```

✏️ **In-context learning**

In-context learning은 GPT와 같은 대규모 언어 모델에서 사용되는 학습 기법 중 하나입니다. 이 기법의 핵심 아이디어는 모델이 주어진 문맥(context) 내에서 학습하고 정보를 추론하는 능력을 갖추게 하는 것입니다. 대규모 언어 모델의 경우 규모가 매우 크기 때문에 학습을 위해서는 많은 자원이 필요합니다. 따라서 주어진 입력 문맥을 활용하여 학습과 유사한 답변을 효과적으로 얻을 수 있습니다.

🌈 **예시: ConversationSummaryMemory**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/kang_lectures/gpt_ada/chat_summary.png?raw=true" width="1000" height="350"></p>

GPT는 우리의 정보를 질문시 마다 학습하지 않습니다. 그럼에도 나와의 대화를 기억한다는 듯 앞서 질문들에 대한 정보를 참고하여 대답을 합니다. 이는 아래와 같이 프롬프트로 우리의 대화 내역을 요약하는 질문을 수행하고, 이를 새로운 질문에 포함시켜 전달하기 때문입니다. 이때 요약을 위해 한가지의 `예시`를 포함하고 있는데, 이를통해 매번 다른 질문이 포함되어 있어도 `일관되고 정확한 요약`을 얻을 수 있습니다. 이렇게 프롬프트에 `약간의 예시`를 포함해 `향상된 답변`을 얻는 방식이 `few-shot learning`입니다.

```python
# Langchain의 Summary Prompt
_DEFAULT_SUMMARIZER_TEMPLATE = """Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.

EXAMPLE
Current summary:
The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.

New lines of conversation:
Human: Why do you think artificial intelligence is a force for good?
AI: Because artificial intelligence will help humans reach their full potential.

New summary:
The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.
END OF EXAMPLE

Current summary:
{summary}

New lines of conversation:
{new_lines}

New summary:"""
```



[한글번역 필요함]



🎹 **실습: Few-shot Learning**

🧑‍💻: 미생의 명장면인 장그래 레전드 PT를 아래 회의록 양식에 맞추어 ChatGPT를 통해 작성해 볼까요?

- 🔗 [미생-장그래의 레전드 PT](https://www.youtube.com/watch?v=wttPb4KDD2k)
- 💾 [장그래 레전드 PT 텍스트]()
- 회의록 예시

```
1. 회의 정보
- 제목: 기능 명세 작성 (채팅/메시지)
- 일시: 2024.08.06
- 장소: 온라인

2. 참석자
- 바비 찰튼
- 데니스 로

3. 목적
채팅/메시지에 대한 기능 명세 작성

4. 채팅/메시지 기능 명세
[frontend]
- 채팅 메인 화면 개발
- DM 메인 화면 개발

[backend]
- 채팅방 모델링
- 채팅 메세지 모델링

[프로세스 정의서 작성]
- 채팅 프로세스 정의

5. 차기 회의 일정
- 일시: 2024.08.07
- 장소: 온라인
- 안건: 서베이

# 작성자: 조지 베스트
```

<br>
<br>
<br>
<br>
<br>
<br>

## 💕 [Bonus] Function Calling

👀 **정의**

```
대형 언어 모델이 외부 시스템, 도구, 또는 API와 상호작용하기 위해 특정 기능을 호출하는 메커니즘
```

- GPT가 알지 못하는 실시간 정보들을 기능으로 구현해놓고, GPT에게 필요하다면 이를 활용하라고 전달

🌈 **예시: Function calling1**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/kang_lectures/gpt_ada/function_call.png?raw=true" height="450"></p>

🌈 **예시: Function calling2**

🧑‍💻: 앞서 주가 정보의 끝판왕을 만나봅니다.

🏷️ **참조**

- 📜 [Langchain 공식문서-Function calling](https://platform.openai.com/docs/guides/function-calling)
