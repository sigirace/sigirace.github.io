---
layout: single
title:  'ADP ì‹¤ê¸° ì—°ìŠµë¬¸ì œ (1)'
toc: true
categories: [ADP]
tags: [SVR, RandomForest Regressor, XGBoost Regressor, Linear Regressor, Two-way ANOVA]

---

ë³¸ ê²Œì‹œë¬¼ì€ ADP ì‹¤ê¸°ì˜ ì—°ìŠµë¬¸ì œ í’€ì´ì— ëŒ€í•œ ê²ƒì´ë‹¤.
{: .notice}

# ë¨¸ì‹ ëŸ¬ë‹

## 1. ì‹œê°í™” í¬í•¨ EDA ìˆ˜í–‰

````python
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np 
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("https://raw.githubusercontent.com/sigirace/page-images/main/adp/problem/practice1/data/student_data.csv")
````

### 1.1 ë°ì´í„° ê°œìš”

````python
df
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/1.png?raw=true" width="900" height="350"></p>

````python
df.describe()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/2.png?raw=true" width="900" height="350"></p>

````python
df.info()
````

````
RangeIndex: 395 entries, 0 to 394
Data columns (total 14 columns):
 #   Column      Non-Null Count  Dtype  
---  ------      --------------  -----  
 0   school      395 non-null    object 
 1   sex         395 non-null    object 
 2   paid        395 non-null    object 
 3   activities  395 non-null    object 
 4   famrel      394 non-null    float64
 5   freetime    393 non-null    float64
 6   goout       392 non-null    float64
 7   Dalc        391 non-null    float64
 8   Walc        393 non-null    float64
 9   health      391 non-null    float64
 10  absences    392 non-null    float64
 11  grade       395 non-null    int64  
 12  G1          392 non-null    float64
 13  G2          392 non-null    float64
dtypes: float64(9), int64(1), object(4)
````

- Null ê°’ì´ ìˆìŒì„ í™•ì¸

### 1.2 ì‹œê°í™”

````python
fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))

columns = df.columns.to_list()

for i, column in enumerate(columns):
    row = i // 4
    col = i % 4
    axes[row, col].hist(df[column], bins=20, color='skyblue', alpha=0.7)
    axes[row, col].set_title(f'Histogram of {column}')
    axes[row, col].set_xlabel(column)
    axes[row, col].set_ylabel('Frequency')

plt.tight_layout()
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/3.png?raw=true" width="900" height="600"></p>

````python
# ì—°ì†í˜•
continuous_columns = ['absences', 'grade', 'G1', 'G2']

# ë²”ì£¼í˜•
categorical_columns = ['school', 'sex', 'paid', 'activities', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health']
````

ğŸ“ **ì—°ì†í˜•-íˆìŠ¤í† ê·¸ë¨**

````python
fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 4))

for i, column in enumerate(continuous_columns):
    df[column].plot(kind='hist', bins=20, ax=axes[i], color='skyblue', edgecolor='black')
    axes[i].set_title(f'Histogram of {column}')
    axes[i].set_xlabel(column)
    axes[i].set_ylabel('Frequency')

plt.tight_layout()

plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/4.png?raw=true" width="900" height="200"></p>

ğŸ“ **ì—°ì†í˜•-ë°•ìŠ¤í”Œë¡¯**

````python
fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(15, 4))

for i, column in enumerate(continuous_columns):
    sns.boxplot(x=df[column], ax=axes[i], color='skyblue')
    axes[i].set_title(f'Histogram of {column}')
    axes[i].set_xlabel(column)
    axes[i].set_ylabel('Frequency')

plt.tight_layout()

plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/5.png?raw=true" width="900" height="200"></p>

ğŸ“ **ë²”ì£¼í˜•-ë§‰ëŒ€ê·¸ë˜í”„**

````python
fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 5))

for i, column in enumerate(categorical_columns):
    row = i // 5
    col = i % 5
    df[column].value_counts().sort_index().plot(kind='bar', ax=axes[row, col], color='skyblue')
    axes[row, col].set_title(f'Bar Plot of {column}')
    axes[row, col].set_xlabel(column)
    axes[row, col].set_ylabel('Frequency')    

plt.tight_layout()
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/6.png?raw=true" width="900" height="350"></p>

ğŸ“**ìƒê´€ê´€ê³„**

````python
df_cor = df.corr(method='pearson')
sns.heatmap(df_cor,
             xticklabels = df_cor.columns,
             yticklabels = df_cor.columns,
             cmap='RdBu_r',
             annot=True, 
             linewidth=3)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/7.png?raw=true" width="500" height="350"></p>

### 1.3 ë°ì´í„° íƒìƒ‰ ê²°ê³¼

- ê²°ì¸¡ì¹˜
  - famrel, freetime, goout, Dalc, Walc, health, absences, G1, G2ì— ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•¨ì„ í™•ì¸
- ì´ìƒì¹˜
  - absences ë°ì´í„°ëŠ” ì´ìƒì¹˜ê°€ ìˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŒ
- ë¶ˆê· í˜•
  - school ë³€ìˆ˜ì— ë¶ˆê· í˜•ì´ ìˆìŒì„ í™•ì¸
- ìƒê´€ê´€ê³„
  - G1ê³¼ G2ê°„ì˜ ìƒê´€ê´€ê³„ê°€ ë†’ê¸°ì— ì„ í˜• ëª¨ë¸ ì‚¬ìš©ì‹œ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œë¥¼ í™•ì¸í•´ì•¼í•¨

## 2. ê²°ì¸¡ì¹˜ë¥¼ ì‹ë³„Â·ì˜ˆì¸¡í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì„ ì“°ê³ , ì´ë¥¼ ì„ íƒí•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.

### 2.1 ê²°ì¸¡ì¹˜ í™•ì¸

````python
df_isna = pd.DataFrame(df.isna().sum(), columns=['cnt'])
df_isna
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/8.png?raw=true" width="100" height="350"></p>

ğŸ“ **ê²°ì¸¡ì¹˜ ë°ì´í„°**

````python
null_df = df[df.isna().any(axis=1)]
null_df
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/9.png?raw=true" width="900" height="500"></p>

- ëª¨ë“  í–‰ì´ ê²°ì¸¡ì¹˜ì¸ ê²½ìš°ëŠ” ì—†ìœ¼ë¯€ë¡œ ì œê±°ë³´ë‹¤ëŠ” ëŒ€ì²´ê°€ íš¨ê³¼ì 

ğŸ“ **ê²°ì¸¡ì¹˜ ë¹„ìœ¨**

````python
null_percentage_by_row = df.isnull().mean() * 100
null_percentage_df = pd.DataFrame({'percent': null_percentage_by_row})
null_percentage_df[null_percentage_df.percent > 0]
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/10.png?raw=true" width="200" height="350"></p>

````python
null_percentage = (len(null_df) / len(df))*100
print("ì „ì²´ ë°ì´í„°ì˜ ê²°ì¸¡ì¹˜ ë¹„ìœ¨: {}".format(null_percentage))
````

````
ì „ì²´ ë°ì´í„°ì˜ ê²°ì¸¡ì¹˜ ë¹„ìœ¨: 6.075949367088607
````

### 2.2 ê²°ì¸¡ì¹˜ ì‹ë³„, ì˜ˆì¸¡ ë°©ë²•

ğŸ“**ë‹¨ìˆœ ëŒ€ì²´**

- ìˆ˜ì¹˜í˜•: í‰ê· ì´ë‚˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
- ë²”ì£¼í˜•: ìµœë¹ˆê°’ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ì²´

````python
cate_null_col = ['famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health']
num_null_col = ['absences', 'G1', 'G2']
````

````python
for col in cate_null_col:
    mod = df[col].mode().iloc[0]
    print("{}ì˜ ê²°ì¸¡ì¹˜ëŠ” {}ë¡œ ëŒ€ì²´í•¨".format(col, mod))
    df[col].fillna(mod, inplace=True)
````

````
famrelì˜ ê²°ì¸¡ì¹˜ëŠ” 4.0ë¡œ ëŒ€ì²´í•¨
freetimeì˜ ê²°ì¸¡ì¹˜ëŠ” 3.0ë¡œ ëŒ€ì²´í•¨
gooutì˜ ê²°ì¸¡ì¹˜ëŠ” 3.0ë¡œ ëŒ€ì²´í•¨
Dalcì˜ ê²°ì¸¡ì¹˜ëŠ” 1.0ë¡œ ëŒ€ì²´í•¨
Walcì˜ ê²°ì¸¡ì¹˜ëŠ” 1.0ë¡œ ëŒ€ì²´í•¨
healthì˜ ê²°ì¸¡ì¹˜ëŠ” 5.0ë¡œ ëŒ€ì²´í•¨
````

ğŸ“**KNNì„ ì‚¬ìš©í•œ ëŒ€ì²´**

````python
from sklearn.impute import KNNImputer

# ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œì„ ì¶”ì¶œ
KNN_data = df.drop(columns=['school','sex','paid','activities'])
#ëª¨ë¸ë§
imputer = KNNImputer()
df_filled = imputer.fit_transform(KNN_data)
df_filled = pd.DataFrame(df_filled, columns=KNN_data.columns)
df[KNN_data.columns] = df_filled

df.isna().sum()
````

````
school        0
sex           0
paid          0
activities    0
famrel        0
freetime      0
goout         0
Dalc          0
Walc          0
health        0
absences      0
grade         0
G1            0
G2            0
````

## 3. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©ì´ í•„ìš”í•œ ê²½ìš°ë¥¼ ì‹ë³„í•˜ê³ , ë³€í™˜ì„ ì ìš©í•˜ì‹œì˜¤. ì´ë¥¼ ì„ íƒí•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.

ğŸ“ **One-hot Encoding**

- ë²”ì£¼í˜• ë³€ìˆ˜ ì¤‘ school, sex, paid, activitesëŠ” ìˆœì„œê°€ ì—†ëŠ” ëª…ëª©í˜•ì´ê¸°ì— Onehot ì‚¬ìš©

````python
df = pd.get_dummies(data = df, columns=['school', 'sex', 'paid', 'activities'], drop_first=True)
df.info()
````

````
Data columns (total 14 columns):
 #   Column          Non-Null Count  Dtype  
---  ------          --------------  -----  
 0   famrel          395 non-null    float64
 1   freetime        395 non-null    float64
 2   goout           395 non-null    float64
 3   Dalc            395 non-null    float64
 4   Walc            395 non-null    float64
 5   health          395 non-null    float64
 6   absences        395 non-null    float64
 7   grade           395 non-null    float64
 8   G1              395 non-null    float64
 9   G2              395 non-null    float64
 10  school_MS       395 non-null    uint8  
 11  sex_M           395 non-null    uint8  
 12  paid_yes        395 non-null    uint8  
 13  activities_yes  395 non-null    uint8  
````

## 4. ë°ì´í„° ë¶„í•  ë°©ë²•ì„ 2ê°€ì§€ ì“°ê³  ì ì ˆí•œ ë°ì´í„° ë¶„í• ì„ ì ìš©í•˜ì‹œì˜¤. ì´ë¥¼ ì„ íƒí•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.

### 4.1 ë°ì´í„° ë¶„í•  ë°©ë²•

1. ëœë¤ ë¶„í• 
   - ë°ì´í„°ë¥¼ í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¡œ ë‚˜ëˆŒ ë•Œ, ì‚¬ìš©ìê°€ ì§€ì •í•œ ë¹„ìœ¨ì— ë”°ë¼ ë¬´ì‘ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
   - ì£¼ë¡œ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.
2. ì¸µí™” ì¶”ì¶œ
   - ì¢…ì† ë³€ìˆ˜ê°€ ë²”ì£¼í˜• ë³€ìˆ˜ì¸ ê²½ìš°ì— ì‚¬ìš©í•©ë‹ˆë‹¤.
   - ê° í´ë˜ìŠ¤ì˜ ë¹„ìœ¨ì„ ê³ ë ¤í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ í´ë˜ìŠ¤ í¸í–¥ì„ ë°©ì§€í•©ë‹ˆë‹¤.

**â˜ ì¢…ì†ë³€ìˆ˜ê°€ ìˆ˜ì¹˜í˜•ì¸ gradeì´ê¸°ì— ëœë¤ ë¶„í• ì„ ì‚¬ìš©í•¨**

### 4.2 ë°ì´í„° ë¶„í•  ìˆ˜í–‰

````python
from sklearn.model_selection import train_test_split 
X = df.drop('grade',axis=1)
y = df['grade']
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=2022)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
````

````
(276, 13)
(119, 13)
(276,)
(119,)
````

## 5. ì—¬ëŸ¬ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•˜ê³  ìµœì ì˜ ê²°ê³¼ ë„ì¶œ

````python
from sklearn.svm import SVR 
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
import time

scaler = StandardScaler() 
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)
````

### 5.1 SVR

````python
param_grid = [{'C': [0.1, 1,10,100],'gamma': [0.001, 0.01, 0.1, 1, 10]}]
grid_svm = GridSearchCV(SVR(), param_grid =param_grid, cv =5)
grid_svm.fit(X_train_scaled, y_train)
result = pd.DataFrame(grid_svm.cv_results_['params'])
result['mean_test_score'] = grid_svm.cv_results_['mean_test_score']
result.sort_values(by='mean_test_score', ascending=False)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/11.png?raw=true" width="300" height="350"></p>

````python
start_time = time.time()
svr = SVR(C=100, gamma =0.001) 
svr.fit(X_train_scaled, y_train)
end_time = time.time()
execution_time = end_time - start_time
print("ì½”ë“œ ì‹¤í–‰ ì‹œê°„: {:.2f} ì´ˆ".format(execution_time))
print("R2 : ", svr.score(X_test_scaled, y_test))
print("RMSE:", np.sqrt(mean_squared_error(y_test,svr.predict(X_test_scaled))))
````

````
ì½”ë“œ ì‹¤í–‰ ì‹œê°„: 0.01 ì´ˆ
R2 :  0.9575168742687914
RMSE: 0.7743847717785174
````

### 5.2 Random Forest Regressor

````python
rf_grid = [
 {'max_depth': [2,4,6,8,10], 'min_samples_split': [2, 4, 6, 8, 10]}
]
rf = GridSearchCV(RandomForestRegressor(n_estimators=100), param_grid =rf_grid, cv =5)
rf.fit(X_train, y_train)
print(rf.best_params_)
````

````
{'max_depth': 8, 'min_samples_split': 4}
````

````python
start_time = time.time()
rf=RandomForestRegressor(**rf.best_params_)
rf.fit(X_train, y_train)
end_time = time.time()
execution_time = end_time - start_time
print("ì½”ë“œ ì‹¤í–‰ ì‹œê°„: {:.2f} ì´ˆ".format(execution_time))
print("R2 : ", rf.score(X_test, y_test))
print("RMSE: ", np.sqrt(mean_squared_error(y_test,rf.predict(X_test))))
````

````
ì½”ë“œ ì‹¤í–‰ ì‹œê°„: 0.10 ì´ˆ
R2 :  0.9594471856156329
RMSE:  0.756587344467249
````

### 5.3 Xgboost Regerssor

````python
xgb_grid = [
 {'max_depth': [2,4,6,8,10]}
]
xgb = GridSearchCV(XGBRegressor(n_estimators=1000), param_grid =xgb_grid, cv =5)
xgb.fit(X_train, y_train)
print(xgb.best_params_)
````

````
{'max_depth': 4}
````

````python
start_time = time.time()
xgb=XGBRegressor(**xgb.best_params_)
xgb.fit(X_train, y_train)
end_time = time.time()
execution_time = end_time - start_time
print("ì½”ë“œ ì‹¤í–‰ ì‹œê°„: {:.2f} ì´ˆ".format(execution_time))
print("R2 : ", xgb.score(X_test, y_test))
print("RMSE: ", np.sqrt(mean_squared_error(y_test,xgb.predict(X_test))))
````

````
ì½”ë“œ ì‹¤í–‰ ì‹œê°„: 0.11 ì´ˆ
R2 :  0.9516113211392506
RMSE:  0.8264573665053062
````

````python
from xgboost import plot_importance
plot_importance(xgb)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/12.png?raw=true" width="900" height="350"></p>

# í†µê³„ë¶„ì„

## 1. íšŒê·€ë¶„ì„

### 1.1 ë°ì´í„°ë¥¼ 8 : 2ë¡œ ë¶„í• í•˜ê³  ì„ í˜• íšŒê·€ë¥¼ ì ìš©í•˜ì‹œì˜¤. ê²°ì •ê³„ìˆ˜ì™€ rmseë¥¼ êµ¬í•˜ì‹œì˜¤.

````python
import pandas as pd 
import numpy as np 
import mglearn

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
X,y = mglearn.datasets.load_extended_boston()

# í›ˆë ¨, í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„ë¦¬
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

lr = LinearRegression() 
lr.fit(X_train,y_train)

## R2
print("ì„ í˜• íšŒê·€ ê²°ì •ê³„ìˆ˜ : ", lr.score(X_test,y_test))
print("ì„ í˜• íšŒê·€ RMSE : ", np.sqrt(mean_squared_error(y_test, lr.predict(X_test))))
````

````
ì„ í˜• íšŒê·€ ê²°ì •ê³„ìˆ˜ :  0.6158858584078888
ì„ í˜• íšŒê·€ RMSE :  5.59265723707854
````

### 1.2 ë°ì´í„°ë¥¼ 8 : 2ë¡œ ë¶„í• í•˜ê³  ë¦¿ì§€ íšŒê·€ë¥¼ ì ìš©í•˜ì‹œì˜¤. alpha ê°’ì„ 0ë¶€í„° 1ê¹Œì§€ 0.1ë‹¨ìœ„ë¡œ ëª¨ë‘ íƒìƒ‰í•´ì„œ ê²°ì •ê³„ìˆ˜ê°€ ê°€ì¥ ë†’ì„ ë•Œì˜ ì•ŒíŒŒë¥¼ ì°¾ê³ , í•´ë‹¹ ì•ŒíŒŒë¡œ ë‹¤ì‹œ ëª¨ë¸ì„ í•™ìŠµí•´ì„œ ê²°ì •ê³„ìˆ˜ì™€ rmseë¥¼ ê³„ì‚°í•˜ì‹œì˜¤.

````python
from sklearn.linear_model import Ridge 
from sklearn.model_selection import GridSearchCV

alpha = np.arange(0,1.1,0.1)
ridge = Ridge() 
param_grid = {'alpha':alpha}
ridge_model = GridSearchCV(ridge, param_grid)
ridge_model.fit(X_train,y_train)
print(ridge_model.best_params_)
print("ë¦¿ì§€ íšŒê·€ ê²°ì •ê³„ìˆ˜ : ", ridge_model.score(X_test,y_test))
print("ë¦¿ì§€ íšŒê·€ RMSE : ", np.sqrt(mean_squared_error(y_test, ridge_model.predict(X_test))))
````

````
{'alpha': 0.1}
ë¦¿ì§€ íšŒê·€ ê²°ì •ê³„ìˆ˜ :  0.7463824108919223
ë¦¿ì§€ íšŒê·€ RMSE :  4.544412437236844
````

### 1.3 ë°ì´í„°ë¥¼ 8 : 2ë¡œ ë¶„í• í•˜ê³  ë¼ì˜ íšŒê·€ë¥¼ ì ìš©í•˜ì‹œì˜¤. alpha ê°’ì„ 0ë¶€í„° 1ê¹Œì§€ 0.1ë‹¨ìœ„ë¡œ ëª¨ë‘ íƒìƒ‰í•´ì„œ ê²°ì •ê³„ìˆ˜ê°€ ê°€ì¥ ë†’ì„ ë•Œì˜ ì•ŒíŒŒë¥¼ ì°¾ê³ , í•´ë‹¹ ì•ŒíŒŒë¡œ ë‹¤ì‹œ ëª¨ë¸ì„ í•™ìŠµí•´ì„œ ê²°ì •ê³„ìˆ˜ì™€ rmseë¥¼ ê³„ì‚°í•˜ì‹œì˜¤.

````python
from sklearn.linear_model import Lasso

lasso = Lasso() 
param_grid = {'alpha':alpha}
lasso_model = GridSearchCV(lasso, param_grid)
lasso_model.fit(X_train,y_train)
print(lasso_model.best_params_)
print("ë¼ì˜ íšŒê·€ ê²°ì •ê³„ìˆ˜ : ", lasso_model.best_estimator_.score(X_test,y_test))
print("ë¼ì˜ íšŒê·€ RMSE : ", np.sqrt(mean_squared_error(y_test, lasso_model.best_estimator_.predict(X_test))))
````

````
{'alpha': 0.0}
ë¼ì˜ íšŒê·€ ê²°ì •ê³„ìˆ˜ :  0.6901880385279766
ë¼ì˜ íšŒê·€ RMSE :  5.022698918447346
````

## 2. ë‹¤í•­ë¶„ì„ ì‹œê°í™”

### 2.1 ë‹¨ìˆœ ì„ í˜• íšŒê·€ë¥¼ ë‹¤í•­ íšŒê·€ë¡œ 3ì°¨ê¹Œì§€ ì ìš©ì‹œì¼œ ê³„ìˆ˜ë¥¼ êµ¬í•˜ê³  3ì°¨í•­ì„ ì ìš©í•œ ëª¨ë¸ì˜ ìŠ¤ìºí„° í”Œë¡¯ê³¼ ê¸°ìš¸ê¸° ì„ ì„ ê·¸ë¦¬ì‹œì˜¤.

````python
import pandas as pd
import numpy as np
m =100
X =6 * np.random.rand(m,1) -3 # [-3,3]ì˜ ëœë¤í•œ ê°’
y =3 * X**3 + X**2 +2*X +2 + np.random.randn(m,1) #ë…¸ì´ì¦ˆ í¬í•¨
line = np.linspace(-3,3,100, endpoint=False).reshape(-1,1)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures

## x**3 ê¹Œì§€ 3ì°¨í•­ì„ ì ìš©ì‹œì¼œì•¼ í•¨
poly = PolynomialFeatures(degree=3, include_bias=False)
poly.fit(X)
X_poly = poly.transform(X)
line_poly = poly.transfã… orm(line)
reg = LinearRegression().fit(X_poly, y)

plt.plot(line, reg.predict(line_poly), c='r',linewidth=3) # lineì€ -3ë¶€í„° 3ê¹Œì§€ì˜ xì¶•
plt.plot(X,y,'o',c ='g', alpha=0.5)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/13.png?raw=true" width="900" height="350"></p>

## 3. ANOVA ë¶„ì„

### 3.1 ë³€ìˆ˜ 3ê°œ(í•˜ë‚˜ëŠ” ì—°ì†í˜• ë³€ìˆ˜/ë‚˜ë¨¸ì§€ ë‘ ê°œëŠ” ë²”ì£¼í˜• ì—°ì†ë³€ìˆ˜)ì˜ ì´ì›ë¶„ì‚°ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  í†µê³„í‘œë¥¼ ì‘ì„±í•˜ì‹œì˜¤.

````python
import pandas as pd
import numpy as np
avocado = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/avocado.csv")
avocado = avocado[["AveragePrice","type","region"]]
avocado = avocado[(avocado['region']=='Orlando') |
                    (avocado['region']=='Boston' )|
                    (avocado['region']=='Chicago')].reset_index(drop=True)

avocado
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/14.png?raw=true" width="200" height="350"></p>

### 3.2 ê°€ì„¤ìˆ˜ë¦½

ğŸ“ **ìƒí˜¸ì‘ìš©íš¨ê³¼ ê²€ì •**

- H0: regionê³¼ avocado type ê°„ì—ëŠ” ìƒí˜¸ì‘ìš© íš¨ê³¼ê°€ ì—†ë‹¤.
- H1: regionê³¼ avocado type ê°„ì—ëŠ” ìƒí˜¸ì‘ìš© íš¨ê³¼ê°€ ìˆë‹¤.

ğŸ“ **ì£¼íš¨ê³¼ ê²€ì •**

- H0: region ì¢…ë¥˜ì— ë”°ë¥¸ AveragePrice ì°¨ì´ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.
- H1: region ì¢…ë¥˜ì— ë”°ë¥¸ AveragePrice ì°¨ì´ëŠ” ì¡´ì¬í•œë‹¤.
- H0: type ì¢…ë¥˜ì— ë”°ë¥¸ AveragePrice ì°¨ì´ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.
- H1: type ì¢…ë¥˜ì— ë”°ë¥¸ AveragePrice ì°¨ì´ëŠ” ì¡´ì¬í•œë‹¤.

````python
import numpy as np
import pandas as pd
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols

# ì´ì› ë¶„ì‚° ë¶„ì„ ìˆ˜í–‰
model = ols('AveragePrice ~ C(region) + C(type) + C(region):C(type)', data=avocado).fit()
anova_table = sm.stats.anova_lm(model, typ=2)

print("ì´ì› ë¶„ì‚° ë¶„ì„ ê²°ê³¼:")
print(anova_table)
````

````
ì´ì› ë¶„ì‚° ë¶„ì„ ê²°ê³¼:
                      sum_sq      df           F         PR(>F)
C(region)           0.432136     2.0    3.189242   4.161918e-02
C(type)            56.111007     1.0  828.218296  1.989417e-133
C(region):C(type)   1.878817     2.0   13.866003   1.146622e-06
Residual           68.291047  1008.0         NaN            NaN
````

- C(region):C(type): regionê³¼ typeì˜ ìƒí˜¸ì‘ìš© íš¨ê³¼ì— ëŒ€í•œ ê²€ì •ê²°ê³¼ p-valuerk 0.05ë³´ë‹¤ ì¦‰ìœ¼ë¯€ë¡œ ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•˜ì§€ ì•ŠëŠ”ë‹¤.
  - êµí˜¸ì‘ìš©ì´ ì¡´ì¬í•¨
- êµí˜¸ì‘ìš©ì´ ì¡´ì¬í•˜ê¸°ì— ì£¼íš¨ê³¼ ê²€ì •ì€ ì˜ë¯¸ê°€ ì—†ìŒ

````python
from statsmodels.graphics.factorplots import interaction_plot
import matplotlib.pyplot as plt

AveragePrice = avocado["AveragePrice"]
avocado_type = avocado["type"]
region = avocado["region"]

fig, ax = plt.subplots(figsize=(6, 6))
fig = interaction_plot(avocado_type, region, AveragePrice,
                       colors=['red', 'blue', 'black'], 
                       markers=['D', '^', 'o'], ms=10, ax=ax)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/15.png?raw=true" width="600" height="350"></p>

````python
fig, ax = plt.subplots(figsize=(6, 6))
fig = interaction_plot(region ,avocado_type, AveragePrice,
                       colors=['red', 'blue'], 
                       markers=['D', '^'], ms=10, ax=ax)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/problem/practice1/16.png?raw=true" width="600" height="350"></p>