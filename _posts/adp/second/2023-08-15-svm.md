---
layout: single
title:  'ADP ì‹¤ê¸° 7ì¥ Support Vector Machine'
toc: true
categories: [ADP]
tags: [ADP ì‹¤ê¸°]

---

ë³¸ ê²Œì‹œë¬¼ì€ Support Vector Machineì— ëŒ€í•´ ì†Œê°œí•œë‹¤.
{: .notice}

## 1. Support Vector Machine

### 1.1 Concpet

> SVMì€ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì…ë ¥ë˜ì—ˆì„ ë•Œ ê¸°ì¡´ ë°ì´í„°ë¥¼ í™œìš©í•´ ë¶„ë¥˜í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, SVCë¥¼ í™•ì¥í•œ ì•Œê³ ë¦¬ì¦˜

### 1.2 ìµœëŒ€ ë§ˆì§„ ë¶„ë¥˜ê¸°

- ì´ˆí‰ë©´: pì°¨ì› ê³µê°„ì—ì„œ ì°¨ì›ì´ (p-1)ì¸ í‰í‰í•œ affin ë¶€ë¶„ ê³µê°„
- margin: ê´€ì¸¡ì¹˜ë¶€í„° ì´ˆí‰ë©´ ì‚¬ì´ì˜ ê°€ì¥ ì§§ì€ ê±°ë¦¬
- maximal margin: ê´€ì¸¡ì¹˜ë¡œë¶€í„° ë§ˆì§„ì´ ê°€ì¥ í° í‰ë©´ì„ ì°¾ìŒ
- support vector: ì´ˆí‰ë©´ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ê´€ì¸¡ì¹˜

### 1.3 ì„œí¬íŠ¸ ë²¡í„° ë¶„ë¥˜ê¸°

- ì¼ë¶€ ì˜¤ë¥˜ë¥¼ ìˆ˜ìš©í•˜ëŠ” ìµœëŒ€ ë§ˆì§„ì„ ê°€ì§„ ì´ˆí‰ë©´ ë¶„ë¥˜ê¸°

### 1.4 ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ 

- SVCëŠ” ë°ì´í„°ê°€ ë‘ í´ë˜ìŠ¤ë¡œ ë‚˜ë‰˜ê³ , ê²½ê³„ê°€ ì„ í˜•ì¼ ê²½ìš°
- SVMì€ SVCì˜ ê°œë…ì„ í™•ì¥í•˜ì—¬ ì»¤ë„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ê³ ì°¨ì›ì—ì„œ ì„ í˜• ë¶„ë¥˜ë¥¼ í•˜ëŠ” ê¸°ë²•

### 1.5 ì„œí¬íŠ¸ ë²¡í„° íšŒê·€

- SVMì€ ì¼ì •í•œ ë§ˆì§„ ì˜¤ë¥˜ ì•ˆì—ì„œ ë‘ í´ë˜ìŠ¤ ê°„ì˜ ë„ë¡œ í­ì´ ìµœëŒ€ê°€ ë˜ë„ë¡ í•™ìŠµ
- SVRì€ ì œí•œëœ ë§ˆì§„ ì˜¤ë¥˜ ì•ˆì—ì„œ ë„ë¡œ ì•ˆì— ê°€ëŠ¥í•œ ë§ì€ ë°ì´í„° ìƒ˜í”Œì´ ì†í•˜ë„ë¡ í•™ìŠµ

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/sv/sv1.png?raw=true" width="600" height="200"></p>

## 2. Support Vector Classifier

### 2.1 Parameters

````python
class sklearn.svm.SVC(*, C=1.0, kernel=â€˜rbfâ€™, degree=3, gamma=â€˜scaleâ€™, coef0=0.0, 
shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, 
verbose=False, max_iter=- 1, decision_function_shape=â€˜ovrâ€™, break_ties=False, 
random_state=None)
````

- C: ì˜¤ë¶„ë¥˜ í—ˆìš© ê³„ìˆ˜ (C>0)
- kernel: linear, poly, rbf, sigmoid, precomputed
- degree: ì»¤ë„ í•¨ìˆ˜ë¥¼ polyë¡œ í•˜ì˜€ì„ ë•Œ ì„¤ì •í•¨
- gamma: ì»¤ë„ í•¨ìˆ˜ë¥¼ poly, rbf, sigmoidë¡œ ì„¤ì •í•˜ì˜€ì„ ë•Œ, scaling ê³„ìˆ˜
- class_weight: ê° í´ë˜ìŠ¤ì˜ ê°€ì¤‘ì¹˜ë¡œ Noneì¼ë•Œ ëª¨ë“  ê°€ì¤‘ì¹˜ëŠ” 1

### 2.2 Attributes

- class_weight_: ê° í´ë˜ìŠ¤ì˜ ê°€ì¤‘ì¹˜
- coef_: ì»¤ë„ì„ linearë¡œ í•˜ì˜€ì„ ë•Œ ê³„ìˆ˜
- support_vectors_: ì„œí¬íŠ¸ ë²¡í„°

### 2.3 Methods

- decision_function(X): ë°ì´í„° ìƒ˜í”Œì˜ confidence scoreë¥¼ ë°˜í™˜
- predict(X), predict_proba(X), predict_log_proba(X)

### 2.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
c=pd.read_csv(â€œhttps://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/classification.csvâ€)
c
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/sv/sv2.png?raw=true" width="400" height="300"></p>

````python
# ë°ì´í„° ë¶„í¬ í™•ì¸
sns.pairplot(hue='success', data=c)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/sv/sv3.png?raw=true" width="800" height="500"></p>

````python
# ë°ì´í„° ë¶„ë¦¬
from sklearn.model_selection import train_test_split
x=c[['age', 'interest']]
y=c['success']
train_x, test_x, train_y, test_y = train_test_split(x,y,stratify=y, 
train_size=0.7, random_state=1)
print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)
````

````
(207, 2) (90, 2) (207,) (90,)
````

````python
# ìŠ¤ì¼€ì¼ë§ ì ìš© ë° ë°ì´í„° ë¶„í¬ í™•ì¸
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
train_x=scaler.fit_transform(train_x)
sns.pairplot(data=pd.concat([pd.DataFrame(train_x),
 train_y.reset_index(drop=True)],
 axis=1),
 hue='success')
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/sv/sv4.png?raw=true" width="800" height="500"></p>

- SVMì€ ì´ìƒì¹˜ì— ë¯¼ê°í•˜ê¸°ì— Standard Scalerë¥¼ ì ìš©
- Scaler ì ìš© í›„ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ëª…í™•íˆ í™•ì¸ ê°€ëŠ¥

````python
# ë¶„ë¥˜ê¸° ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
from sklearn.svm import SVC
clf = SVC(C=0.5, random_state=45)
clf.fit(train_x, train_y)
````

````python
# í‰ê°€
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
test_x_scal = scaler.transform(test_x)
pred=clf.predict(test_x_scal)
test_cm=confusion_matrix(test_y, pred)
test_acc=accuracy_score(test_y, pred)
test_prc=precision_score(test_y, pred)
test_rcll=recall_score(test_y, pred)
test_f1=f1_score(test_y, pred)
print(test_cm)
print('ì •í™•ë„\t{}%'.format(round(test_acc*100,2)))
print('ì •ë°€ë„\t{}%'.format(round(test_prc*100,2)))
print('ì¬í˜„ìœ¨\t{}%'.format(round(test_rcll*100,2)))
print('F1\t{}%'.format(round(test_f1*100,2)))
````

````
[[37  2]
 [10 41]]
ì •í™•ë„	86.67%
ì •ë°€ë„	95.35%
ì¬í˜„ìœ¨	80.39%
F1	87.23%
````

ğŸ“ **ì˜¤ë¶„ë¥˜ ê³„ìˆ˜ì— ë”°ë¥¸ Marginì˜ ë³€í™”**

````python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import LinearSVC
plt.figure(figsize=(10, 5))
for i, C in enumerate([1, 500]):
 clf = LinearSVC(C=C, loss="hinge", random_state=42).fit(train_x, 
train_y)
 # decision functionìœ¼ë¡œ ì„œí¬íŠ¸ ë²¡í„° ì–»ê¸°
 decision_function = clf.decision_function(train_x)
 support_vector_indices = np.where(np.abs(decision_function) <= 1 +
1e-15)[0]
 support_vectors = train_x[support_vector_indices]
 plt.subplot(1, 2, i + 1)
 plt.scatter(train_x[:, 0], train_x[:, 1], c=train_y, s=30, cmap
=plt.cm.Paired)
 ax = plt.gca()
 xlim = ax.get_xlim()
 ylim = ax.get_ylim()
 xx, yy = np.meshgrid(
 np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50)
 )
 Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
 Z = Z.reshape(xx.shape)
 plt.contour(
 xx,
 yy,
 Z,
 colors="k",
 levels=[-1, 0, 1],
 alpha=0.5,
 linestyles=["--", "-", "--"],
 )
 plt.scatter(
 support_vectors[:, 0],
 support_vectors[:, 1],
 s=100,
 linewidth=1,
 facecolors="none",
 edgecolors="k",
 )
plt.title("C=" + str(C))
plt.tight_layout()
plt.show()

````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/sv/sv5.png?raw=true" width="900" height="300"></p>

## 3. Support Vector Regression

### 3.1 Parameters

````python
class sklearn.svm.SVR(*, kernel=â€˜rbfâ€™, degree=3, gamma=â€˜scaleâ€™, coef0=0.0, 
tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, 
max_iter=- 1)
````

- SVCì™€ ë™ì¼

### 3.2 Attributes

- SVCì™€ ë™ì¼

### 3.3 Methods

- ë¶„ë¥˜ê°€ ì•„ë‹ˆê¸°ì— probë¥¼ êµ¬í•˜ëŠ” methodëŠ” ì—†ìŒ
- ë‚˜ë¨¸ì§„ ë™ì¼

### 3.4 Implementation

ğŸ˜— **ë°ì´í„° ìƒì„±í•˜ê¸°**

````python
import numpy as np
X = np.sort(5 * np.random.rand(40, 1), axis=0)
y = np.sin(X).ravel()
print(X[0:6], '\n\n',y[0:10])
````

````
[[0.52871606]
 [0.57116235]
 [0.66348438]
 [0.69907167]
 [0.85967932]
 [0.96984676]] 

 [0.50442513 0.54061027 0.61586575 0.64350738 0.7576333  0.82479908
 0.83998352 0.86173085 0.86618382 0.89379124]
````

````python
# íƒ€ê¹ƒë°ì´í„°ì— ë…¸ì´ì¦ˆ ì¶”ê°€í•˜ê¸°
y[::5] += 3 * (0.5 - np.random.rand(8))
print(y[0:10])
````

- ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œ ì‹¤ì œ í™˜ê²½ê³¼ ë¹„ìŠ·í•˜ê²Œ í•¨

````python
# ì»¤ë„ë³„ íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
from sklearn.svm import SVR
svr_rbf = SVR(kernel="rbf", C=100, gamma=0.1, epsilon=0.1)
svr_lin = SVR(kernel="linear", C=100, gamma="auto")
svr_poly = SVR(kernel="poly", C=100, gamma="auto", degree=3, 
epsilon=0.1, coef0=1)
svr_rbf.fit(X, y)
svr_lin.fit(X, y)
svr_poly.fit(X, y)
````

````python
# ì»¤ë„ë³„ ëª¨ë¸ í‰ê°€
rbf_pred=svr_rbf.predict(X)
lin_pred=svr_lin.predict(X)
poly_pred=svr_poly.predict(X)
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error
import pandas as pd
import numpy as np
preds = [rbf_pred, lin_pred, poly_pred]
kernel = ['Random_Forest', 'Linear', 'Polynomial']
evls = ['mse', 'rmse', 'mae']
results=pd.DataFrame(index=kernel,columns=evls)
for pred, nm in zip(preds, kernel):
 mse = mean_squared_error(y, pred)
 mae = mean_absolute_error(y, pred)
 rmse = np.sqrt(mse)
 
 results.loc[nm]['mse']=round(mse,2)
 results.loc[nm]['rmse']=round(rmse,2)
 results.loc[nm]['mae']=round(mae,2)
results
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/sv/sv6.png?raw=true" width="300" height="100"></p>

````python
lw = 2
svrs = [svr_rbf, svr_lin, svr_poly]
kernel_label = ["RBF", "Linear", "Polynomial"]
model_color = ["m", "c", "g"]
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)
for ix, svr in enumerate(svrs):
   axes[ix].plot(
     X,
     svr.fit(X, y).predict(X),
     color=model_color[ix],
     lw=lw,
     label="{} model".format(kernel_label[ix]),
   )
   axes[ix].scatter(
     X[svr.support_],
     y[svr.support_],
     facecolor="none",
     edgecolor=model_color[ix],
     s=50,
     label="{} support vectors".format(kernel_label[ix]),
   )
   axes[ix].scatter(
     X[np.setdiff1d(np.arange(len(X)), svr.support_)],
     y[np.setdiff1d(np.arange(len(X)), svr.support_)],
     facecolor="none",
     edgecolor="k",
     s=50,
     label="other training data",
   )
   axes[ix].legend(
     loc="upper center",
     bbox_to_anchor=(0.5, 1.1),
     ncol=1,
     fancybox=True,
     shadow=True,
   )
fig.text(0.5, 0.04, "data", ha="center", va="center")
fig.text(0.06, 0.5, "target", ha="center", va="center", rotation="vertical")
fig.suptitle("Support Vector Regression", fontsize=14)
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/sv/sv7.png?raw=true" width="900" height="600"></p>
