---
layout: single
title:  'ADP ì‹¤ê¸° 4ì¥ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì„¸ìŠ¤'
toc: true
categories: [ADP]
tags: [ADP ì‹¤ê¸°]

---

ë³¸ ê²Œì‹œë¬¼ì€ í‰ê°€í•¨ìˆ˜ì™€ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ ì˜ˆì œì— ëŒ€í•´ ì†Œê°œí•œë‹¤.
{: .notice}

## 1. í‰ê°€ í•¨ìˆ˜

### 1.1 íšŒê·€ ë¶„ì„

ğŸ“ **MAE**

- ì ˆëŒ€ê°’ ì°¨ì´
- ì—ëŸ¬ì˜ í¬ê¸°ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ì˜ â˜ ì´ìƒì¹˜ì— ì˜í–¥ì„ ë°›ìŒ

````python
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, y_pred)
````

ğŸ“ **MSE**

- ì œê³±í•© í‰ê·  â˜ ì‹¤ì¸¡ê³¼ ì˜ˆì¸¡ ì°¨ì´ì˜ ë©´ì ê³¼ ë™ì¼
- íŠ¹ì´ê°’ì´ ì¡´ì¬í•˜ë©´ ìˆ˜ì¹˜ ì¦ê°€

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ml/ml1.png?raw=true" width="650" height="350"></p>

````python
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)
````

ğŸ“ **RMSE**

- MSEì— ë£¨íŠ¸ ì ìš©
- ì œê³±í•©ìœ¼ë¡œ ì¸í•´ ì»¤ì§€ëŠ” ì†ì‹¤ì„ scale down í•¨

````python
from sklearn.metrics import mean_squared_error
import numpy as np
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse) 
````

ğŸ“ **MSLE**

- MSEì— ë¡œê·¸ ì ìš©
- ì°¨ì´ê°€ ì‘ì„ìˆ˜ë¡ í° ê°’ì„ ê°€ì§€ê¸°ì— ì˜ˆì¸¡ê°’ì´ ì‘ì„ë•Œ ìœ ìš©

```` python
from sklearn.metrics import mean_squared_log_error
msle = mean_squared_log_error(y_test, y_pred) 
````

ğŸ“ **MAPE**

- MAEë¥¼ í¼ì„¼íŠ¸ë¡œ ë³€í™˜
- ì˜¤ì°¨ê°€ ì˜ˆì¸¡ê°’ì—ì„œ ì°¨ì§€í•˜ëŠ” í¼ì„¼íŠ¸

````python
import numpy as np
def MAPE(y_test, y_pred):
 mape = np.mean(np.abs((y_test - y_pred)/y_test)) * 100
 return mape
mape = MAPE(y_test, y_pred)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ml/ml2.png?raw=true" width="650" height="350"></p>

### 1.2 ë¶„ë¥˜ ë¶„ì„

ğŸ“ **Accuracy**

````python
from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test, y_pred)
````

ğŸ“ **Confusion Matrix**

````python
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
````

ğŸ“ **Precision**

````python
from sklearn.metrics import precision_score
precision = precision_score(y_test, y_pred)
````

ğŸ“ **Recall**

````python
from sklearn.metrics import recall_score
recall = recall_score(y_test, y_pred)
````

ğŸ“ **F1-score**

````python
from sklearn.metrics import f1_score
f1 = f1_score(y_test, y_pred)
````

ğŸ“ **ROC curve**

- ë¶„ë¥˜ ê¸°ì¤€ì´ ë˜ëŠ” thresholdë¥¼ ê¸°ì¤€ìœ¼ë¡œ FFRê³¼ TPRì„ êµ¬í•˜ê³  ê·¸ë˜í”„ë¥¼ ê·¸ë¦¼

````python
from sklearn.metrics import roc_curve
fpr, tpr, thres = roc_curve(y_test, y_pred, pos_label = 1)
import matplotlib.pyplot as plt
plt.plot(fpr, tpr)
````

ğŸ“**AUC score**

- ROC curveì˜ ì•„ë˜ ë©´ì ìœ¼ë¡œ ëœë¤ ìˆ˜ì¤€ì˜ AUC scoreëŠ” 0.5

````python
from sklearn.metrics import roc_curve, auc
fpr, tpr, thres = roc_curve(y_test, y_pred, pos_label = 1)
auc = auc(fpr, tpr)
````

## 2. íšŒê·€ë¶„ì„

ğŸ˜— **ë°ì´í„° ê°€ì ¸ì˜¤ê¸°**

````python
from sklearn import datasets
import pandas as pd
df, price = datasets.fetch_openml('boston', return_X_y=True)
df['PRICE'] = price
````

### 2.1 ë°ì´í„° í™•ì¸í•˜ê¸°

````python
# ëŒ€ëµì  í˜•íƒœ í™•ì¸
df.head()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ml/ml3.png?raw=true" width="600" height="200"></p>

````python
# í–‰, ì—´ í™•ì¸
df.shape
````

````
(506, 14)
````

````python
# íƒ€ì… í™•ì¸
df.info()
````

````
 #   Column   Non-Null Count  Dtype   
---  ------   --------------  -----   
 0   CRIM     506 non-null    float64 
 1   ZN       506 non-null    float64 
 2   INDUS    506 non-null    float64 
 3   CHAS     506 non-null    category
 4   NOX      506 non-null    float64 
 5   RM       506 non-null    float64 
 6   AGE      506 non-null    float64 
 7   DIS      506 non-null    float64 
 8   RAD      506 non-null    category
 9   TAX      506 non-null    float64 
 10  PTRATIO  506 non-null    float64 
 11  B        506 non-null    float64 
 12  LSTAT    506 non-null    float64 
 13  PRICE    506 non-null    float64 
dtypes: category(2), float64(12)
````

- CHASë¥¼ ì œì™¸í•˜ê³ ëŠ” ëª¨ë‘ floatí˜•ì´ì–´ì•¼ í•˜ì§€ë§Œ, RADê°€ 1.0, 2.0 .. ê³¼ ê°™ì€ í˜•íƒœë¼ categoryë¡œ ì¸ì‹ë¨ â˜ í˜•ë³€í™˜ í•„ìš”

````python
df['RAD'] = df['RAD'].astype('float')
df.info()
````

````
 #   Column   Non-Null Count  Dtype   
---  ------   --------------  -----   
 0   CRIM     506 non-null    float64 
 1   ZN       506 non-null    float64 
 2   INDUS    506 non-null    float64 
 3   CHAS     506 non-null    category
 4   NOX      506 non-null    float64 
 5   RM       506 non-null    float64 
 6   AGE      506 non-null    float64 
 7   DIS      506 non-null    float64 
 8   RAD      506 non-null    float64 
 9   TAX      506 non-null    float64 
 10  PTRATIO  506 non-null    float64 
 11  B        506 non-null    float64 
 12  LSTAT    506 non-null    float64 
 13  PRICE    506 non-null    float64 
dtypes: category(1), float64(13)
````

````python
# null í™•ì¸
df.isna().sum()
````

````
CRIM       0
ZN         0
INDUS      0
CHAS       0
NOX        0
RM         0
AGE        0
DIS        0
RAD        0
TAX        0
PTRATIO    0
B          0
LSTAT      0
PRICE      0
dtype: int64
````

### 2.2 ë°ì´í„° ì‹œê°í™”

````python
import matplotlib.pyplot as plt
import seaborn as sns
# 3ê°œì˜ í–‰ê³¼ 4ê°œì˜ ì—´ì„ ê°€ì§„ subplot ê·¸ë¦¬ê¸°
fig, axs = plt.subplots(figsize=(16,10), ncols=4, nrows=3, constrained_layout=True)
features = df.columns.difference(['PRICE', 'CHAS'])

for i, feature in zip(range(12), features):
    row = int(i/4) # í–‰ë²ˆí˜¸ ì„¤ì •
    col = i%4 # ì—´ë²ˆí˜¸ ì„¤ì •
    # seabornì˜ regplotì„ ì´ìš©í•´ ì‚°ì ë„ì™€ ì„ í˜• íšŒê·€ì§ì„ ì„ í•¨ê»˜ ì‹œê°í™”í•¨
    sns.regplot(x=feature, y=df['PRICE'], data=df, ax=axs[row][col])
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ml/ml4.png?raw=true" width="900" height="700"></p>

### 2.3 ë°ì´í„° ë¶„í• 

````python
from sklearn.model_selection import train_test_split
x = df[['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']].values
y = df['PRICE'].values
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
print('í•™ìŠµë°ì´í„°ì„¸íŠ¸ PRICE í‰ê· : ', y_train.mean())
print('í‰ê°€ë°ì´í„°ì„¸íŠ¸ PRICE í‰ê· : ', y_test.mean())
````

````
í•™ìŠµë°ì´í„°ì„¸íŠ¸ PRICE í‰ê· :  22.796534653465343
í‰ê°€ë°ì´í„°ì„¸íŠ¸ PRICE í‰ê· :  21.488235294117644
````

### 2.4 ì „ì²˜ë¦¬

````python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_train_scaled = scaler.fit_transform(x_train) 
````

### 2.5 ëª¨ë¸ í•™ìŠµ

````python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.metrics import r2_score
import numpy as np

# ëª¨ë¸ í•›ë¸Œ
linear = LinearRegression()
linear.fit(x_train_scaled, y_train)

# í•™ìŠµë°ì´í„°ë¡œ í‰ê°€
pred_train = linear.predict(x_train_scaled)
mae = mean_absolute_error(y_train, pred_train)
mse = mean_squared_error(y_train, pred_train)
rmse = np.sqrt(mse)
r2 = r2_score(y_train, pred_train)
print('MAE: {0: .5f}'.format(mae))
print('MSE: {0: .5f}'.format(mse))
print('RMSE: {0: .5f}'.format(rmse))

# ëª¨ë¸ì˜ ì„¤ëª…ë ¥
print('R2: {0: .5f}'.format(r2))
````

````
MAE:  3.32616
MSE:  22.11246
RMSE:  4.70239
R2:  0.74546
````

- R2ëŠ” ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì„ ì˜ë¯¸í•¨

### 2.6 í‰ê°€

````python
x_test_scaled = scaler.transform(x_test)
pred = linear.predict(x_test_scaled)

mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, pred)
print('MAE: {0: .5f}'.format(mae))
print('MSE: {0: .5f}'.format(mse))
print('RMSE: {0: .5f}'.format(rmse))
print('R2: {0: .5f}'.format(r2))
````

````python
MAE:  3.23724
MSE:  24.63539
RMSE:  4.96341
R2:  0.66406
````

## 3. ë¶„ë¥˜ë¶„ì„

ğŸ˜— **ë°ì´í„° ê°€ì ¸ì˜¤ê¸°**

````python
from sklearn.datasets import load_iris
import pandas as pd
iris = load_iris()
iris_dt = iris.data
iris_label = iris.target
df = pd.DataFrame(data=iris_dt, columns=iris.feature_names)
df['label'] = iris_label
````

### 3.1 ë°ì´í„° í™•ì¸

````
df.head()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ml/ml5.png?raw=true" width="650" height="250"></p>

````python
df.label.unique()
````

````
array([0, 1, 2])
````

````
df.shape
````

````
(150, 5)
````

````python
df.isna().sum()
````

````
sepal length (cm) 0
sepal width (cm) 0
petal length (cm) 0
petal width (cm) 0
Species 0
dtype: int64 
````

### 3.2 ë°ì´í„° ë¶„í• 

````python
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(iris_dt, iris_label, test_size=0.2, random_state=0, stratify=iris_label)
````

### 3.3 ëª¨ë¸ í•™ìŠµ

````python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

dtree_clf_5 = DecisionTreeClassifier(max_depth=5, random_state=100)
dtree_clf_3 = DecisionTreeClassifier(max_depth=3, random_state=100)
dtree_clf_1 = DecisionTreeClassifier(max_depth=1, random_state=100) 

scores = cross_val_score(dtree_clf_5, x_train, y_train, scoring='accuracy', cv=10)
print("max depth 5")
print('êµì°¨ê²€ì¦ ì •í™•ë„: ', np.round(scores, 3))
print('í‰ê·  ê²€ì¦ ì •í™•ë„: ', np.round(np.mean(scores), 4))

scores = cross_val_score(dtree_clf_3, x_train, y_train, scoring='accuracy', cv=10)
print("max depth 3")
print('êµì°¨ê²€ì¦ ì •í™•ë„: ', np.round(scores, 3))
print('í‰ê·  ê²€ì¦ ì •í™•ë„: ', np.round(np.mean(scores), 4))


scores = cross_val_score(dtree_clf_1, x_train, y_train, scoring='accuracy', cv=10)
print("max depth 1")
print('êµì°¨ê²€ì¦ ì •í™•ë„: ', np.round(scores, 3))
print('í‰ê·  ê²€ì¦ ì •í™•ë„: ', np.round(np.mean(scores), 4))
````

````
max depth 5
êµì°¨ê²€ì¦ ì •í™•ë„:  [0.917 1.    0.917 1.    1.    0.833 1.    0.917 1.    0.833]
í‰ê·  ê²€ì¦ ì •í™•ë„:  0.9417
max depth 3
êµì°¨ê²€ì¦ ì •í™•ë„:  [0.917 1.    0.917 0.917 1.    0.833 1.    0.917 0.917 0.833]
í‰ê·  ê²€ì¦ ì •í™•ë„:  0.925
max depth 1
êµì°¨ê²€ì¦ ì •í™•ë„:  [0.667 0.667 0.667 0.667 0.667 0.667 0.667 0.667 0.667 0.667]
í‰ê·  ê²€ì¦ ì •í™•ë„:  0.6667
````

### 3.4 ì„±ëŠ¥ í‰ê°€

````python
dtree_clf_5.fit(x_train, y_train)
pred = dtree_clf_5.predict(x_test)
from sklearn.metrics import accuracy_score
print('ì˜ì‚¬ê²°ì •ë‚˜ë¬´(êµì°¨ê²€ì¦ í›„) ì˜ˆì¸¡ ì •í™•ë„: {0:.5f}'.format(accuracy_score(y_test, pred)))
````

````
ì˜ì‚¬ê²°ì •ë‚˜ë¬´(êµì°¨ê²€ì¦ í›„) ì˜ˆì¸¡ ì •í™•ë„: 0.96667
````



















