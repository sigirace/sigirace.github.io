---
layout: single
title:  'ADP ì‹¤ê¸° 10ì¥ Ensemble'
toc: true
categories: [ADP]
tags: [ADP ì‹¤ê¸°]

---

ë³¸ ê²Œì‹œë¬¼ì€ Ensembleì— ëŒ€í•´ ì†Œê°œí•œë‹¤.
{: .notice}

## 1. Ensemble

### 1.1 Concept

> ë‹¨ì¼ ê²°ì • íŠ¸ë¦¬ì˜ ë‹¨ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì—°ê²°í•˜ì—¬ ê°•í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²•

### 1.2 Bootstrap

- ëœë¤ ìƒ˜í”Œë§ì˜ ì¼ì¢…
- ë‹¨ìˆœì„ì˜ë³µì›ì¶”ì¶œ(=ì¤‘ë³µí—ˆìš©)

### 1.3 Bagging

- ì£¼ì–´ì§„ ìë£Œë¥¼ ëª¨ì§‘ë‹¨ìœ¼ë¡œ ë³´ê³  ì—¬ëŸ¬ ê°œì˜ bootstrap ìƒì„±
- ê° bootstrap ìë£Œì— ëŒ€í•´ ì˜ˆì¸¡ ëª¨í˜•ì„ ë§Œë“  í›„ ê²°í•©
- ë³´íŒ…: ê° íŠ¸ë¦¬ë¥¼ ìµœëŒ€ë¡œ ì„±ì¥ì‹œí‚¨ í›„ (=ê°€ì§€ì¹˜ê¸° ì•ˆí•¨, ì˜¤ë²„í”¼íŒ… ê³ ë ¤ ì•ˆí•¨) ë‹¤ìˆ˜ê²°
- íŠ¹ì§•
  - ê° bootstrapì— ëŒ€í•´ ë³‘ë ¬ ìˆ˜í–‰
  - ë¶„ì‚°ì´ ì ì€ ì•™ìƒë¸” ëª¨ë¸ì„ ì–»ìŒ
- OOB: í‰ê· ì ìœ¼ë¡œ 63%ì •ë„ë§Œ ìƒ˜í”Œë§ ë˜ê¸°ì— ë‚˜ë¨¸ì§€ 37%ë¥¼ í†µí•´ ê²€ì¦
  - ë‹¨ ëª¨ë¸ë§ˆë‹¤ ë‚¨ê²¨ì§„ ë°ì´í„°ëŠ” ë‹¤ë¦„

### 1.4 Boosting

- ì•½í•œ ëª¨í˜•(íŠ¸ë¦¬ê°€ ì‘ì€)ì„ ê²°í•©í•´ ê°•í•œ ëª¨í˜•ì„ ë§Œë“¦
- ë³‘ë ¬ì´ ì•„ë‹Œ ìˆœì°¨ì  í•™ìŠµ ì§„í–‰
- trainì— ëŒ€í•´ ì˜¤ë¥˜ê°€ ì‘ìœ¼ë‚˜ ê³¼ì í•© ìœ„í—˜ ìˆìŒ

### 1.5 Random Forest

- baggingê³¼ boosting ë³´ë‹¤ ë” ë§ì€ ë¬´ì‘ìœ„ì„±ì„ ì£¼ì–´ ì•½í•œ ëª¨ë¸ ìƒì„± í›„ ì„ í˜•ê²°í•©
- ìˆ˜ì²œ ê°œì˜ ë³€ìˆ˜ë¥¼ ì œê±° ì—†ì´ ëª¨ë¸ë§ í•˜ì—¬ ì¢‹ì€ ì •í™•ë„
- ê²°ê³¼ í•´ì„ì´ ì–´ë ¤ì›€
- ì…ë ¥ë³€ìˆ˜ê°€ ë§ì€ ê²½ìš° ë°°ê¹… ë° ë¶€ìŠ¤íŒ…ê³¼ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ì¢‹ìŒ

## 2. Bagging Classifier

### 2.1 Parameters

````python
class sklearn.ensemble.BaggingClassifier(estimator=None, n_estimators=10, *, 
max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, 
oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)
````

- estimator: ë°°ê¹…ì—ì„œ ìˆ˜í–‰í•  ë¶„ë¥˜ê¸° (defaultëŠ” ë‹¨ì¼ D.T)
- n_estimators: ëª¨ë¸ ìˆ˜
- max_samples: ê° ëª¨ë¸ì— ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜ ë¹„ìœ¨ (0~1)
- max_features: ê° ëª¨ë¸ì— ì‚¬ìš©í•  ì»¬ëŸ¼ ë¹„ìœ¨ (0~1)
- oob_score: ì¼ë°˜í™” ì˜¤ë¥˜ ì¶”ì •ì„ ìœ„í•´ oob ìƒ˜í”Œ ì‚¬ìš© ì—¬ë¶€

### 2.2 Attributes

- oob_score_: oobë¥¼ ì‚¬ìš©í•´ ì–»ì€ train ë°ì´í„°ì˜ ì ìˆ˜, ë§¤ê°œë³€ìˆ˜ê°€ Trueë¡œ ì„¤ì •ë˜ì–´ì•¼ í•¨

### 2.3 Methods

- fit(X, y)
- predict(X)
- predict_proba(X)
- score(X, y): ë¶„ë¥˜ê¸°ì´ê¸°ì— ì˜ˆì¸¡ì˜ ì •í™•ë„ë¥¼ ë°˜í™˜

### 2.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
breast = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/breast-cancer.csv")
breast
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens1.png?raw=true" width="900" height="270"></p>

````python
# target ì‹œê°í™”
import seaborn as sns
import matplotlib.pyplot as plt
plt.figure()
sns.countplot(x='diagnosis', data =breast)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens2.png?raw=true" width="600" height="300"></p>

````python
# target - feature ìƒê´€ê´€ê³„ ì‹œê°í™”
# area_maen, texture_meanê³¼ diagnosisì˜ ê´€ê³„ í™•ì¸
sns.relplot(x='area_mean', y="texture_mean", hue='diagnosis', data=breast)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens3.png?raw=true" width="600" height="300"></p>

````python
# ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬
import numpy as np
from sklearn.model_selection import train_test_split
breast["diagnosis"] = np.where(breast["diagnosis"]=="M", 1, 0)

# feature, target ì„¤ì •
features = ["area_mean", "area_worst"]
X = breast[features]
y = breast["diagnosis"]

# train_test split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size =0.3, 
stratify =y, random_state =1)
print(x_train.shape, x_test.shape)
print(y_train.shape, y_test.shape)
````

````
(398, 2) (171, 2)
(398,) (171,)
````

````python
# modeling
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
clf = BaggingClassifier(estimator =DecisionTreeClassifier())
pred = clf.fit(x_train, y_train).predict(x_test)
print("Accuracy Score : ", clf.score(x_test, y_test))
````

````
Accuracy Score :  0.9239766081871345
````

````python
# í‰ê°€
from sklearn.metrics import confusion_matrix 
pd.DataFrame(confusion_matrix(y_test, pred),
 index=['True[0]', 'True[1]'],
 columns=['Pred[0]','Pred[1]'])
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens4.png?raw=true" width="200" height="100"></p>

````python
# ROC Curve, AUC Score
import matplotlib.pyplot as plt

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(x_test)[:, 1])
roc_auc = roc_auc_score(y_test, clf.predict_proba(x_test)[:, 1])

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

print("ROC_AUC_score : ", roc_auc)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens5.png?raw=true" width="600" height="400"></p>

````
ROC_AUC_score :  0.9324620327102803
````

````python
# oob score
clf_oob=BaggingClassifier(estimator =DecisionTreeClassifier(), 
                          n_estimators=50, 
                          oob_score=True)
oob=clf_oob.fit(X, y).oob_score_
print(oob)
````

````
0.9244288224956063
````



## 3. Bagging Regressor

### 3.1 Parameters

````
class sklearn.ensemble.BaggingRegressor(base_estimator=None, n_estimators=10, *, 
max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, 
oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0) 
````

- classifierì™€ ë™ì¼

### 3.2 Attributes

- oob_score_: classifierì™€ ë™ì¼

### 3.3 Methods

- classifierì™€ ë™ì¼

### 3.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
car = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/CarPrice_Assignment.csv")
car.info()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens6.png?raw=true" width="400" height="500"></p>

````python
# target, feature ì„¤ì •
car_num = car.select_dtypes(['number'])
features = list(car_num.columns.difference(['car_ID', 'symboling', 'price']))
X=car_num[features]
y=car_num['price']
print(X.shape, y.shape)
````

````
(205, 13) (205,)
````

````python
# oob score
from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
reg = BaggingRegressor(estimator =DecisionTreeRegressor(),
                       n_estimators=50,
                        oob_score=True)
reg=reg.fit(X, y)
reg.oob_score_
````

````
0.9224681669421886
````



## 4. AdaBoost Classifier

### 4.1 Parameters

````python
class sklearn.ensemble.AdaBoostClassifier(estimator=None, *, n_estimators=50, 
learning_rate=1.0, algorithm=â€˜SAMME.Râ€™, random_state=None) 
````

- estimator: ëª¨ë¸ ì¢…ë¥˜ Noneì¼ì‹œ D.T
- n_esitmators: ì¢…ë£Œ ì¡°ê±´ (ìµœëŒ€ ëª¨ë¸ ìˆ˜)
- learning_rate: ë°˜ë³µì‹œ ì ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜

### 4.2 Attributes

- feature_importance_: ë¶ˆìˆœë„ ê¸°ë°˜ì˜ ë³€ìˆ˜ ì¤‘ìš”ë„ ì¶œë ¥

### 4.3 Methods

- fit(X,y)
- predict(X)
- predict_proba(X)
- score(X,y): í‰ê· ì •í™•ë„

### 4.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
breast = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/breast-cancer.csv")
````

````python
import numpy as np
from sklearn.model_selection import train_test_split

# ë²”ì£¼í˜• ë³€ìˆ˜ ë³€í™˜
breast["diagnosis"] = np.where(breast["diagnosis"]=="M", 1, 0)
features = ["area_mean", "texture_mean"]

# feature target ì„¤ì •
X = breast[features]
y = breast["diagnosis"]

# data split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size =0.3, stratify =y, random_state =1)
print(x_train.shape, x_test.shape)
print(y_train.shape, y_test.shape)
````

````
(398, 2) (171, 2)
(398,) (171,)
````

````python
# modeling & eval
from sklearn.ensemble import AdaBoostClassifier
clf = AdaBoostClassifier(estimator =None)
pred=clf.fit(x_train, y_train).predict(x_test)
print("ì •í™•ë„ : ", clf.score(x_test, y_test))
````

````
ì •í™•ë„ :  0.9122807017543859
````

````python
# confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
pred=clf.predict(x_test)
test_cm=confusion_matrix(y_test, pred)
test_acc=accuracy_score(y_test, pred)
test_prc=precision_score(y_test, pred)
test_rcll=recall_score(y_test, pred)
test_f1=f1_score(y_test, pred)
print(test_cm)
print('ì •í™•ë„\t{}%'.format(round(test_acc *100,2)))
print('ì •ë°€ë„\t{}%'.format(round(test_prc *100,2)))
print('ì¬í˜„ìœ¨\t{}%'.format(round(test_rcll *100,2)))
````

````
[[102   5]
 [ 10  54]]
ì •í™•ë„	91.23%
ì •ë°€ë„	91.53%
ì¬í˜„ìœ¨	84.38%
````

````python
# ROC, AUC
import matplotlib.pyplot as plt

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(x_test)[:, 1])
roc_auc = roc_auc_score(y_test, clf.predict_proba(x_test)[:, 1])

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

print("ROC_AUC_score : ", roc_auc)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens7.png?raw=true" width="600" height="400"></p>

````
ROC_AUC_score :  0.9444363317757009
````

````python
# feature importances
importances = clf.feature_importances_
column_nm = pd.DataFrame(["area_mean", "texture_mean"])
feature_importances = pd.concat([column_nm,
 pd.DataFrame(importances)],
 axis=1)
feature_importances.columns = ['feature_nm', 'importances']
print(feature_importances)
````

````
     feature_nm  importances
0     area_mean         0.56
1  texture_mean         0.44
````

````python
# feature importance ì‹œê°í™”
f = features
xtick_label_position = list(range(len(f)))
plt.xticks(xtick_label_position, f)
plt.bar([x for x in range(len(importances))], importances)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens8.png?raw=true" width="500" height="300"></p>

## 5. AdaBoost Regressor

### 5.1 Parameters

````python
class sklearn.ensemble.AdaBoostRegressor(base_estimator=None, *, 
n_estimators=50, learning_rate=1.0, loss=â€˜linearâ€™, random_state=None)
````

- classifierì™€ ë™ì¼

### 5.2 Attributes

- classifierì™€ ë™ì¼

### 5.3 Methods

- classifierì™€ ë™ì¼

### 5.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
car = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/CarPrice_Assignment.csv")
````

````python
# ì „ì²˜ë¦¬
car_num = car.select_dtypes(['number'])

# feature & target ì„¤ì •
features = list(car_num.columns.difference(['car_ID', 'symboling', 'price']))
X=car_num[features]
y=car_num['price']

# train_test split
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size =0.3, random_state =1)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
````

````
(143, 13)
(62, 13)
(143,)
(62,)
````

````python
# modeling
from sklearn.ensemble import AdaBoostRegressor
reg = AdaBoostRegressor(estimator =None)
pred=reg.fit(x_train, y_train).predict(x_test)

# eval
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error
mse = mean_squared_error(y_test, pred)
mae = mean_absolute_error(y_test, pred)
rmse = np.sqrt(mse)
acc = reg.score(x_test, y_test)
print('MSE\t{}'.format(round(mse,3)))
print('MAE\t{}'.format(round(mae,3)))
print('RMSE\t{}'.format(round(rmse,3)))
print('ACC\t{}%'.format(round(acc *100,3)))
````

````
MSE	6047513.193
MAE	1847.222
RMSE	2459.169
ACC	89.983%
````

````python
# feature importance
importances = reg.feature_importances_
column_nm = pd.DataFrame(features)
feature_importances = pd.concat([column_nm,
 pd.DataFrame(importances)],
 axis=1)
feature_importances.columns = ['feature_nm', 'importances']
print(feature_importances)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens9.png?raw=true" width="300" height="300"></p>

````python
# feature importance ì‹œê°í™”
n_features = x_train.shape[1]
importances = reg.feature_importances_
column_nm = features
plt.barh(range(n_features), importances, align ='center')
plt.yticks(np.arange(n_features), column_nm)
plt.xlabel("feature importances")
plt.ylabel("feature")
plt.ylim(-1, n_features)
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens10.png?raw=true" width="600" height="400"></p>



## 6. Random Forest Classifier

### 6.1 Parameters

````python
class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion=â€˜giniâ€™, 
max_depth=None, min_samples_split=2, min_samples_leaf=1, 
min_weight_fraction_leaf=0.0, max_features=â€˜autoâ€™, max_leaf_nodes=None, 
min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, 
random_state=None, verbose=0, warm_start=False, class_weight=None, 
ccp_alpha=0.0, max_samples=None)
````

- n_estimators: DTì˜ ê°œìˆ˜
- criterion: ë¶„í•  ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ë¶ˆìˆœë„ (gini, entropy)
- max_depth: ë‚˜ë¬´ì˜ ìµœëŒ€ ê¹Šì´, Noneì´ë©´ ëª¨ë“  ë…¸ë“œì˜ ë¶ˆìˆœë„ê°€ 0ì´ê±°ë‚˜ min_sample_split ë¯¸ë§Œì˜ ìƒ˜í”Œë§Œ ì¡´ì¬í•  ë•Œ ê¹Œì§€ ë¶„í•  í™•ì¥
- min_samples_split: ë‚´ë¶€ ë…¸ë“œë¥¼ ë¶„í• í•˜ëŠ”ë° í•„ìš”í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
- min_samples_leaf: ë¦¬í”„ ë…¸ë“œì— ìˆì–´ì•¼ í•˜ëŠ” ìµœì†Œ ìƒ˜í”Œ ìˆ˜
- max_leaf_nodes: ë¦¬í”„ ë…¸ë“œì˜ ìµœëŒ€ ê°œìˆ˜, Noneì´ë©´ ì œí•œì´ ì—†ìŒ
- bootstrap: Falseë©´ ëª¨ë“  ë°ì´í„° ì‚¬ìš©
- oob: 
- ccp_aplha: pruningì— ì‚¬ìš©í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ, ìµœì†Œ ë¹„ìš©-ë³µì¡ì„± ì •ë¦¬ì— ì‚¬ìš©ë¨, ccp_alphaë³´ë‹¤ ì‘ì€ ë¹„ìš©-ë³µì¡ì„±ì„ ê°€ì§„ ì„œë¸ŒíŠ¸ë¦¬ì¤‘ ê°€ì¥ ë¹„ìš©-ë³µì¡ì„±ì´ í° íŠ¸ë¦¬ë¥¼ ì„ íƒ, Noneì¼ ê²½ìš° pruningì€ ìˆ˜í–‰ë˜ì§€ ì•ŠìŒ
- min_impurity_decrease: ë…¸ë“œê°€ ë¶„í• ë˜ëŠ” ì¡°ê±´ìœ¼ë¡œ í•´ë‹¹ ê°’ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì€ ìˆ˜ì¤€ìœ¼ë¡œ ë¶ˆìˆœë„ê°€ ê°ì†Œí•  ê²½ìš° ë…¸ë“œê°€ ë¶„í• 

### 6.2 Attributes

- feature_importances_: ë³€ìˆ˜ ì¤‘ìš”ë„
- oob_score_

### 6.3 Methods

- fit(X,y)
- predict(X)
- predict_proba(X)
- score(X,y)

### 6.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
breast = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/breast-cancer.csv")
````

````python
import numpy as np
from sklearn.model_selection import train_test_split
breast["diagnosis"] = np.where(breast["diagnosis"]=="M", 1, 0)
features = ["area_mean", "texture_mean"]
X = breast[features]
y = breast["diagnosis"]
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size =0.3, 
stratify =y, random_state =1)
print(x_train.shape, x_test.shape)
print(y_train.shape, y_test.shape)
````

````
(398, 2) (171, 2)
(398,) (171,)
````

````python
from sklearn.ensemble import RandomForestClassifier 
clf = RandomForestClassifier(n_estimators =100, min_samples_split =5)
pred=clf.fit(x_train, y_train).predict(x_test)
print("ì •í™•ë„ : ", clf.score(x_test, y_test))
````

````
ì •í™•ë„ :  0.9005847953216374
````

````python
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
pred=clf.predict(x_test)
test_cm=confusion_matrix(y_test, pred)
test_acc=accuracy_score(y_test, pred)
test_prc=precision_score(y_test, pred)
test_rcll=recall_score(y_test, pred)
test_f1=f1_score(y_test, pred)
print(test_cm)
print('ì •í™•ë„\t{}%'.format(round(test_acc *100,2)))
print('ì •ë°€ë„\t{}%'.format(round(test_prc *100,2)))
print('ì¬í˜„ìœ¨\t{}%'.format(round(test_rcll *100,2)))
````

````
[[103   4]
 [ 13  51]]
ì •í™•ë„	90.06%
ì •ë°€ë„	92.73%
ì¬í˜„ìœ¨	79.69%
````

````python
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(x_test)[:, 1])
roc_auc = roc_auc_score(y_test, clf.predict_proba(x_test)[:, 1])

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

print("ROC_AUC_score : ", roc_auc)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens11.png?raw=true" width="600" height="400"></p>

````python
importances = clf.feature_importances_
column_nm = pd.DataFrame(["area_mean", "texture_mean"])
feature_importances = pd.concat([column_nm,
 pd.DataFrame(importances)],
 axis=1)
feature_importances.columns = ['feature_nm', 'importances']
print(feature_importances)
````

````
     feature_nm  importances
0     area_mean     0.687528
1  texture_mean     0.312472
````

````python
f = features
xtick_label_position = list(range(len(f)))
plt.xticks(xtick_label_position, f)
plt.bar([x for x in range(len(importances))], importances)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens12.png?raw=true" width="450" height="250"></p>

## 7. Random Forest Regressor

### 7.1 Parameters

````python
class sklearn.ensemble.RandomForestRegressor(n_estimators=100, *, 
criterion=â€˜squared_errorâ€™, max_depth=None, min_samples_split=2, 
min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=â€˜autoâ€™, max_leaf_
nodes=None, min_impurity_decrease=0.0, bootstrap=True, 
oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, 
ccp_alpha=0.0, max_samples=None)[source] 
````

- classifierì™€ ë™ì¼

### 7.2 Attributes

- classifierì™€ ë™ì¼

### 7.3 Methods

- classifierì™€ ë™ì¼

### 7.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
car = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/CarPrice_Assignment.csv")
car_num = car.select_dtypes(['number'])
features = list(car_num.columns.difference(['car_ID', 'symboling', 'price']))
X=car_num[features]
y=car_num['price']
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size =0.3, 
random_state =1)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
````

````
(143, 13)
(62, 13)
(143,)
(62,)
````

````python
from sklearn.ensemble import RandomForestRegressor
reg = RandomForestRegressor()
pred=reg.fit(x_train, y_train).predict(x_test)
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error
mse = mean_squared_error(y_test, pred)
mae = mean_absolute_error(y_test, pred)
rmse = np.sqrt(mse)
acc = reg.score(x_test, y_test)
print('MSE\t{}'.format(round(mse,3)))
print('MAE\t{}'.format(round(mae,3)))
print('RMSE\t{}'.format(round(rmse,3)))
print('ACC\t{}%'.format(round(acc *100,3)))
````

````
MSE	4171875.557
MAE	1333.243
RMSE	2042.517
ACC	93.09%
````

````python
importances = reg.feature_importances_
column_nm = pd.DataFrame(features)
feature_importances = pd.concat([column_nm,
 pd.DataFrame(importances)],
 axis=1)
feature_importances.columns = ['feature_nm', 'importances']
print(feature_importances)
````

````
          feature_nm  importances
0          boreratio     0.005480
1          carheight     0.003741
2          carlength     0.009772
3           carwidth     0.017285
4            citympg     0.005848
5   compressionratio     0.003881
6         curbweight     0.183016
7         enginesize     0.663279
8         highwaympg     0.059726
9         horsepower     0.024374
10           peakrpm     0.006611
11            stroke     0.003564
12         wheelbase     0.013423
````

````python
n_features = x_train.shape[1]
importances = reg.feature_importances_
column_nm = features
plt.barh(range(n_features), importances, align ='center')
plt.yticks(np.arange(n_features), column_nm)
plt.xlabel("feature importances")
plt.ylabel("feature")
plt.ylim(-1, n_features)
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/ens/ens13.png?raw=true" width="600" height="400"></p>

