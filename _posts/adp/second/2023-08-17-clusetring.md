---
layout: single
title:  'ADP ì‹¤ê¸° 8ì¥ KNN Clustering'
toc: true
categories: [ADP]
tags: [ADP ì‹¤ê¸°]


---

ë³¸ ê²Œì‹œë¬¼ì€ KNN Clusteringì— ëŒ€í•´ ì†Œê°œí•œë‹¤.
{: .notice}

## 1. KNN Classifier

### 1.1 Concept

> ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ Kê°œì˜ ë°ì´í„°ì˜ êµ°ì§‘ì„ í™•ì¸í•˜ê³  ë¶„ë¥˜

- ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬: ë°ì´í„°ê°„ ê±°ë¦¬ ì¸¡ì •ì„ ìœ„í•´ ì‚¬ìš©
- Noise: Kì˜ ê°œìˆ˜ê°€ ì‘ì•„ ê²½ê³„ê°€ ëšœë ·í•˜ì§€ ì•Šì€ ê²½ìš° ë°œìƒí•˜ëŠ” ì˜¤ë¥˜

### 1.2 Parameters

````python
class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights=â€˜uniformâ€™, 
algorithm=â€˜autoâ€™, leaf_size=30, p=2, metric=â€˜minkowskiâ€™, metric_params=None, 
n_jobs=None) 
````

- n_neighbors: K
- weight: uniform-ëª¨ë“  ë°ì´í„°ë“¤ì´ ë™ë“±í•œ ê°€ì¤‘ì¹˜, distance-ê±°ë¦¬ìƒ ê°€ê¹Œìš´ ë°ì´í„°ê°€ í° ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§
- algorithm: ê±°ë¦¬ê³„ì‚°ì„ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜, íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©
- leaf_size: BallTree, KDTreeì— ì „ë‹¬ëœ ë¦¬í”„ì˜ í¬ê¸°
- metric: ê±°ë¦¬ ê³„ì‚° ë©”íŠ¸ë¦­

ğŸ“ **KNNì—ì„œ Tree êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ **

>  ë°ì´í„°ê°€ í´ ê²½ìš° ë§¤ë²ˆ ê±°ë¦¬ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê¸° ë¶€ë‹´ìŠ¤ëŸ¬ì›€, ë”°ë¼ì„œ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ë°ì´í„°ë¥¼ ì •ë ¬í•œë‹¤ë©´ ê°€ê¹Œìš´ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆìŒ

### 1.3 Methods

- fit(X,y)
- get_params
- kneighbors([X, n_neighbors, return_distance])
  - X: ë°ì´í„°(n_queries, n_features), ì…ë ¥í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ìˆ˜í–‰
  - n_neighbors: ì´ì›ƒì˜ ê°œìˆ˜, ì…ë ¥í•˜ì§€ ì•Šì„ ê²½ìš° ìƒì„±ìì˜ K
  - return_distance: ê±°ë¦¬ ë°˜í™˜ ì—¬ë¶€
- score(X, y[,sample_weight]): ì •í™•ë„

### 1.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
liver = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/indian_liver_patient.csv")
print(liver.Dataset.unique())
liver.head()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/knn/knn1.png?raw=true" width="800" height="200"></p>

````python
# ì„±ë³„ ë¶„ë¥˜ ì „ì²˜ë¦¬
import numpy as np
liver.Gender = np.where(liver.Gender=='Female',0,1)
liver.head()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/knn/knn2.png?raw=true" width="800" height="150"></p>

````python
# ê²°ì¸¡ì¹˜ í™•ì¸
print(liver.isna().sum())
````

````
Age                           0
Gender                        0
Total_Bilirubin               0
Direct_Bilirubin              0
Alkaline_Phosphotase          0
Alamine_Aminotransferase      0
Aspartate_Aminotransferase    0
Total_Protiens                0
Albumin                       0
Albumin_and_Globulin_Ratio    4
Dataset                       0
````

````python
# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
liver.dropna(axis=0, inplace=True)
print(liver.isna().sum())
````

````
Age                           0
Gender                        0
Total_Bilirubin               0
Direct_Bilirubin              0
Alkaline_Phosphotase          0
Alamine_Aminotransferase      0
Aspartate_Aminotransferase    0
Total_Protiens                0
Albumin                       0
Albumin_and_Globulin_Ratio    0
Dataset                       0
````

````python
# train-test ë¶„ë¦¬
from sklearn.model_selection import train_test_split
x=liver[liver.columns.difference(['Dataset'])]
y=liver['Dataset']
train_x, test_x, train_y, test_y = train_test_split(x,y,stratify=y, 
train_size=0.7, random_state=1)
print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)
````

````
(405, 10) (174, 10) (405,) (174,)
````

````python
# ëª¨ë¸ë§
from sklearn.neighbors import KNeighborsClassifier
clf=KNeighborsClassifier(n_neighbors=15, weights='uniform')
clf.fit(train_x, train_y)
````

````python
# í‰ê°€
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
pred=clf.predict(test_x)
test_cm=confusion_matrix(test_y, pred)
test_acc=accuracy_score(test_y, pred)
test_prc=precision_score(test_y, pred)
test_rcll=recall_score(test_y, pred)
test_f1=f1_score(test_y, pred)
print(test_cm)
print('\n')
print('ì •í™•ë„\t{}%'.format(round(test_acc*100,2)))
print('ì •ë°€ë„\t{}%'.format(round(test_prc*100,2)))
print('ì¬í˜„ìœ¨\t{}%'.format(round(test_rcll*100,2)))
````

````
[[106  18]
 [ 39  11]]
ì •í™•ë„	67.24%
ì •ë°€ë„	73.1%
ì¬í˜„ìœ¨	85.48%
````



## 2. KNN Regressor

### 2.1 Concept

> íŠ¹ì • Xì—ì„œ ì§ì„ ì„ ê·¸ë ¤ ê°€ê¹Œìš´ Kê°œìˆ˜ ë§Œí¼ì˜ ë°ì´í„°ë¥¼ êµ¬í•´ í‰ê· ì„ ì·¨í•˜ê³ , ì´ë“¤ì„ ì—°ê²°í•˜ì—¬ íšŒê·€ì„ ì„ ê·¸ë¦¼

- ë‹¨ìˆœ í‰ê· ì˜ ì§‘í•©ì¼ ë¿ì´ê¸°ì— ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ íšŒê·€ê³„ìˆ˜ë¥¼ í™•ì¸ í•  ìˆ˜ ì—†ìŒ
- ì£¼ë¡œ ì‹œê³„ì—´ì—ì„œ ì‚¬ìš©ë¨

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/knn/knn3.png?raw=true" width="600" height="400"></p>

### 2.2 Parameters

````python
class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, *, 
weights=â€˜uniformâ€™, algorithm=â€˜autoâ€™, leaf_size=30, p=2, 
metric=â€˜minkowskiâ€™, metric_params=None, n_jobs=None)
````

- classifierì™€ ë™ì¼

### 2.3 Methods

- classifierì™€ ë™ì¼

### 2.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import numpy as np
# ì„ì˜ì˜ ìƒ˜í”Œë°ì´í„° ìƒì„±í•˜ê¸°
np.random.seed(0)
X = np.sort(5 * np.random.rand(400, 1), axis=0)
T = np.linspace(0, 5, 500)[:, np.newaxis]
y = np.sin(X).ravel()
print(X[:10])
print(T[:10])
print(y[:10])
````

````
[[0.02347738]
 [0.05713729]
 [0.05857042]
 [0.06618429]
 [0.08164251]
 [0.08214815]
 [0.09260897]
 [0.093949  ]
 [0.09596599]
 [0.10053773]]
[[0.        ]
 [0.01002004]
 [0.02004008]
 [0.03006012]
 [0.04008016]
 [0.0501002 ]
 [0.06012024]
 [0.07014028]
 [0.08016032]
 [0.09018036]]
[0.02347522 0.05710621 0.05853694 0.06613598 0.08155185 0.08205579
 0.09247665 0.09381086 0.09581876 0.10036845]
````

````python
# íƒ€ê¹ƒë°ì´í„°ì— ë…¸ì´ì¦ˆ ì¶”ê°€í•˜ê¸°
y[::1] += 1 * (0.5 - np.random.rand(400))
````

````python
from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(X,y,train_size=0.7, 
random_state=1)
print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)
````

````
(280, 1) (120, 1) (280,) (120,)
````

````python
from sklearn.neighbors import KNeighborsRegressor
knn_uni = KNeighborsRegressor(n_neighbors=20, weights='uniform')
knn_dis = KNeighborsRegressor(n_neighbors=20, weights='distance')
knn_uni.fit(train_x, train_y)
knn_dis.fit(train_x, train_y)
````

````python
uni_pred=knn_uni.predict(test_x)
dis_pred=knn_dis.predict(test_x)
from sklearn.metrics import mean_squared_error, mean_absolute_error, 
mean_squared_error
import pandas as pd
import numpy as np
preds = [uni_pred, dis_pred]
weights = ['uniform', 'distance']
evls = ['mse', 'rmse', 'mae']
results=pd.DataFrame(index=weights,columns=evls)
for pred, nm in zip(preds, weights):
 mse = mean_squared_error(test_y, pred)
 mae = mean_absolute_error(test_y, pred)
 rmse = np.sqrt(mse)
 
 results.loc[nm]['mse']=round(mse,2)
 results.loc[nm]['rmse']=round(rmse,2)
 results.loc[nm]['mae']=round(mae,2)
results
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/knn/knn4.png?raw=true" width="300" height="100"></p>

````python
import matplotlib.pyplot as plt
plt.figure(figsize=(8,5))
for i, weights in enumerate(["uniform", "distance"]):
 knn=KNeighborsRegressor(n_neighbors=20, weights=weights)
 
 y_ = knn.fit(X, y).predict(T)
 
 plt.subplot(2, 1, i + 1)
 plt.scatter(X, y, color="darkorange", label="data")
 plt.plot(T, y_, color="navy", label="prediction")
 plt.axis("tight")
 plt.legend()
 plt.title("KNeighborsRegressor (k = %i, weights = '%s')" % (20, weights))
plt.tight_layout()
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/knn/knn5.png?raw=true" width="800" height="400"></p>
