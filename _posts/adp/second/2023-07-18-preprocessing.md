---
layout: single
title:  'ADP ì‹¤ê¸° 3ì¥ ë°ì´í„° ì „ì²˜ë¦¬'
toc: true
categories: [ADP]
tags: [ADP ì‹¤ê¸°]

---

ë³¸ ê²Œì‹œë¬¼ì€ ë°ì´í„° ì „ì²˜ë¦¬ ì¢…ë¥˜ì™€ ë°©ì‹ì— ëŒ€í•œ ë‚´ìš©ì„ ì†Œê°œí•œë‹¤.
{: .notice}

## 1. ë°ì´í„° ì „ì²˜ë¦¬ ì¢…ë¥˜

- ë°ì´í„° í´ë¦¬ë‹: **ê²°ì¸¡ì¹˜** ì²˜ë¦¬, **ì´ìƒì¹˜** í™•ì¸ ë° ì •ì œ
- ë°ì´í„° ë³€í™˜: **ìŠ¤ì¼€ì¼ë§**, ìš”ì•½
- ë°ì´í„° ì¶•ì†Œ: **ì°¨ì›ì¶•ì†Œ**, **ë¼ë²¨ë§**
- ë¶ˆê· í˜• ì²˜ë¦¬: **ì–¸ë”,ì˜¤ë²„ ìƒ˜í”Œë§**
- ë°ì´í„° ë¶„í• : **train, test split**

## 2. ì´ìƒì¹˜ í™•ì¸ ë° ì •ì œ

> ê²°ì¸¡ì¹˜ì™€ ê°’ì´ í¬ê²Œ ì°¨ì´ê°€ ë‚˜ëŠ” ë°ì´í„°ë¡œ ì¸¡ì •ì˜ ë³€ë™ì„±ì´ë‚˜ ì‹¤í—˜ì˜ ì˜¤ë¥˜, ì¥ë¹„ì˜ ì´ìƒ ë“±ì˜ ì´ë¥˜ë¡œ ë°œìƒ

### 2.1 ì´ìƒì¹˜ ì¢…ë¥˜

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/preproc/out1.png?raw=true" width="650" height="250"></p>

### 2.2 ì´ìƒì¹˜ ì œê±°: IQR ë°©ì‹

> Box plotì˜ ì´ìƒì¹˜ ê²°ì • ë°©ë²•ì„ ì´ìš©í•˜ë©°, ë°ì´í„°ë¥¼ 4ë¶„ìœ„ë¡œ ë‚˜ëˆˆ ë’¤ IQR(Q3-Q1)ì„ ì„¤ì •í•˜ê³  Q1, Q3 ë¶€í„° alpha * IQR ë§Œí¼ ë–¨ì–´ì§„ ê²ƒì„ ì´ìƒì¹˜ë¡œ ê°„ì£¼

ğŸ˜— **ë°ì´í„° ê°€ì ¸ì˜¤ê¸°**

````python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_wine
wine_load = load_wine()
wine = pd.DataFrame(wine_load.data, columns=wine_load.feature_names)
wine['Class'] = wine_load.target
wine['Class'] = wine['Class'].map({0:'class_0', 1:'class_1', 2:'class_2'})
````

ğŸ“ **Box plot ê·¸ë¦¬ê¸°**

````python
plt.boxplot(wine['color_intensity'], whis=1.5)
plt.title('color_intensity')
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/preproc/out2.png?raw=true" width="500" height="300"></p>

ğŸ“ **ì´ìƒì¹˜ì˜ ìœ„ì¹˜ì™€ ê°’ í™•ì¸**

````python
import numpy as np
def outliers_iqr(dt, col):
 quartile_1, quartile_3 = np.percentile(dt[col], [25, 75])
 iqr = quartile_3 - quartile_1
 lower_whis = quartile_1 - (iqr * 1.5)
 upper_whis = quartile_3 + (iqr * 1.5)
 outliers = dt[(dt[col] > upper_whis) | (dt[col] < lower_whis)]
 return outliers[[col]]
outliers = outliers_iqr(wine,â€˜color_intensityâ€™)
outliers
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/preproc/out3.png?raw=true" width="150" height="150"></p>

### 2.3 ì´ìƒì¹˜ ì •ì œ

ğŸ“ **ì´ìƒì¹˜ ì œê±°**

````python
drop_outliers = wine.drop(index=outliers.index)
# ì´ìƒì¹˜ë¥¼ ì‚­ì œí•˜ê¸° ì „
print('Original :', wine.shape)
# ì´ìƒì¹˜ ì‚­ì œ í›„
print('Drop outliers :', drop_outliers.shape)
````

````
Original : (178, 14)
Drop outliers : (174, 14)
````

ğŸ“**ì´ìƒì¹˜ ëŒ€ì²´**

````python
# ì´ìƒì¹˜ë¥¼ NaNìœ¼ë¡œ ë³€ê²½
wine.loc[outliers.index, 'color_intensity'] = np.nan
wine['color_intensity'].fillna(wine['color_intensity'].mean())
wine.loc[outliers.index, 'color_intensity']
````

````
151   NaN
158   NaN
159   NaN
166   NaN
````

````python
wine['color_intensity'] = wine['color_intensity'].fillna(wine['color_intensity'].mean())
wine.loc[outliers.index, 'color_intensity']
````

````
151    4.908678
158    4.908678
159    4.908678
166    4.908678
````

## 3. ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬

> ë¶€ë¥˜, ë²”ìœ„ ê·¸ë¦¬ê³  ì„œì—´ ë“±ìœ¼ë¡œ êµ¬ë¶„ë˜ëŠ” ë³€ìˆ˜ë¡œ ì§ˆì ë³€ìˆ˜ë¼ê³ ë„ í•˜ë©°, objectë‚˜ category í˜•ìœ¼ë¡œ ì €ì¥ë˜ê³  ìˆ˜í•™ì  ì˜ë¯¸ê°€ ì—†ìŒ

### 3.1 Label encoding

> ì•ŒíŒŒë²³ ìˆœì„œë¡œ ìˆ«ìë¥¼ í• ë‹¹í•´ ì£¼ê¸°ì— ë­í¬ëœ ìˆ«ì ì •ë³´ê°€ ì˜ë¯¸ë¥¼ ì˜ëª» ë‚´í¬í•˜ëŠ”ì§€ í™•ì¸ í•„ìš”

ğŸ“**Label encoder ì˜ˆì‹œ**

````python
from sklearn.preprocessing import LabelEncoder

item_label = ['b','a','c','d','a','b']
encoder = LabelEncoder()
encoder.fit(item_label)
test_label = ['a','a','b','d','c']
digit_label = encoder.transform(test_label)

print(encoder.classes_)
print(digit_label)
print(encoder.inverse_transform(digit_label))
````

````
['a' 'b' 'c' 'd']
[0 0 1 3 2]
['a' 'a' 'b' 'd' 'c']
````

### 3.2 One-hot encoding

> ë”ë¯¸ ë³€ìˆ˜ë¥¼ í†µí•´ category ë³€ìˆ˜ë¥¼ ì´ì§„í™” ì‹œí‚´

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
from sklearn.datasets import load_iris
iris = load_iris()
iris = pd.DataFrame(iris.data, columns=iris.feature_names)
iris['Class'] = load_iris().target
iris['Class'] = iris['Class'].map({0:'Setosa', 1:'Versicolour', 2:'Virginica'})
````

ğŸ“ **dummy ë³€ìˆ˜ë¥¼ í†µí•œ one-hot encoder ì˜ˆì‹œ**

````python
iris_dummy = pd.get_dummies(iris, columns = ['Class'])
iris_dummy
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/preproc/oh1.png?raw=true" width="600" height="350"></p>

ğŸ“**sklearnì„ í™œìš©í•œ one-hot encoder**

````python
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

data_dic = {'label':['Apple', 'Banana', 'Pear', 'Apple', 'Mango']}
df = pd.DataFrame(data_dic)

oh = OneHotEncoder(sparse_output=False)
oh.fit(df)
sk_oh_encoded = oh.transform(df)
pd.DataFrame(sk_oh_encoded.astype('int'), columns=oh.get_feature_names_out())
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/preproc/oh2.png?raw=true" width="350" height="200"></p>

````python
oh.inverse_transform(pd.DataFrame([1, 0, 0, 0]).T)
````

````
array([['Apple']], dtype=object)
````

ğŸ‘€ **Dummy variable trap**

> One-hot encodingì„ í†µí•´ ìƒì„±ëœ ë³€ìˆ˜ê°€ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ê°„ì˜ ìƒê´€ì„±ì´ ìˆëŠ” ê²ƒ â˜ **multicollinearity**<br>ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ì„œëŠ” dummy variables ì¤‘ í•˜ë‚˜ë¥¼ ë²„ë ¤ì•¼ í•¨ by using VIF

### 3.3 Label vs One-hot ì„ íƒ ê¸°ì¤€

- Label: ìˆœì„œì˜ ì˜ë¯¸ê°€ ìˆì„ ë•Œ, ê³ ìœ ê°’ì˜ ê°œìˆ˜ê°€ ë§ì•„ one-hot ì‹œ ë©”ëª¨ë¦¬ ì´ìŠˆê°€ ìˆì„ ë•Œ
- One-hot: ìˆœì„œê°€ ì—†ì„ ë•Œ, ê³ ìœ ê°’ì˜ ê°œìˆ˜ê°€ ë§ì§€ ì•Šì„ ë•Œ

## 4. ë°ì´í„° ë¶„í• 

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
from sklearn.datasets import load_iris
iris = load_iris()
iris = pd.DataFrame(iris.data, columns=iris.feature_names)
iris['Class'] = load_iris().target
iris['Class'] = iris['Class'].map({0:'Setosa', 1:'Versicolour', 2:'Virginica'})
````

ğŸ“**train test split**

````python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris.drop(columns='Class'), iris['Class'], test_size = 0.2, random_state=1004)
print('X_train :', X_train.shape, ' X_test :', X_test.shape)
print('y_train :', y_train.shape, ' y_test :', y_test.shape)
````

````
X_train : (120, 4)  X_test : (30, 4)
y_train : (120,)  y_test : (30,)
````

ğŸ‘€ **Parameter**

- random state: ê°™ì€ ìˆ˜ë¥¼ ì§€ì •í•˜ë©´ ê°™ì€ ê²°ê³¼ê°€ ë„ì¶œë¨
- stratify: ì§€ì •í•˜ì§€ ì•Šì„ ì‹œ, class ë¹„ìœ¨ì´ ì›ë³¸ê³¼ ì°¨ì´ê°€ ë‚˜ê²Œ ë¨ â˜ ì§€ì •í•˜ë©´ ì¸µí™”ì¶”ì¶œë²•ìœ¼ë¡œ ëª¨ë“  í´ë˜ìŠ¤ê°€ ë™ì¼í•˜ê²Œ split

ğŸ“**ì¸µí™” ì¶”ì¶œë²•**

````python
X_train, X_test, y_train, y_test = train_test_split(iris.drop(columns='Class'), iris['Class'], test_size = 0.2, stratify =iris['Class'])
print('X_train :', X_train.shape, '\tX_test :', X_test.shape)
print('y_train :', y_train.shape, '\ty_test :', y_test.shape)
````

````
X_train : (120, 4) 	X_test : (30, 4)
y_train : (120,) 	y_test : (30,)
````

````python
y_train.value_counts()
````

````
Class
Versicolour    40
Setosa         40
Virginica      40
Name: count, dtype: int64
````

## 5. ë°ì´í„° ìŠ¤ì¼€ì¼ë§

### 5.1 Standard scaler

- í‰ê· ì´ 0 ë¶„ì‚°ì´ 1ì¸ ì •ê·œ ë¶„í¬ë¡œ ìŠ¤ì¼€ì¼ë§
- ìµœì†Œê°’ì˜ í¬ê¸°ë¥¼ ì œí•œí•˜ì§€ ì•Šì•„ **ì´ìƒì¹˜ì— ë¯¼ê°** â˜ ì´ìƒì¹˜ ì²˜ë¦¬ í•„ìˆ˜
-  **ë¶„ë¥˜ë¶„ì„ì—ì„œ ìœ ìš©**

ğŸ“**standard scaler ì˜ˆì‹œ**

````python
from sklearn.preprocessing import StandardScaler
StdScaler = StandardScaler()
# Train ë°ì´í„°ì˜ fittingê³¼ ìŠ¤ì¼€ì¼ë§
StdScaler.fit(X_train)
X_train_sc = StdScaler.transform(X_train)
# Test ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ë§
X_test_sc = StdScaler.transform(X_test)
# ê²°ê³¼ í™•ì¸
print('\t\t(min, max) (mean, std)')
print ('Train_scaled (%.2f, %.2f) (%.2f, %.2f)'%(X_train_sc.min(), X_train_sc.max(), X_train_sc.mean(), X_train_sc.std()))
print ('Test_scaled (%.2f, %.2f) (%.2f, %.2f)'%(X_test_sc.min(), X_test_sc.max(), X_test_sc.mean(), X_test_sc.std()))
````

````
		(min, max) (mean, std)
Train_scaled (-2.00, 3.13) (0.00, 1.00)
Test_scaled (-2.46, 2.34) (0.03, 1.06)
````

### 5.2 Min-max sclaer

- 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§
- **ì´ìƒì¹˜ì— ë¯¼ê°** â˜ ì´ìƒì¹˜ ì²˜ë¦¬ í•„ìˆ˜
- **íšŒê·€ë¶„ì„ì— ìœ ìš©**

ğŸ“**min-max scaler ì˜ˆì‹œ**

````python
from sklearn.preprocessing import MinMaxScaler
MmScaler = MinMaxScaler()
# Train ë°ì´í„°ì˜ fittingê³¼ ìŠ¤ì¼€ì¼ë§
MmScaler.fit(X_train)
X_train_sc = MmScaler.transform(X_train)
# Test ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ë§
X_test_sc = MmScaler.transform(X_test)
print('\t\t(min, max) (mean, std)')
print ('Train_scaled (%.2f, %.2f) (%.2f, %.2f)'%(X_train_sc.min(), X_train_sc.max(), X_train_sc.mean(), X_train_sc.std()))
print ('Test_scaled (%.2f, %.2f) (%.2f, %.2f)'%(X_test_sc.min(), X_test_sc.max(), X_test_sc.mean(), X_test_sc.std()))
````

````
		(min, max) (mean, std)
Train_scaled (0.00, 1.00) (0.43, 0.26)
Test_scaled (-0.09, 1.00) (0.44, 0.28)
````

### 5.3 Max abs scaler

- ëª¨ë“  ê°’ì´ -1ê³¼ 1 ì‚¬ì´ì— í‘œí˜„ â˜ ë°ì´í„°ê°€ ì–‘ìˆ˜ì¸ ê²½ìš° min-maxì™€ ë™ì¼
- **ì´ìƒì¹˜ì— ë¯¼ê°** â˜ ì´ìƒì¹˜ ì²˜ë¦¬ í•„ìˆ˜
- **íšŒê·€ë¶„ì„ì— ìœ ìš©**

ğŸ“**max abs scaler ì˜ˆì‹œ**

````python
from sklearn.preprocessing import MaxAbsScaler
MaScaler = MaxAbsScaler()
# Train ë°ì´í„°ì˜ fittingê³¼ ìŠ¤ì¼€ì¼ë§
MaScaler.fit(X_train)
X_train_sc = MaScaler.transform(X_train)
# Test ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ë§
X_test_sc = MaScaler.transform(X_test)
print('\t\t(min, max) (mean, std)')
print ('Train_scaled (%.2f, %.2f) (%.2f, %.2f)'%(X_train_sc.min(), X_train_sc.max(), X_train_sc.mean(), X_train_sc.std()))
print ('Test_scaled (%.2f, %.2f) (%.2f, %.2f)'%(X_test_sc.min(), X_test_sc.max(), X_test_sc.mean(), X_test_sc.std()))
````

````
		(min, max) (mean, std)
Train_scaled (0.04, 1.00) (0.61, 0.24)
Test_scaled (0.08, 1.00) (0.62, 0.24)
````

### 5.4 Robust scaler

- í‰ê· ê³¼ ë¶„ì‚° ëŒ€ì‹  ì¤‘ì•™ê°’ê³¼ ì‚¬ë¶„ìœ„ë¥¼ í™œìš©
- IQRì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ ì˜í–¥ì„ ìµœì†Œí™”

ğŸ“**robust scaler ì˜ˆì‹œ**

````python
from sklearn.preprocessing import RobustScaler
RuScaler = RobustScaler()
# Train ë°ì´í„°ì˜ fittingê³¼ ìŠ¤ì¼€ì¼ë§
RuScaler.fit(X_train)
X_train_sc = RuScaler.transform(X_train)
# Test ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ë§
X_test_sc = RuScaler.transform(X_test)
print('\t\t(min, max) (mean, std)')
print ('Train_scaled (%.2f, %.2f) (%.2f, %.2f)'%(X_train_sc.min(), X_train_sc.max(), X_train_sc.mean(), X_train_sc.std()))
print ('Test_scaled (%.2f, %.2f) (%.2f, %.2f)'%(X_test_sc.min(), X_test_sc.max(), X_test_sc.mean(), X_test_sc.std()))
````

````
		(min, max) (mean, std)
Train_scaled (-1.60, 2.80) (-0.02, 0.64)
Test_scaled (-2.00, 1.60) (-0.01, 0.68)
````

## 6. ì°¨ì› ì¶•ì†Œ

ğŸ‘€ **ì°¨ì›ì˜ ì €ì£¼**

> ë°ì´í„°ì˜ ì°¨ì›ì´ ì¦ê°€(=ì„¤ëª…ë³€ìˆ˜ê°€ ëŠ˜ì–´ë‚¨)ì— ë”°ë¼ ë°ì´í„° ê°„ì˜ ê±°ë¦¬ê°€ ë©€ì–´ì§€ê³ , ì „ì²´ ì˜ì—­ì—ì„œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì˜ ë¹„ìœ¨ì´ ì¤„ì–´ë“¦ <br>â˜ But, ì°¨ì›ì´ ëŠ˜ì–´ë‚˜ëŠ”ê²Œ ë¬´ì¡°ê±´ ë‚˜ìœê±´ ì•„ë‹˜

### 6.1 ì„¤ëª…ë³€ìˆ˜ ì„ íƒ

- EDAì—ì„œ ìƒê´€ê´€ê³„ê°€ ë†’ì•˜ë˜ ì„¤ëª…ë³€ìˆ˜ë§Œì„ ì‚¬ìš©
- ê³ ì°¨ì›ì ì¸ ìƒê´€ê´€ê³„ëŠ” ê³ ë ¤í•˜ê¸° ì–´ë ¤ì›€

### 6.2 ì£¼ì„±ë¶„ ë¶„ì„

- ê¸°ì¡´ ì»¬ëŸ¼ì„ ì €ì°¨ì›ì˜ ì´ˆí‰ë©´ì— íˆ¬ì˜
- ë¶„ì‚°ì´ ê°€ì¥ ë†’ì€ ì¶•(=ì„¤ëª…ì„ ê°€ì¥ ì˜ í•˜ëŠ” ì¶•)ì„ ì°¾ì•„ ìƒˆë¡œìš´ ì£¼ì„±ë¶„ìœ¼ë¡œ ê²°ì •
- ìˆ˜ì¹˜í˜• ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ì§„í–‰
- ìŠ¤ì¼€ì¼ ì°¨ì´ê°€ ì£¼ì„±ë¶„ ì„ ì •ì— ì˜í–¥ì„ ì£¼ëŠ”ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì´ìƒì¹˜ ì œê±° ë° ìŠ¤ì¼€ì¼ë§

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
from sklearn.datasets import load_iris
iris = load_iris()
iris = pd.DataFrame(iris.data, columns=iris.feature_names)
iris['Class'] = load_iris().target
iris['Class'] = iris['Class'].map({0:'Setosa', 1:'Versicolour', 2:'Virginica'})
````

ğŸ“**PCA ì˜ˆì‹œ**

````python
# ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ ì¶”ì¶œ
features = ['ìˆ˜ì¹˜í˜• ë³€ìˆ˜1', 'ìˆ˜ì¹˜í˜• ë³€ìˆ˜2']
x = iris.drop(columns = 'Class')
# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì •ê·œí™”
from sklearn.preprocessing import StandardScaler
x = StandardScaler().fit_transform(x)
pd.DataFrame(x).head()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/preproc/pca1.png?raw=true" width="300" height="150"></p>

````python
from sklearn.decomposition import PCA
pca = PCA( n_components = 4) 
pca_fit = pca.fit(x)
print('ê³ ìœ  ê°’ : ', pca.singular_values_)
print('ë¶„ì‚° ì„¤ëª…ë ¥: ', pca.explained_variance_ratio_)
````

````
ê³ ìœ  ê°’ :  [20.92306556 11.7091661   4.69185798  1.76273239]
ë¶„ì‚° ì„¤ëª…ë ¥:  [0.72962445 0.22850762 0.03668922 0.00517871]
````

ğŸ‘€ **ê³ ìœ ê°’**

> ë°ì´í„°ì—ì„œ í•´ë‹¹í•˜ëŠ” ì£¼ì„±ë¶„ ê°œìˆ˜ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¶„ì‚°ì˜ ë¹„ìœ¨

ğŸ‘€ **ë¶„ì‚° ì„¤ëª…ë ¥**

> ì „ì²´ ë°ì´í„°ì—ì„œ ê° ì£¼ì„±ë¶„ì´ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¶„ì‚°ì˜ ë¹„ìœ¨

â˜€ï¸ **ê³ ìœ ê°’ê³¼ ë¶„ì‚° ì„¤ëª…ë ¥ì„ í†µí•´ ì£¼ì„±ë¶„ì˜ ê°œìˆ˜ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ**

ğŸ“ **Scree plotìœ¼ë¡œ ì£¼ì„±ë¶„ ê°œìˆ˜ êµ¬í•˜ê¸°**

ğŸ‘€ **scree plot**

> ì£¼ì„±ë¶„ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¶„ì‚°ì˜ ì •ë„ë¥¼ ì ìœ¼ë¡œ í‘œì‹œí•˜ê³  ê° ì ë“¤ì„ ì´ì€ ì„  â˜ **ê¸°ìš¸ê¸°ê°€ ê¸‰ê²©íˆ ê°ì†Œí•˜ëŠ” ì§€ì  ì§ì „ê¹Œì§€ë¥¼ ì£¼ì„±ë¶„ ì„ íƒ**

````python
import matplotlib.pyplot as plt

plt.title('Scree Plot')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.plot(pca.explained_variance_ratio_ , 'o-')
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/preproc/pca2.png?raw=true" width="500" height="300"></p>

````python
# PCA ê°ì²´ ìƒì„± (ì£¼ì„±ë¶„ ê°œìˆ˜ 2ê°œ ìƒì„±)
pca = PCA(n_components = 2)
# 2ê°œì˜ ì£¼ì„±ë¶„ì„ ê°€ì§„ ë°ì´í„°ë¡œ ë³€í™˜
principalComponents = pca.fit_transform(x)
principal_iris = pd.DataFrame (data = principalComponents, columns =['pc1', 'pc2']) 
principal_iris.head()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/preproc/pca3.png?raw=true" width="200" height="150"></p>

````python
import matplotlib.pyplot as plt
import seaborn as sns
plt.title('2 component PCA' )
sns.scatterplot (x = 'pc1', y = 'pc2', hue = iris.Class, data = principal_iris)
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/preproc/pca4.png?raw=true" width="500" height="300"></p>

- ì‚°í¬ë„ë¥¼ í™•ì¸í•˜ë©´ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ê·¸ë¦° ì‚°í¬ë„ë³´ë‹¤ ì¢…ì†ë³€ìˆ˜ë¥¼ ë” ì˜ ì„¤ëª…í•˜ëŠ” ì‚°í¬ë„ í™•ì¸ ê°€ëŠ¥

## 7. ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œ ì²˜ë¦¬

### 7.1 ì–¸ë” ìƒ˜í”Œë§

> ì‘ì€ í´ë˜ìŠ¤ì— ë§ì¶”ì–´ ì „ì²´ ë°ì´í„°ë¥¼ ê°ì†Œí•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ ë¶ˆê· í˜•ì€ í•´ê²°í•  ìˆ˜ ìˆìœ¼ë‚˜ í•™ìŠµ ì„±ëŠ¥ì„ ë–¨ì–´ëœ¨ë¦´ ìˆ˜ ìˆìŒ

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
x, y = make_classification(n_samples=2000, n_features=6, weights=[0.95], flip_y=0)
print(Counter(y))
````

````
Counter({0: 1900, 1: 100})
````

ğŸ“ **Random under sampling**

````python
undersample = RandomUnderSampler(sampling_strategy='majority')
x_under, y_under = undersample.fit_resample(x, y)
print(Counter(y_under))
````

````
Counter({0: 100, 1: 100})
````

````python
undersample = RandomUnderSampler(sampling_strategy=0.5) 
x_under2, y_under2 = undersample.fit_resample(x, y)
print(Counter(y_under2))
````

````
Counter({0: 200, 1: 100})
````

### 7.2 ì˜¤ë²„ ìƒ˜í”Œë§

> ë‹¤ìˆ˜ ë ˆì´ë¸”ì— ë§ì¶° ì†Œìˆ˜ ë ˆì´ë¸”ì˜ ë°ì´í„°ë¥¼ ì¦ì‹ì‹œí‚¤ëŠ” ë°©ë²•ìœ¼ë¡œ ì–¸ë” ìƒ˜í”Œë§ë³´ë‹¤ ë³´í†µ ìœ ìš©í•¨

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
from imblearn.over_sampling import RandomOverSampler
oversample = RandomOverSampler(sampling_strategy=0.5) 
x_over, y_over = oversample.fit_resample(x, y)
print(Counter(y_over))
````

````
Counter({0: 1900, 1: 950})
````

ğŸ“ **Random over sampling**

````python
oversample = RandomOverSampler(sampling_strategy='minority')
x_over, y_over = oversample.fit_resample(x, y)
print(Counter(y_over))
````

````
Counter({0: 1900, 1: 1900})
````

ğŸ“ **SMOTE**

> ì†Œìˆ˜ ë ˆì´ë¸”ì„ ì§€ë‹Œ ë°ì´í„° ì„¸íŠ¸ì˜ ê´€ì¸¡ ê°’ì— ëŒ€í•œ KNNì„ ì°¾ê³ , ê´€ì¸¡ ê°’ê³¼ ì´ì›ƒìœ¼ë¡œ ì„ íƒëœ ê°’ ì‚¬ì´ì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì—¬ ë°ì´í„° ì¦ì‹

````python
from imblearn.over_sampling import SMOTE
smote_sample = SMOTE(sampling_strategy='minority') 
x_sm, y_sm = smote_sample.fit_resample(x, y)
print(Counter(y_sm))
````

````
Counter({0: 1900, 1: 1900})
````







