---
layout: single
title:  'ADP ì‹¤ê¸° 5ì¥ Linear Regression'
toc: true
categories: [ADP]
tags: [ADP ì‹¤ê¸°]

---

ë³¸ ê²Œì‹œë¬¼ì€ Linear Regressionì— ëŒ€í•´ ì†Œê°œí•œë‹¤.
{: .notice}

## 1. Simple Linear Regression

### 1.1 Concept

> ì…ë ¥ íŠ¹ì„±ì— ëŒ€í•œ ì„ í˜• í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì˜ˆì¸¡í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë…ë¦½ë³€ìˆ˜ê°€ í•˜ë‚˜ì¸ ê²½ìš° ë°ì´í„°ì˜ íŠ¹ì§•ì„ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ì§ì„ ì„ í•™ìŠµí•¨

- í†µê³„ì  ë°©ì‹ì˜ ì„ í˜•íšŒê·€ëŠ” ì •ê·œ ë°©ì •ì‹ì„ ì‚¬ìš©
- ë¨¸ì‹ ëŸ¬ë‹ ë°©ì‹ì˜ ì„ í˜•íšŒê·€ëŠ” ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©

ğŸ“ **ì •ê·œ ë°©ì •ì‹**<br><br>
$$
\hat{\beta} = (X^TX)^{-1}X^Ty
$$

- [ì¦ëª…](https://blog.naver.com/PostView.nhn?blogId=muke0822&logNo=222053957661)

### 1.2 Parameters

````python
# default
sklearn.linear_model.LinearRegression( * , fit_intercept = True , 
                                      normalize = False , n_jobs = None)
````

- fit_intercept: ì ˆí¸ì„ ê³„ì‚°í• ì§€ì— ëŒ€í•œ ì—¬ë¶€, Falseì¼ ê²½ìš° ì›ì ì„ ì§€ë‚˜ëŠ” ì§ì„  ëª¨ë¸ ìƒì„±
- normalize: Xì— ëŒ€í•œ ì •ê·œí™”ë¥¼ ì§„í–‰í• ì§€ì— ëŒ€í•œ ì—¬ë¶€, Trueì¼ ê²½ìš° ë°ì´í„°ë¥¼ $(X-\bar{X})/\sqrt{\sum{x}^2}$ë¡œ ì •ê·œí™”í•¨, Falseì¼ ê²½ìš° í•™ìŠµì „ ì •ê·œí™” í•„ìš”
- n_jobs: ë³´ìœ í•œ cpuë¡œ ë³‘ë ¬í•™ìŠµ, -1ì¼ì‹œ ìµœëŒ€ cpu ì‚¬ìš©

### 1.3 Attribute

- coef_: ì„ í˜• íšŒê·€ ë¬¸ì œì— ëŒ€í•œ ì¶”ì •ëœ ê³„ìˆ˜
- rank_: ë‹¤ì¤‘ê³µì„ ì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•´ íšŒê·€ëª¨ë¸ì— ì‚¬ìš©ëœ ë…ë¦½ ë³€ìˆ˜ë“¤ì˜ ì„ í˜•ë…ë¦½ì˜ ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì†ì„±, ë…ë¦½ ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ê°€ ì–¼ë§ˆë‚˜ ê°•í•œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¡œ ì‚¬ìš©, ë­í¬ê°€ ë…ë¦½ ë³€ìˆ˜ì˜ ê°œìˆ˜ì™€ ë™ì¼í•˜ê±°ë‚˜ ê±°ì˜ ìœ ì‚¬í•œ ê²½ìš°ê°€ ë°”ëŒì§í•˜ë©° ë­í¬ê°€ ë…ë¦½ ë³€ìˆ˜ì˜ ê°œìˆ˜ë³´ë‹¤ ì‘ë‹¤ë©´, ë‹¤ì¤‘ê³µì„ ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë³€ìˆ˜ ì„ íƒ, ë³€ìˆ˜ ë³€í™˜ ë“±ì˜ ì „ì²˜ë¦¬ ë°©ë²•ì„ ê³ ë ¤í•´ì•¼ í•¨
- singular_: ë°ì´í„°ì— ëŒ€í•œ íŠ¹ì´ê°’ í–‰ë ¬ë¡œ, í–‰ë ¬ì˜ ë°ì´í„° ì¤‘ í•˜ë‚˜ê°€ 0ì— ê°€ê¹Œìš´ ì‘ì€ ê°’ì´ë¼ë©´, í•´ë‹¹ ë…ë¦½ ë³€ìˆ˜ëŠ” ë‹¤ë¥¸ ë…ë¦½ ë³€ìˆ˜ë“¤ê³¼ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆì–´ ë‹¤ì¤‘ê³µì„±ì„±ì„ ì´ˆë˜í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ, rank ê³„ì‚°ì— ì‚¬ìš©ë¨
- intercept_: ì ˆí¸

### 1.4 Method

- fit(X, y[, sample_weight]): ëª¨ë¸ í•™ìŠµ
- get_params([deep]): ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì ¸ì˜´
- predict(X): ì˜ˆì¸¡
- score(X, y[, sample_weight]): ëª¨ë¸ í•™ìŠµ ê²°ê³¼ë¡œ R-sqaureë¥¼ ê³„ì‚°

ğŸ“ **sample_weight**

> ì¼ë°˜ì ì¸ íšŒê·€ë¶„ì„ì€ ëª¨ë“  ë°ì´í„°ê°€ ë™ì¼í•œ ì¤‘ìš”ì„±ì„ ê°–ê³  ì—°ì‚°ë˜ë‚˜, sample_weightë¥¼ ì‚¬ìš©í•´ ê° ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŒ

- ë¶ˆê· í˜• ë°ì´í„°: ì†Œìˆ˜ì˜ í´ë˜ìŠ¤ì— ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬
- ì •í™•ì„± ì¡°ì ˆ: ì‹ ë¢°ì„±ì´ ë†’ì€ ë°ì´í„°ì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
- ì˜¤ë¥˜ì— ë¯¼ê°í•œ ë°ì´í„°: ì˜¤ë¥˜ê°€ í° ë°ì´í„°ì— ë‚®ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•˜ê²Œ í•™ìŠµ

### 1.5 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
data = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/insurance.csv")
data.head()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr1.png?raw=true" width="650" height="200"></p>

````python
# ageëŒ€ chargesì˜ ê´€ê³„ë¥¼ ì‹œê°í™”
x=data['age']
y=data['charges']
plt.figure(figsize=(10,5))
plt.scatter(x,y)
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr2.png?raw=true" width="650" height="350"></p>

````python
# X, y ì„ ì • ë° ì°¨ì› ë§ì¶”ê¸°
x=np.array(data['age'])
y=np.array(data['charges'])
x=x.reshape(-1 ,1) 
y=y.reshape(-1 ,1)

print(x.shape, y.shape)
````

````
(1338, 1) (1338, 1)
````

````python
# ëª¨ë¸ í•™ìŠµ
lr = LinearRegression()
lr.fit(x,y)

# í‰ê°€
print("ì ˆí¸", lr.intercept_, "ê³„ìˆ˜", lr.coef_)
print("R-square", lr.score(x,y))
````

````
ì ˆí¸ [3165.88500606] ê³„ìˆ˜ [[257.72261867]]
R-square 0.08940589967885804
````

````python
# íšŒê·€ì„ ì„ ê·¸ë ¤ í™•ì¸
plt.figure(figsize=(10,5))
plt.plot(x, lr.predict(x), color='red', linewidth=2)
plt.plot(x, y, "b.")
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr3.png?raw=true" width="650" height="350"></p>

## 2. Gradinet Descent

### 2.1 Concept

> ì§€ì •í•œ loss í•¨ìˆ˜ë¥¼ ìµœì†Œë¡œ í•˜ëŠ” ë…ë¦½ ë³€ìˆ˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë°˜ë³µì  ì—°ì‚°ì„ í†µí•´ ì°¾ëŠ” ë°©ì‹

### 2.2 Gradinet Descent ì¢…ë¥˜

- batch: iteration ë§ˆë‹¤ ëª¨ë“  ë°ì´í„° ì„¸íŠ¸ ì‚¬ìš©í•˜ì—¬ gradient ê³„ì‚° ë° weight update
- sthocastic: í•œê°œì˜ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ ì„ íƒí•˜ì—¬ gradient ê³„ì‚° ë° weight update
- mini batch: ê° step ë§ˆë‹¤ ì¼ì •í•œ batch_sizeë¡œ gradient ê³„ì‚° ë° weight update

ğŸ“ **Terminology**

- Epoch: ì „ì²´ ë°ì´í„° ì…‹ì„ í•œë²ˆ ëª¨ë¸ì— í†µê³¼ ì‹œí‚¤ëŠ” ê²ƒ, 1 Epoch ë§ˆë‹¤ ëª¨ë¸ì´ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ í•œ ë²ˆ í•™ìŠµì„ ìˆ˜í–‰í•œ ê²ƒ
- Step: ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ í•œ ë²ˆ ì—…ë°ì´íŠ¸í•˜ëŠ”ë° í•„ìš”í•œ ë°˜ë³µ íšŸìˆ˜, epochëŠ” ì—¬ëŸ¬ê°œì˜ stepìœ¼ë¡œ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆìŒ
- Iteration: 1 Epcoh ë™ì•ˆ ìˆ˜í–‰ë˜ëŠ” stepì˜ ì´ íšŸìˆ˜, ì´ iterationì€ epoch * step
- batch: í•œë²ˆì˜ stepì—ì„œ ì²˜ë¦¬í•˜ëŠ” ë°ì´í„°ì˜ ê°œìˆ˜

### 2.3 Parameters

````python
class sklearn.linear_model.SGDRegressor(loss=â€˜squared_errorâ€™, *, penalty=â€˜l2â€™, alpha=0.0001, 
                                        l1_ratio=0.15, fit_intercept=True, max_iter=1000, 
                                        tol=0.001, shuffle=True, verbose=0, 
                                        epsilon=0.1, random_state=None, learning_rate=â€˜invscalingâ€™, 
                                        eta0=0.01, power_t=0.25, early_stopping=False, 
                                        validation_fraction=0.1, n_iter_no_change=5, warm_start=False, 
                                        average=False) 
````

- loss: loss functionë¡œ ì¢…ë¥˜ë¡œëŠ” 'squared_error', 'huber', 'epsilon_insentive', 'squared_epslion_insentive'ê°€ ìˆìŒ
- penalty: loss í•¨ìˆ˜ì— panelty(=ì •ê·œí™”)ë¥¼ ì ìš©í•´ ê³¼ì í•© ë° ì•ˆì •ì  ìˆ˜ë ´ 'l1', 'l2', 'elasticnet'ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
- alpha: ì •ê·œí™”ì˜ ê°•ë„
- l1_ratio: 0=L2 panelty, 1=L1 panelty, 0~1=elastic net
- fit_intercept: ì ˆí¸ ì¶”ì • ì—¬ë¶€
- max_iter: max epochì— ëŒ€í•œ ìµœëŒ€ pass
- shuffle: ê° Epoch ì´í›„ í•™ìŠµ ë°ì´í„°ë¥¼ ì„ì„ì§€ ì—¬ë¶€
- eta0: learning rateì˜ ì´ˆê¸°ê°’ì„ ì„¤ì •
- early_stopping: validation epochê°€ í•´ë‹¹ ë³€ìˆ˜ë¡œ ì§€ì •í•œ epoch ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šì„ ê²½ìš° ì¤‘ë‹¨
- validation_fraction: early stopì„ ìœ„í•´ í•™ìŠµ ë°ì´í„° ì¤‘ ì¼ì • ë¶€ë¶„ì„ validation setìœ¼ë¡œ ì§€ì •
- average: ëª¨ë“  ì—…ë°ì´íŠ¸ì—ì„œ í‰ê·  ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ê³  ì´ë¥¼ coefì— ì €ì¥

### 2.4 Attribute

- coef_: ê°€ì¤‘ì¹˜
- intercept_: ì ˆí¸
- n_iter: ì¤‘ì§€ê¹Œì§€ ê±¸ë¦° ì´ ë°˜ë³µ íšŸìˆ˜
- t_: ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ íšŸìˆ˜

### 2.5 Method

- fit(X,y): í•™ìŠµ
- get_params([deep]): ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì ¸ì˜´
- partial_fit(X, y): ì£¼ì–´ì§„ ìƒ˜í”Œì— ëŒ€í•´ í•œ epochì˜ í™•ë¥ ì  ê²½ì‚¬í•˜ê°• ìˆ˜í–‰
- predict(X): ì˜ˆì¸¡
- score(X, y[,sample_weight]): ê²°ì •ê³„ìˆ˜ ë°˜í™˜

### 2.6 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
import numpy as np
from sklearn.linear_model import SGDRegressor
data= pd.read_csv('https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/insurance.csv')
x=np.array(data['age'])
y=np.array(data['charges'])
x=x.reshape(1338 ,1) 
y=y.reshape(1338 ,1)

print(x.shape, y.shape)
````

````
(1338, 1) (1338, 1)
````

````python
# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
sgd_reg =SGDRegressor(max_iter=1000, random_state=45)
sgd_reg.fit(x,y.ravel())
print('SGD íšŒê·€ ëª¨ë¸ ê²°ê³¼')
print('ì ˆí¸', sgd_reg.intercept_, 'ê³„ìˆ˜', sgd_reg.coef_)
````

````
SGD íšŒê·€ ëª¨ë¸ ê²°ê³¼
ì ˆí¸ [6931.05771231] ê³„ìˆ˜ [90.62149214]
````

````python
# ì˜ˆì¸¡
x_new=[[19],[64]]
y_hat=sgd_reg.predict(x_new)
print(y_hat)
````

````
[ 8652.866063   12730.83320937]
````

## 3. Polynomial Regression

### 3.1 Concept

> ë°ì´í„°ê°€ ë‹¨ìˆœí•œ ì„ í˜•ì´ ì•„ë‹Œ ë¹„ì„ í˜•ì˜ í˜•íƒœë¥¼ ê°–ê³  ìˆì„ ë•Œ, ê° ë³€ìˆ˜ì˜ ê±°ë“­ì œê³±ì„ ìƒˆë¡œìš´ ë³€ìˆ˜ë¡œ ì‚¬ìš©í•˜ë©´ ì„ í˜• ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ

ğŸ“ **ë‹¤í•­ ë³€ìˆ˜ì˜ ìƒì„±**

- scikit-learnì˜ `PolynomialFeatures(degree=d)`ë¥¼ í™œìš©í•´ ë³€ìˆ˜ì˜ ì œê³±í•­ì„ featureë¡œ ì¶”ê°€í•  ìˆ˜ ìˆìŒ
- ì´ë•Œ ê° í•­ì˜ êµì°¨í•­ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŒ

### 3.2 Parameters

````python
class sklearn.preprocessing.PolynomialFeatures(degree=2, *, interaction_only=False, 
                                               include_bias=True, order=â€˜Câ€™)
````

- degree: ë‹¤í•­ì‹ì˜ ì°¨ìˆ˜ ê²°ì •
- interaction_only: Trueì¼ ê²½ìš° êµì°¨í•­ë§Œ ì¶”ê°€, Falseì¸ ê²½ìš° êµì°¨í•­ê³¼ í•¨ê»˜ ë™ì¼í•œ ë³€ìˆ˜ì˜ 2ì œê³± ì´ìƒ í•­ë„ ì¶”ê°€
- include_bias: Trueì¼ ê²½ìš° bias ì»¬ëŸ¼ ì¶”ê°€

### 3.3 Method

- fit_transform(X): ë°ì´í„°ë¥¼ ì í•© í›„ ë³€í™˜
- transform(X): ë°ì´í„°ë¥¼ ë³€í™˜

### 3.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
cereal = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/cereal.csv")
cereal.info()
````

````
Data columns (total 16 columns):
 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   name      77 non-null     object 
 1   mfr       77 non-null     object 
 2   type      77 non-null     object 
 3   calories  77 non-null     int64  
 4   protein   77 non-null     int64  
 5   fat       77 non-null     int64  
 6   sodium    77 non-null     int64  
 7   fiber     77 non-null     float64
 8   carbo     77 non-null     float64
 9   sugars    77 non-null     int64  
 10  potass    77 non-null     int64  
 11  vitamins  77 non-null     int64  
 12  shelf     77 non-null     int64  
 13  weight    77 non-null     float64
 14  cups      77 non-null     float64
 15  rating    77 non-null     float64
dtypes: float64(5), int64(8), object(3)
````

ğŸ“ **ì „ì²˜ë¦¬ ìˆ˜í–‰**

````python
# object ì œì™¸ ë° sugarsê°€ 0 ì´ìƒì¸ ë°ì´í„°ë§Œ ì¶”ì¶œ
cereal = cereal[cereal.columns[3:]]
cereal =cereal[cereal.sugars >=0]
cereal.head()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr4.png?raw=true" width="650" height="200"></p>

ğŸ“ **ì„ í˜•ê´€ê³„ í™•ì¸**

````python
# sugarsì™€ rating ê°„ì˜ ì„ í˜•ê´€ê³„ í™•ì¸
import matplotlib.pyplot as plt
cereal2 = cereal[['sugars', 'rating']]
cereal2.sort_values(by=['sugars'], inplace =True)
cereal2.reset_index(drop=True, inplace =True)
x=cereal2['sugars'].values
y=cereal2['rating'].values
plt.scatter(x,y)
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr5.png?raw=true" width="650" height="350"></p>

- ì•„ë˜ë¡œ ë³¼ë¡í•œ í˜•íƒœì˜ ê·¸ë˜í”„ê°€ ê·¸ë ¤ì§

ğŸ“ **Train/Test split**

````python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size =0.3, 
random_state =1)
print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)
````

````
(53,) (23,)
(53,) (23,)
````

ğŸ“ **PolynomialFeatures**

````python
from sklearn.preprocessing import PolynomialFeatures
# ì•„ë˜ë¡œ ë³¼ë¡í•œ ê·¸ë˜í”„ê°€ ê·¸ë ¤ì¡Œê¸°ì— ì°¨ìˆ˜ëŠ” 2ì°¨ë¡œ ì„¤ì •
poly_reg=PolynomialFeatures(degree=2)
X_poly=poly_reg.fit_transform(X_train.reshape(-1,1))
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(X_poly, y_train)
````

ğŸ“ **í‰ê°€**

````python
import numpy as np
X_test_poly=poly_reg.transform(X_test.reshape(-1,1))
pred=reg.predict(X_test_poly)
np.set_printoptions(precision=2) # ì†Œìˆ˜ì  ë‘˜ì§¸ìë¦¬ê¹Œì§€ í‘œí˜„
print(np.concatenate((pred.reshape(len(pred),1),y_test.reshape(len(y_test),1)),1)) 
````

````
[[51.63 46.66]
 [32.1  28.74]
 [55.79 59.64]
 [31.08 37.84]
 [32.1  31.44]
 [44.46 44.33]
 [38.82 40.4 ]
 [41.45 55.33]
 [41.45 49.12]
 [31.38 27.75]
 [36.56 34.38]
 [34.7  29.92]
 [65.25 63.01]
 [33.21 31.07]
 [44.46 52.08]
 [38.82 40.45]
 [51.63 53.13]
 [36.56 33.98]
 [41.45 49.51]
 [31.04 22.74]
 [31.38 39.26]
 [31.5  31.23]
 [32.1  21.87]]
````

````python
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error

mse = mean_squared_error(y_test, pred)
mae = mean_absolute_error(y_test, pred)
rmse = np.sqrt(mse)
acc = reg.score(poly_reg.transform(X_test.reshape(-1,1)), y_test)
print('MSE\t{}'.format(round(mse,3)))
print('MAE\t{}'.format(round(mae,3)))
print('RMSE\t{}'.format(round(rmse,3)))
print('ACC\t{}%'.format(round(acc *100,3)))
````

````
MSE	33.565
MAE	4.606
RMSE	5.794
ACC	74.376%
````

ğŸ“ **ì‹œê°í™”**

````python
X_new=np.linspace(0,15,100).reshape(100,1)
X_new_poly=poly_reg.transform(X_new)
y_new = reg.predict(X_new_poly)
plt.plot(x,y, 'o', label ='Actual')
plt.plot(X_new, y_new, 'r-', label ="Prediction")
plt.legend(loc='upper right')
plt.xlabel("$Sugars_1$")
plt.ylabel("$Rating$")
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr6.png?raw=true" width="650" height="350"></p>

## 4. Multiple Regression

### 4.1 Concept

> ë‹¤ì¤‘ ë…ë¦½ë³€ìˆ˜ê°€ ìˆëŠ” íšŒê·€ë¶„ì„ì„ ë§í•˜ë©°, ì—¬ëŸ¬ê°œì˜ ë…ë¦½ë³€ìˆ˜ê°€ ë³µí•©ì ìœ¼ë¡œ ì¢…ì†ë³€ìˆ˜ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²½ìš° ë‹¤ì¤‘ íšŒê·€ ëª¨í˜•ìœ¼ë¡œ ë°ì´í„°ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŒ

### 4.2 Regularization

- ì—¬ëŸ¬ê°œì˜ ë…ë¦½ ë³€ìˆ˜ê°€ ìˆì„ ê²½ìš° ë¶€ë¶„ ì§‘í•©ì„ ì„ íƒí•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒ
- ì´ëŠ” ê° ë…ë¦½ë³€ìˆ˜ì˜ íšŒê·€ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ìˆ˜ë ´ or 0ìœ¼ë¡œ ë§Œë“¤ì–´ ì œê±°í•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•¨

## 5. Ridge Regression

### 5.1 Concept

> Loss function(MSE)ì— L2 paneltyë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ë…ë¦½ë³€ìˆ˜ì˜ ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” íš¨ê³¼ê°€ ìˆìŒ (í•™ìŠµì‹œì— ì ìš©)

### 5.2 Parameteres

````python
class sklearn.linear_model.Ridge(alpha=1.0, *, fit_intercept=True, normalize=â€˜deprecatedâ€™, 
                                 copy_X=True, max_iter=None, tol=0.001, solver=â€˜autoâ€™, 
                                 positive=False, random_state=None) 
````

- alpha: ì •ê·œí™” ê°•ë„, ê°•ë„ê°€ í´ìˆ˜ë¡ ìœ ì—°ì„±ì´ ì¤„ì–´ë“¤ì–´ biasëŠ” ì¦ê°€í•˜ì§€ë§Œ varianceëŠ” ê°ì†Œ
- normalize: ë°ì´í„°ë¥¼ ëª¨ë¸ í•™ìŠµ ì „ì— ì •ê·œí™” í•¨, Falseì¼ ê²½ìš° ë”°ë¡œ scaler ì ìš©
- positive: ê³„ìˆ˜ë¥¼ ì–‘ìˆ˜ë¡œí•¨

### 5.3 Attribute

- coef_: ê³„ìˆ˜
- inetercept_: ì ˆí¸

### 5.4 Method

- fit: ëª¨ë¸ í•™ìŠµ
- get_params: ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì ¸ì˜´
- predict: ì˜ˆì¸¡
- score: ê²°ì •ê³„ìˆ˜ ë°˜í™˜

### 5.5 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
from sklearn.datasets import load_diabetes
import pandas as pd
diabetes = load_diabetes()
x = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
y = diabetes.target
x.head()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr7.png?raw=true" width="800" height="200"></p>

ğŸ“ **Ridge Regression**

````python
from sklearn.linear_model import Ridge
## ë‹¤ì–‘í•œ alpha ê°’ ì„¤ì •
alpha = np.logspace(-3, 1, 5)
data=[]
for i, a in enumerate(alpha):
 ridge=Ridge(alpha=a, random_state=45)
 ridge.fit(x, y)
 data.append(pd.Series(np.hstack([ridge.coef_])))
df_ridge=pd.DataFrame(data, index=alpha)
df_ridge.columns=x.columns
df_ridge
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr8.png?raw=true" width="850" height="200"></p>

````python
plt.semilogx(df_ridge)
plt.xticks(alpha, labels=np.log10(alpha))
plt.legend(labels=df_ridge.columns, bbox_to_anchor=(1, 1))
plt.title("Ridge")
plt.xlabel('alpha')
plt.ylabel('Coefficient (size)')
plt.axhline(y=0, linestyle='--', color='black', linewidth=3)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr9.png?raw=true" width="650" height="350"></p>

- alpha ê°’ì´ ì¦ê°€í•˜ë©´ íšŒê·€ê³„ìˆ˜ì˜ ê°’ì´ 0ì— ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ

````python
from sklearn.linear_model import LinearRegression
lr=LinearRegression()
lr.fit(x,y)
plt.axhline(y=0, linestyle='--', color='black', linewidth=2)
plt.plot(df_ridge.loc[0.001], '^-', label='Ridge alpa = 0.001')
plt.plot(df_ridge.loc[0.010], 's', label='Ridge alpa = 0.010')
plt.plot(df_ridge.loc[0.100], 'v', label='Ridge alpa = 0.100')
plt.plot(df_ridge.loc[1.000], '*', label='Ridge alpa = 1.000')
plt.plot(df_ridge.loc[10.000], 'o-', label='Ridge alpa = 10.000')
plt.plot(lr.coef_,label="LinearRegression")
plt.xlabel('Feature Names')
plt.ylabel('Coefficient (size)')
plt.legend(bbox_to_anchor=(1,1))
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr10.png?raw=true" width="650" height="350"></p>

- LinearRegressionì„ ì‚¬ìš©í•˜ì—¬ ì–»ì€ ê³„ìˆ˜ë“¤ì€ MSEë¥¼ í†µí•´ ì–»ì€ ê²ƒ
- ê° featureì— ëŒ€í•´ alphaê°’ì´ ì‘ì€ ëª¨ë¸ì˜ íšŒê·€ ê³„ìˆ˜ëŠ” MSEì™€ ë¹„ìŠ·í•œ ê³„ìˆ˜ ê°’ì„ ì–»ìŒ
- alpha ê°’ì´ ì¦ê°€í•˜ë©´ì„œ íšŒê·€ê³„ìˆ˜ê°€ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

## 6. Lasso

### 6.1 Concept

> Loss function(MSE)ì— L1 paneltyë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒìœ¼ë¡œ ë…ë¦½ë³€ìˆ˜ì˜ ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¦ (í•™ìŠµì‹œì— ì ìš©)

### 6.2 Parameters

````python
class sklearn.linear_model.Lasso(alpha=1.0, *, fit_intercept=True, normalize=â€˜deprecatedâ€™, 
                                 precompute=False, copy_X=True, max_iter=1000, tol=0.0001, 
                                 warm_start=False, positive=False, random_state=None, 
                                 selection=â€˜cyclicâ€™)
````

- alpha: alphaê°€ 0ì´ë©´ ê¸°ë³¸ MSEì™€ ë™ì¼
- ë‚˜ë¨¸ì§€ ridgeì™€ ë™ì¼

### 6.3 Attribute & Method

- ridgeì™€ ë™ì¼

### 6.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
from sklearn.linear_model import Lasso
alpha=np.logspace(-3, 1, 5)
data=[]
for i, a in enumerate(alpha):
 lasso=Lasso(alpha=a, random_state=45)
 lasso.fit(x, y)
 data.append(pd.Series(np.hstack([lasso.coef_])))
df_lasso = pd.DataFrame(data, index=alpha)
df_lasso.columns=x.columns
df_lasso
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr11.png?raw=true" width="900" height="200"></p>

ğŸ“ **Lasso Regression**

````python
plt.semilogx(df_lasso)
plt.xticks(alpha, labels=np.log10(alpha))
plt.legend(labels=df_lasso.columns, bbox_to_anchor=(1, 1))
plt.title("Lasso")
plt.xlabel('alpha')
plt.ylabel('Coefficient (size)')
plt.axhline(y=0, linestyle='--', color='black', linewidth=3)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr12.png?raw=true" width="650" height="350"></p>

````python
plt.axhline(y=0, linestyle='--', color='black', linewidth=2)
plt.plot(df_lasso.loc[0.001], '^', label='Lasso alpa = 0.001')
plt.plot(df_lasso.loc[0.010], 's', label='Lasso alpa = 0.010')
plt.plot(df_lasso.loc[0.100], 'v', label='Lasso alpa = 0.100')
plt.plot(df_lasso.loc[1.000], '*', label='Lasso alpa = 1.000')
plt.plot(df_lasso.loc[10.000], 'o-', label='Lasso alpa = 10.000')
plt.plot(lr.coef_,label="LinearRegression")
plt.xlabel('Feature Names')
plt.ylabel('Coefficient (size)')
plt.legend(bbox_to_anchor=(1,1))
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr13.png?raw=true" width="650" height="350"></p>

- Ridgeì™€ ë‹¤ë¥´ê²Œ íšŒê·€ê³„ìˆ˜ê°€ 0ì´ ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ

## 7. Elastic Net

### 7.1 Concept

> Ridgeì™€ Lassoë¥¼ í˜¼í•© ë¹„ìœ¨ì— ë”°ë¼ ì ˆì¶©í•œ ê²ƒ

### 7.2 Parameters

````python
class sklearn.linear_model.ElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, 
                                      normalize=â€˜deprecatedâ€™, precompute=False, 
                                      max_iter=1000, copy_X=True, tol=0.0001, 
                                      warm_start=False, positive=False, random_state=None, 
                                      selection=â€˜cyclicâ€™) 
````

- l1_ratio: í˜¼í•© ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ë©°, 0ì¼ê²½ìš° L2, 1ì¼ê²½ìš° L1ì„ ì‚¬ìš©

### 7.3 Attribute & Method

- ìœ„ì™€ ë™ì¼

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
from sklearn.linear_model import ElasticNet
alpha = np.logspace(-3, 1, 5)
data=[]
for i, a in enumerate(alpha):
 ela=ElasticNet(alpha=a, l1_ratio=0.5, random_state=45)
 ela.fit(x, y)
 data.append(pd.Series(np.hstack([ela.coef_])))
df_ela=pd.DataFrame(data, index=alpha)
df_ela.columns=x.columns
df_ela
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr1.png?raw=true" width="650" height="200"></p>

````python
plt.semilogx(df_ela)
plt.xticks(alpha, labels=np.log10(alpha))
plt.legend(labels=df_ela.columns, bbox_to_anchor=(1, 1))
plt.title("Elastic")
plt.xlabel('alpha')
plt.ylabel('Coefficient (size)')
plt.axhline(y=0, linestyle='--', color='black', linewidth=3)
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr14.png?raw=true" width="800" height="200"></p>

````python
plt.axhline(y=0, linestyle='--', color='black', linewidth=2)
plt.plot(df_ela.loc[0.001], '^-', label='Elastic alpa = 0.001')
plt.plot(df_ela.loc[0.010], 's', label='Elastic alpa = 0.010')
plt.plot(df_ela.loc[0.100], 'v', label='Elastic alpa = 0.100')
plt.plot(df_ela.loc[1.000], '*', label='Elastic alpa = 1.000')
plt.plot(df_ela.loc[10.000], 'o-', label='Elastic alpa = 10.000')
plt.plot(lr.coef_,label="LinearRegression")
plt.xlabel('Feature Names')
plt.ylabel('Coefficient (size)')
plt.legend(bbox_to_anchor=(1,1))
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/lr/lr15.png?raw=true" width="650" height="350"></p>



