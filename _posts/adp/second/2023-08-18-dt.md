---
layout: single
title:  'ADP ì‹¤ê¸° 9ì¥ Decision Tree'
toc: true
categories: [ADP]
tags: [ADP ì‹¤ê¸°]

---

ë³¸ ê²Œì‹œë¬¼ì€ Decision Treeì— ëŒ€í•´ ì†Œê°œí•œë‹¤.
{: .notice}

## 1. Decision Tree

> ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ê³  ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ íŠ¹ì • ê¸°ì¤€ì— ë”°ë¼ 'Yes/No'ë¡œ ëŒ€ë‹µí•  ìˆ˜ ìˆëŠ” ë¶„ê¸°ë¥¼ ë‘ë„ë¡ í•™ìŠµ

- ì„ í˜•ì„±, ì •ê·œì„± ë“± ê°€ì •ì´ í•„ìš”í•˜ì§€ ì•Šì•„ ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ëª¨ë¸ ì„±ëŠ¥ ì˜í–¥ì´ í¬ì§€ ì•ŠìŒ
- ì—°ì†í˜•, ë²”ì£¼í˜• ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥
- ë¶„ë¥˜ ê·œì¹™ ëª…í™•í•˜ì—¬ í•´ì„ì´ ì‰¬ì›€

## 2. Decision Tree Classifier

### 2.1 Parameters

````python
class sklearn.tree.DecisionTreeClassifier(*, criterion=â€˜giniâ€™, splitter=â€˜bestâ€™, 
max_depth=None, min_samples_split=2, min_samples_leaf=1, 
min_weight_fraction_leaf=0.0, max_features=None, random_state=None, 
max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, 
ccp_alpha=0.0)
````

- criterion: ë…¸ë“œ ë¶„í• ì‹œ ì‚¬ìš© í•¨ìˆ˜
- max_depth: íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´
- min_samples_split: ë…¸ë“œ ë¶„í• ì‹œ í•„ìš”í•œ ìµœì†Œ ë°ì´í„° ìˆ˜
- min_samples_leaf: ë¦¬í”„ë…¸ë“œì— ìˆì–´ì•¼í•  ìµœì†Œ ë°ì´í„° ìˆ˜
- max_features: ë…¸ë“œ ë¶„í• ì‹œ ì‚¬ìš©í•  featureì˜ ìˆ˜, None or autoì¼ ê²½ìš° ì›ë³¸ ë°ì´í„° ê°œìˆ˜ë¥¼ ì‚¬ìš©
- max_leaf_node: ë¦¬í”„ë…¸ë“œì˜ ìµœëŒ€ ë°ì´í„° ìˆ˜
- max_impurity_decrease: ë…¸ë“œê°€ ë¶„í• ë˜ëŠ” ì¡°ê±´ìœ¼ë¡œ í•´ë‹¹ ê°’ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì€ ìˆ˜ì¤€ìœ¼ë¡œ ë³µì¡ë„ê°€ ê°ì†Œí•  ì‹œ ë¶„í• 
- ccp_alpha: pruningì— ì‚¬ìš©ë˜ëŠ” parameterë¡œ ccp-alphaë³´ë‹¤ ì‘ì€ ë¹„ìš© - ë³µì¡ì„±ì„ ê°€ì§„ ì„œë¸ŒíŠ¸ë¦¬ ì¤‘ ê°€ì¥ ë¹„ìš© - ë³µì¡ì„±ì´ í° íŠ¸ë¦¬ë¥¼ ì„ íƒí•¨, 0ì¼ê²½ìš° pruneí•˜ì§€ ì•ŠìŒ

### 2.2 Attributes

- feature_importances_: ë³€ìˆ˜ ì¤‘ìš”ë„ë¥¼ ë°˜í™”

### 2.3 Method

- cost_complexity_pruning_path
  - X : ìƒ˜í”Œë°ì´í„°. 2ì°¨ì› array í˜•íƒœë¡œ ì…ë ¥
  - y : íƒ€ê¹ƒë°ì´í„°. (n_samples,) ë˜ëŠ” (n_samples, n_targets) í˜•íƒœë¡œ ì…ë ¥
  - ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ccp_pathê°€ ë°˜í™˜. ccp_alphasëŠ” ê°€ì§€ì¹˜ê¸° ë™ì•ˆì˜ ì„œë¸ŒíŠ¸ë¦¬ì— ëŒ€í•œ effective alphaê°’ì´ë©°, impuritiesëŠ” ccp_alphaê°’ì— ìƒì‘í•˜ëŠ” ì„œë¸ŒíŠ¸ë¦¬ ë¦¬í”„ë“¤ì˜ ë¶ˆìˆœë„ í•©

### 2.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
credit = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/credit_final.csv")
credit
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/dt/dt1.png?raw=true" width="900" height="270"></p>

````python
# feature & target ìƒì„±
feature_columns = list(credit.columns.difference(['credit.rating']))
X = credit[feature_columns]
y = credit['credit.rating']

# train-test split
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, stratify =y, 
test_size =0.3, random_state =1)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)
````

````
(700, 20)
(300, 20)
(700,)
(300,)
````

````python
# modeling
from sklearn.tree import DecisionTreeClassifier
clf=DecisionTreeClassifier(max_depth =5)
clf.fit(x_train, y_train)
````

````python
# í‰ê°€
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
pred=clf.predict(x_test)
test_cm=confusion_matrix(y_test, pred)
test_acc=accuracy_score(y_test, pred)
test_prc=precision_score(y_test, pred)
test_rcll=recall_score(y_test, pred)
test_f1=f1_score(y_test, pred)
print(test_cm)
print('\n')
print('ì •í™•ë„\t{}%'.format(round(test_acc *100,2)))
print('ì •ë°€ë„\t{}%'.format(round(test_prc *100,2)))
print('ì¬í˜„ìœ¨\t{}%'.format(round(test_rcll *100,2)))
````

````
[[ 28  62]
 [ 27 183]]

ì •í™•ë„	70.33%
ì •ë°€ë„	74.69%
ì¬í˜„ìœ¨	87.14%
````

````python
# í‰ê°€í‘œ
from sklearn.metrics import classification_report
report = classification_report(y_test, pred)
print(report)
````

````
              precision    recall  f1-score   support

           0       0.51      0.31      0.39        90
           1       0.75      0.87      0.80       210

    accuracy                           0.70       300
   macro avg       0.63      0.59      0.60       300
weighted avg       0.68      0.70      0.68       300
````

````python
# roc
from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_test, clf.predict_proba(x_test)[:, 1])

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/dt/dt2.png?raw=true" width="650" height="400"></p>

````python
# auc
from sklearn.metrics import roc_auc_score
roc_auc = roc_auc_score(y_test, clf.predict_proba(x_test)[:, 1])
print("ROC_AUC_score : ", roc_auc)
````

````
ROC_AUC_score :  0.717857142857143
````

````python
# feature importance
importances = clf.feature_importances_
column_nm = pd.DataFrame(X.columns)
feature_importances = pd.concat([column_nm, pd.DataFrame(importances)], axis=1)
feature_importances.columns = ['feature_nm', 'importances']
print(feature_importances)
````

````
                        feature_nm  importances
0                  account.balance     0.263282
1                              age     0.112494
2                   apartment.type     0.021665
3                     bank.credits     0.000000
4                    credit.amount     0.095584
5           credit.duration.months     0.187908
6                   credit.purpose     0.059083
7                   current.assets     0.000000
8                       dependents     0.000000
9              employment.duration     0.000000
10                  foreign.worker     0.000000
11                       guarantor     0.011790
12                installment.rate     0.000000
13                  marital.status     0.016325
14                      occupation     0.000000
15                   other.credits     0.034003
16  previous.credit.payment.status     0.123825
17              residence.duration     0.020960
18                         savings     0.053080
19                       telephone     0.000000
````

```python
# feature importance ì‹œê°í™”
import os
os.environ["PATH"] += os.pathsep + '/path/to/graphviz/bin'

import numpy as np
feature_names = feature_columns
target_names = np.array(['0', '1'])
import pydot
import pydotplus
import graphviz
from sklearn.tree import export_graphviz
dt_dot_data = export_graphviz(clf, feature_names = feature_names,
 class_names = target_names,
 filled=True, rounded =True,
 special_characters=True)
dt_graph=pydotplus.graph_from_dot_data(dt_dot_data)
from IPython.display import Image
Image(dt_graph.create_png())
```

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/dt/dt3.png?raw=true" width="1000" height="400"></p>

## 3. Decision Tree Regressior

### 3.1 Parameters

````python
class sklearn.tree.DecisionTreeRegressor(*, criterion=â€˜squared_errorâ€™, splitter=â€˜bestâ€™, 
max_depth=None, min_samples_split=2, min_samples_leaf=1, 
min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, 
min_impurity_decrease=0.0, ccp_alpha=0.0) 
````

- classifierì™€ ë™ì¼

### 3.2 Attribute

- classifierì™€ ë™ì¼

### 3.3 Methods

- classifierì™€ ë™ì¼

### 3.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt

np.random.seed(0)
X = np.sort(5 * np.random.rand(400, 1), axis =0)
T = np.linspace(0, 5, 500)[:, np.newaxis]
y = np.sin(X).ravel()
#ë…¸ì´ì¦ˆ ì¶”ê°€í•˜ê¸°
y[::1] +=1 * (0.5 - np.random.rand(400))
plt.scatter(X, y, s=20, edgecolor ="black", c ="darkorange", label="data")
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/dt/dt4.png?raw=true" width="650" height="400"></p>

````python
# train-test split
from sklearn.model_selection import train_test_split
train_x, test_x, train_y, test_y = train_test_split(X,y,train_size =0.7, 
random_state =1)
print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)
````

````
(280, 1) (120, 1) (280,) (120,)
````

````python
# modeling
regr_1 = DecisionTreeRegressor(max_depth=2)
regr_2 = DecisionTreeRegressor(max_depth=5)
````

````python
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error
import pandas as pd
import numpy as np
y_1 = regr_1.fit(train_x, train_y).predict(test_x)
y_2 = regr_2.fit(train_x, train_y).predict(test_x)
preds = [y_1, y_2]
weights = ["max depth = 2", "max depth = 5"]
evls = ['mse', 'rmse', 'mae']
results=pd.DataFrame(index =weights,columns =evls)
for pred, nm in zip(preds, weights):
 mse = mean_squared_error(test_y, pred)
 mae = mean_absolute_error(test_y, pred)
 rmse = np.sqrt(mse)
 
 results.loc[nm]['mse']=round(mse,2)
 results.loc[nm]['rmse']=round(rmse,2)
 results.loc[nm]['mae']=round(mae,2)
results
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/dt/dt5.png?raw=true" width="400" height="100"></p>

````python
X_test = np.sort(5 * np.random.rand(40, 1), axis=0)
regrs=[regr_1, regr_2]
depths=["max depth = 2", "max depth = 5"]
model_color=["m", "c"]
fig, axes = plt.subplots(nrows =1, ncols =2, sharey =True, figsize =(13, 5))
for ix, regr in enumerate(regrs):
 pred = regr.fit(X,y).predict(X_test)
 r2 = regr.score(X_test, pred)
 mae=mean_absolute_error(X_test, pred)
 axes[ix].plot(X_test,
                pred,
                color=model_color[ix],
                label="{}".format(depths[ix])
                )
 axes[ix].scatter(X, y, 
                s=20, 
                edgecolor="gray", 
                c="darkorange", 
                label="data"
                )
 axes[ix].legend(loc="upper right",
                ncol=1,
                fancybox=True,
                shadow=True
                )
 axes[ix].set_title("R2 : {r} , MAE : {m}".format(r =round(r2,3), m=round(mae, 3)))
fig.text(0.5, 0.04, "data", ha ="center", va ="center")
fig.text(0.06, 0.5, "target", ha ="center", va ="center", rotation ="vertical")
fig.suptitle("Decision Tree Regression", fontsize =14)
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/dt/dt6.png?raw=true" width="900" height="500"></p>