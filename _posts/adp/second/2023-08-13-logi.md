---
layout: single
title:  'ADP ì‹¤ê¸° 6ì¥ Logistic Regression'
toc: true
categories: [ADP]
tags: [ADP ì‹¤ê¸°]

---

ë³¸ ê²Œì‹œë¬¼ì€ Logistic Regression ì˜ˆì œì— ëŒ€í•´ ì†Œê°œí•œë‹¤.
{: .notice}

## 1. Simple Logistic Regression

### 1.1 Concept

> ìƒ˜í”Œì´ íŠ¹ì • í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ì¶”ì •í•˜ëŠ” ë¶„ë¥˜ë¬¸ì œì™€ ê°™ì´ targetì´ ë²”ì£¼í˜•ì¼ ê²½ìš° ì ìš©í•˜ëŠ” íšŒê·€ë¶„ì„ì„ Logistic Regerssionì´ë¼í•¨

- Odds: ì‹¤íŒ¨ì— ë¹„í•´ ì„±ê³µí•  í™•ë¥ , ì„±ê³µ í™•ë¥ ì´ Pì¼ ê²½ìš° p/(p-1)ë¡œ êµ¬í•  ìˆ˜ ìˆìŒ
- Logit Transform: log(odds)ë¥¼ ë‚˜íƒ€ë‚´ë©°, Yì¶•ì´ í™•ë¥ ì¸ **Logistic Regression**ì„ logit ë³€í™˜ì„ í†µí•´ ì‹¤ìˆ˜ë¡œ ì¹˜í™˜í•¨
- Maximum Likelihood: Logistic Regressionì˜ best fitted lineì„ ì°¾ê¸° ìœ„í•´ ì‚¬ìš©, ì¦‰ ë¹„ìš©í•¨ìˆ˜ ê°œë…

### 1.2 Parameters

````python
class sklearn.linear_model.LogisticRegression(penalty=â€˜l2â€™, *, dual=False, tol=0.0001, 
C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, 
solver=â€˜lbfgsâ€™, max_iter=100, multi_class=â€˜autoâ€™, 
verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)
````

- class_weight: í´ë˜ìŠ¤ì™€ ê´€ë ¨ëœ ê°€ì¤‘ì¹˜, ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë‘ 1
- solver: ìµœì í™” ë¬¸ì œë¥¼ í‘¸ëŠ” í•´ë¥¼ êµ¬í•  ë•Œ ì‚¬ìš©í•  ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒ, ë°ì´í„° ì…‹ì˜ í¬ê¸°ê°€ ì‘ìœ¼ë©´ liblinearê°€ ì¢‹ë‹¤ê³  ì•Œë ¤ì§, ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì¼ ê²½ìš° newton-cg, sag, saga, lbfgs

### 1.3 Attributes

- classes_: ë¼ë²¨ë§ëœ í´ë˜ìŠ¤
- coef_: featureì— í• ë‹¹ëœ ê°€ì¤‘ì¹˜

### 1.4 Methods

- decision_function: ìƒ˜í”Œë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡í•œ ê°’ì´ Hyperplaneìœ¼ë¡œ ë¶€í„° ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ (logisticì—ì„œ hyper planeì€ 0)
- predict(X): ì˜ˆì¸¡ê°’ì„ arrayë¡œ ë°˜í™˜
- predict_log_proba(X): í´ë˜ìŠ¤ì— ëŒ€í•œ ìƒ˜í”Œì˜ ë¡œê·¸ í™•ë¥ 
- predict_proba(X): í´ë˜ìŠ¤ì— ëŒ€í•œ ìƒ˜í”Œì˜ í™•ë¥ 

### 1.4 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
body=pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/bodyPerformance.csv")
````

ğŸ“**ì„±ë³„ ë¶„ë¥˜ê¸° ë§Œë“¤ê¸°**

````python
# ì„±ë³„ì„ 0/1 ìœ¼ë¡œ ë³€í™˜
body['gender']=np.where(body['gender']=='M', 0, 1)
body['class_1']=np.where(body['class']=='A', 1, 0)
body
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/logi/logi1.png?raw=true" width="900" height="300"></p>

````python
# ë°ì´í„° ì…‹ ë¶„ë¥˜
from sklearn.model_selection import train_test_split
feature_columns = list(body.columns.difference(['class', 'class_1']))
x=body[feature_columns]
y=body['class_1']
train_x, test_x, train_y, test_y=train_test_split(x,y,stratify=y, 
train_size=0.7, random_state=1)
print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)
````

````
(9375, 11) (4018, 11) (9375,) (4018,)
````

````python
# ëª¨ë¸ í•™ìŠµ
from sklearn.linear_model import LogisticRegression
logR=LogisticRegression(random_state=45)
logR.fit(train_x, train_y)
````

````python
# ê²°ê³¼ ì‹œê°í™”
proba=pd.DataFrame(logR.predict_proba(train_x))
cs=logR.decision_function(train_x)
df=pd.concat([proba, pd.DataFrame(cs)], axis=1)
df.columns=['Not A','A', 'decision_function']
df.sort_values(['decision_function'], inplace=True)
df.reset_index(inplace=True, drop=True)
df
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/logi/logi2.png?raw=true" width="350" height="300"></p>

````python
import matplotlib.pyplot as plt

plt.figure(figsize=(15,5))
plt.axhline(y=0.5, linestyle='--', color='black', linewidth=1)
plt.axvline(x=0, linestyle='--', color='black', linewidth=1)
plt.plot(df['decision_function'], df['Not A'], 'g--', label='Not A')
plt.plot(df['decision_function'], df['Not A'], 'g^')
plt.plot(df['decision_function'], df['A'], 'b--', label='A')
plt.plot(df['decision_function'], df['A'], 'b*')
plt.xlabel
plt.ylabel
plt.legend(loc='upper left')
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/logi/logi3.png?raw=true" width="650" height="350"></p>

````python
# í‰ê°€
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
pred=logR.predict(test_x)
test_cm=confusion_matrix(test_y, pred)
test_acc=accuracy_score(test_y, pred)
test_prc=precision_score(test_y, pred)
test_rcll=recall_score(test_y, pred)
test_f1=f1_score(test_y, pred)
print(test_cm)
print('\n')
print('ì •í™•ë„\t{}%'.format(round(test_acc*100,2)))
print('ì •ë°€ë„\t{}%'.format(round(test_prc*100,2)))
print('ì¬í˜„ìœ¨\t{}%'.format(round(test_rcll*100,2)))
print('F1\t{}%'.format(round(test_f1*100,2)))
````

````
[[2763  251]
 [ 342  662]]

ì •í™•ë„	85.24%
ì •ë°€ë„	72.51%
ì¬í˜„ìœ¨	65.94%
F1	69.07%
````

````python
# ROC curve ì‹œê°í™”
from sklearn.metrics import RocCurveDisplay
RocCurveDisplay.from_estimator(logR, test_x, test_y)
plt.show()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/logi/logi4.png?raw=true" width="650" height="350"></p>

## 2. Multi Class Softmax Regression

### 2.1 Concept

> ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ 2ê°œ ì´ìƒ í´ë˜ìŠ¤ì¸ ë‹¤ì¤‘ í´ë˜ìŠ¤ì— ëŒ€í•´ ì¼ë°˜í™” í•œ ê²ƒ

- softmax: K ì°¨ì›ì˜ ë²¡í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ê° ì°¨ì›(í´ë˜ìŠ¤)ì— ì†í•  í™•ë¥ ì„ ê³„ì‚°í•¨
- logisticì˜ multi_classë¥¼ multinomailë¡œ, solverì—ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©

### 2.2 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings(â€˜ignoreâ€™)
body = pd.read_csv(â€œhttps://raw.githubusercontent.com/ADPclass/ADP_
book_ver01/main/data/bodyPerformance.csvâ€)
````

ğŸ“ **Class ë¶„ë¥˜ê¸°**

````python
# gender ë³€ìˆ˜ ì „ì²˜ë¦¬
body['gender']=np.where(body['gender']=='M', 0, 1)
# class ë³€ìˆ˜ ì „ì²˜ë¦¬
mapping={'A':0, 'B':1, 'C':2, 'D':4}
body['class_2']=body['class'].map(mapping)
body
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/logi/logi5.png?raw=true" width="900" height="300"></p>

````python
# ë°ì´í„° ë¶„ë¦¬
from sklearn.model_selection import train_test_split
feature_columns = list(body.columns.difference(['class', 'class_2']))
x=body[feature_columns]
y=body['class_2']
train_x, test_x, train_y, test_y = train_test_split(x,y,stratify=y, 
train_size=0.7, random_state=1)
print(train_x.shape, test_x.shape, train_y.shape, test_y.shape)
````

````
(9375, 11) (4018, 11) (9375,) (4018,)
````

````python
# ëª¨ë¸ í•™ìŠµ
from sklearn.linear_model import LogisticRegression
softm=LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10, 
random_state=45)
softm.fit(train_x, train_y)
````

````python
# í‰ê°€
from sklearn.metrics import confusion_matrix, accuracy_score
pred=softm.predict(test_x)
test_cm=confusion_matrix(test_y, pred)
test_acc=accuracy_score(test_y, pred)
print(test_cm)
print('\n')
print('ì •í™•ë„\t{}%'.format(round(test_acc*100,2)))
````

````
[[707 261  36   0]
 [269 403 300  32]
 [ 92 207 525 181]
 [ 13  63 157 772]]

ì •í™•ë„	59.91%
````

````python
print("ìƒ˜í”Œì˜ class ì˜ˆì¸¡")
print(softm.predict([test_x.iloc[-1,:]]))
print("ê° classì— ì†í•  í™•ë¥ ")
print(softm.predict_proba([test_x.iloc[-1,:]]))
````

````
ìƒ˜í”Œì˜ class ì˜ˆì¸¡
[0]

ê° classì— ì†í•  í™•ë¥ 
[[0.62639722 0.311902   0.06015632 0.00154446]]
````









