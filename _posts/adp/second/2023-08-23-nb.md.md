---
layout: single
title:  'ADP ì‹¤ê¸° 11ì¥ Naive Bayes'
toc: true
categories: [ADP]
tags: [ADP ì‹¤ê¸°]

---

ë³¸ ê²Œì‹œë¬¼ì€ Naive Bayesì— ëŒ€í•´ ì†Œê°œí•œë‹¤.
{: .notice}

## 1. Bayes` Theorem

### 1.1 Concept

> ë‘ í™•ë¥  ë³€ìˆ˜ì˜ ì‚¬ì „ í™•ë¥ ê³¼ ì‚¬í›„ í™•ë¥  ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ë¦¬

### 1.2 Formular

- Posterior: ì‚¬ê±´ Bê°€ ë°œìƒí–ˆì„ ë•Œ, ì‚¬ê±´ Aê°€ ë°œìƒí•  í™•ë¥ ë¡œ ë³´í†µ ë¬¸ì œì—ì„œ ì•Œê³ ìí•˜ëŠ” ê²ƒ
  - $P(A|B)$
- Likelihood: ì‚¬ê±´ Aê°€ ë°œìƒí–ˆì„ ë•Œ, ì‚¬ê±´ Bê°€ ë°œìƒí™œ í™•ë¥ 
  - $P(B|A)$
- Prior: ì‚¬ê±´ Aê°€ ë°œìƒí•  í™•ë¥ 
  - $P(A)$
- Evidence: ê´€ì°°ê°’, ì‚¬ê±´ Bê°€ ë°œìƒí•  í™•ë¥ 
  - $P(B)$

## 2. Naive Bayes

### 2.1 Concept

> í•˜ë‚˜ì˜ ì†ì„±ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ë¥¸ ì†ì„±ì´ ë…ë¦½ì ì´ë¼ ì „ì œí–ˆì„ ë•Œ í•´ë‹¹ ì†ì„±ê°’ì´ í´ë˜ìŠ¤ ë¶„ë¥˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¸¡ì •

### 2.2 Condition

- ë°ì´í„°ê°€ ë§ì§€ ì•Šì„ ë•Œ: ë°ì´í„°ê°€ ì ìœ¼ë©´ í†µê³„ì  ê¸°ë²•ì˜ ì‹ ë¢°ë„ê°€ ë–¨ì–´ì§
- ëª©ì ì´ ë¯¸ë˜ ì˜ˆì¸¡: í•˜ë‚˜ì˜ ì¶”ì •ì¹˜ë¥¼ ê³ ì§‘í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê°’ì„ ìˆ˜ì •í•˜ë©° í˜„ì‹¤ì ì¸ ì¶”ì •ì¹˜ë¥¼ ì°¾ì•„ê°

### 2.3 Kind

- BernoulliNB: ì´ì§„ ë¶„ë¥˜ (ì´ì‚°í˜•)
- MultinomialNB: ì¹´ìš´íŠ¸ ë°ì´í„° (ì´ì‚°í˜•)
- GaussianNB: ë°ì´í„°ê°€ ì—°ì†ì ì´ë©° ì •ê·œë¶„í¬ë¼ëŠ” ê°€ì • í•˜ì— ì ìš© (ì—°ì†í˜•)

## 3. GaussianNB

### 3.1 Parameters

````python
class sklearn.naive_bayes.GaussianNB(*, priors=None, var_smoothing=1e-09) 
````

- priors: í´ë˜ìŠ¤ì˜ ì‚¬ì „ í™•ë¥ 

### 3.2 Methods

- fit(X, y)
- predict(X)
- predict_proba(X)
- score(X, y)

### 3.3 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````
import pandas as pd
sky = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/Skyserver.csv")
sky.info()
````

````
Data columns (total 18 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   objid      10000 non-null  float64
 1   ra         10000 non-null  float64
 2   dec        10000 non-null  float64
 3   u          10000 non-null  float64
 4   g          10000 non-null  float64
 5   r          10000 non-null  float64
 6   i          10000 non-null  float64
 7   z          10000 non-null  float64
 8   run        10000 non-null  int64  
 9   rerun      10000 non-null  int64  
 10  camcol     10000 non-null  int64  
 11  field      10000 non-null  int64  
 12  specobjid  10000 non-null  float64
 13  class      10000 non-null  object 
 14  redshift   10000 non-null  float64
 15  plate      10000 non-null  int64  
 16  mjd        10000 non-null  int64  
 17  fiberid    10000 non-null  int64  
dtypes: float64(10), int64(7), object(1)
````

````python
sky['class'].unique()
````

````
array(['STAR', 'GALAXY', 'QSO'], dtype=object)
````

````python
import seaborn as sns
sns.pairplot(hue='class', data =sky[['z', 'run', 'i', 'class']])
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/nb/nb2.png?raw=true" width="900" height="500"></p>

````python
import numpy as np
features = list(sky.columns)
features.remove('class')
X = sky[features]
y = sky['class']
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size =0.3, 
random_state =1, stratify =y)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)
````

````
(7000, 17) (7000,)
(3000, 17) (3000,)
````

````python
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
pred = gnb.fit(x_train, y_train).predict(x_test)
print("Accuracy Score : ", gnb.score(x_test, y_test))
````

````
Accuracy Score :  0.799
````

````python
gnb.predict_proba(x_test)[[0, 13, 68]]
````

````
array([[8.26737014e-01, 4.43137039e-02, 1.28949282e-01],
       [5.39851854e-05, 9.64092748e-02, 9.03536740e-01],
       [8.32868012e-01, 4.48282737e-02, 1.22303715e-01]])
````

````python
gnb.predict(x_test)[[0, 13, 68]]
````

````
array(['GALAXY', 'STAR', 'GALAXY'], dtype='<U6')
````

````python
from sklearn.metrics import classification_report
pred=gnb.predict(x_test)
print(classification_report(y_test, pred))
````

````
              precision    recall  f1-score   support

      GALAXY       0.74      0.97      0.84      1499
         QSO       0.00      0.00      0.00       255
        STAR       0.91      0.75      0.83      1246

    accuracy                           0.80      3000
   macro avg       0.55      0.58      0.56      3000
weighted avg       0.75      0.80      0.76      3000
````

## 4. BernoulliNB

### 4.1 Parameters

````python
class sklearn.naive_bayes.BernoulliNB(*, alpha=1.0, fit_prior=True, class_prior=None)
````

- fit_prior: í´ë˜ìŠ¤ì˜ ì‚¬ì „í™•ë¥  í•™ìŠµ ì—¬ë¶€, Falseë©´ ê· ë“±í™•ë¥ 
- class_prior: í´ë˜ìŠ¤ì˜ ì‚¬ì „ í™•ë¥ 

### 4.2 Methods

- Gaussianê³¼ ë™ì¼

### 4.3 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
import pandas as pd
spam = pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/spam.csv", encoding ='utf-8')
````

````python
spam.isna().sum()
````

````
v1               0
v2               0
Unnamed: 2    5522
Unnamed: 3    5560
Unnamed: 4    5566
````

````python
spam=spam[['v1', 'v2']]
spam
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/nb/nb3.png?raw=true" width="350" height="300"></p>

````python
spam['v1'].unique()
````

````
array(['ham', 'spam'], dtype=object)
````

````python
import numpy as np
spam['label'] = np.where(spam['v1']=='spam', 1, 0)
spam
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/nb/nb4.png?raw=true" width="350" height="300"></p>

````python
X= spam['v2']
y = spam['label']
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size =0.3, 
random_state =1, stratify =y)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)
````

````
(3900,) (3900,)
(1672,) (1672,)
````

````python
from sklearn.feature_extraction.text import CountVectorizer 
cv = CountVectorizer(binary =True)
x_traincv = cv.fit_transform(x_train)
x_traincv.shape
````

````
(3900, 7175)
````

- ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œë² ì´ì¦ˆëŠ” ì´ì‚°í˜•ìœ¼ë¡œ(1,0) êµ¬ì„±ëœ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê¸°ì—, CountVectorizerë¥¼ ì‚¬ìš©
- binaryë¥¼ Trueë¡œ ì„¤ì •ì‹œ ì´ë©”ì¼ë§ˆë‹¤ ë‹¨ì–´ê°€ í•œë²ˆ ì´ìƒ ë“±ì¥í•˜ë©´ 1 ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0ìœ¼ë¡œ ì„¤ì •

````python
encoded_input = x_traincv.toarray()
encoded_input
````

````
array([[0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       ...,
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0],
       [0, 0, 0, ..., 0, 0, 0]])
````

````python
print(cv.inverse_transform(encoded_input[[0]]))
````

````
[array(['couple', 'down', 'give', 'me', 'minutes', 'my', 'sure', 'to',
       'track', 'wallet', 'yeah'], dtype='<U34')]
````

- inverse_transform: ë²¡í„°ë¡œ ì¸ì½”ë”©ëœ ì´ë©”ì¼ ì œëª©ì— ì–´ë–¤ ë‹¨ì–´ë“¤ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•¨

````python
print(cv.get_feature_names_out()[1000 :1010 ], end ='')
````

````
['at' 'ate' 'athletic' 'athome' 'atlanta' 'atlast' 'atm' 'attach'
 'attached' 'attack']
````

- get_feature_names_out: ë²¡í„°ì˜ ì¸ë±ìŠ¤ê°€ ì–´ë–¤ ë‹¨ì–´ë¥¼ ëœ»í•˜ëŠ”ì§€ í™•ì¸

````python
from sklearn.naive_bayes import BernoulliNB
bnb = BernoulliNB()
bnb.fit(x_traincv, y_train)
````

````python
x_testcv = cv.transform(x_test)
pred = bnb.predict(x_testcv)
from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test, pred)
print("Accuracy Score : ", acc)
````

````
Accuracy Score :  0.9754784688995215
````

````python
from sklearn.metrics import classification_report
print(classification_report(y_test, pred))
````

```
              precision    recall  f1-score   support

           0       0.97      1.00      0.99      1448
           1       0.99      0.82      0.90       224

    accuracy                           0.98      1672
   macro avg       0.98      0.91      0.94      1672
weighted avg       0.98      0.98      0.97      1672
```

## 5. MultinomialNB

### 5.1 Parameters

````Python
class sklearn.naive_bayes.MultinomialNB(*, alpha=1.0, fit_prior=True, class_prior=None)
````

- ìœ„ì™€ ë™ì¼

### 5.2 Methods

- ìœ„ì™€ ë™ì¼

### 5.3 Implementation

ğŸ˜— **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**

````python
from keras.datasets import imdb
(X_train, y_train), (X_test, y_test) = imdb.load_data()
print(X_train.shape)
print(X_test.shape)
````

````
(25000,)
(25000,)
````

````python
import pandas as pd

word_to_index = imdb.get_word_index()
index_to_word = {}
for key, value in word_to_index.items():
 index_to_word[value+3] = key

for index, token in enumerate(("<pad>", "<sos>", "<unk>")):
 index_to_word[index] = token

train_reviews = []
for X in X_train:
 tmp =' '.join([index_to_word[index] for index in X])
 train_reviews.append(tmp)

test_reviews = []
for X in X_test:
 tmp =' '.join([index_to_word[index] for index in X])
 test_reviews.append(tmp)

train = pd.concat([pd.DataFrame(train_reviews), pd.DataFrame(y_train)], axis =1)
train.columns = ['reviews', 'label']
train['reviews'] = train['reviews'].str[6:]
test = pd.concat([pd.DataFrame(test_reviews), pd.DataFrame(y_test)], axis=1)
test.columns = ['reviews', 'label']
train['reviews'] = train['reviews'].str[6:]
````

````python
print("<<<<<<<<< Train Dataset for MNB >>>>>>>>>")
train.head()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/nb/nb5.png?raw=true" width="350" height="200"></p>

````python
print("<<<<<<<<< Test Dataset for MNB >>>>>>>>>")
test.head()
````

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/adp/nb/nb6.png?raw=true" width="350" height="200"></p>

````python
x_train, x_test = train['reviews'].values, test['reviews'].values
y_train, y_test = train['label'].values, test['label'].values
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)
````

````
(25000,) (25000,)
(25000,) (25000,)
````

````python
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer(binary=False)
x_traincv = cv.fit_transform(x_train)
x_traincv.shape
````

````
(25000, 76521)
````

````python
print(cv.inverse_transform(x_traincv)[0 ])
````

````
['ilm' 'was' 'just' 'brilliant' 'casting' 'location' 'scenery' 'story'
 'direction' 'everyone' 'really' 'suited' 'the' 'part' 'they' 'played'
 'and' 'you' 'could' 'imagine' 'being' 'there' 'robert' 'redford' 'is'
 'an' 'amazing' 'actor' 'now' 'same' 'director' 'norman' 'father' 'came'
 'from' 'scottish' 'island' 'as' 'myself' 'so' 'loved' 'fact' 'real'
 'connection' 'with' 'this' 'film' 'witty' 'remarks' 'throughout' 'were'
 'great' 'it' 'much' 'that' 'bought' 'soon' 'released' 'for' 'retail'
 'would' 'recommend' 'to' 'watch' 'fly' 'fishing' 'cried' 'at' 'end' 'sad'
 'know' 'what' 'say' 'if' 'cry' 'must' 'have' 'been' 'good' 'definitely'
 'also' 'congratulations' 'two' 'little' 'boy' 'of' 'paul' 'children'
 'are' 'often' 'left' 'out' 'praising' 'list' 'think' 'because' 'stars'
 'play' 'them' 'all' 'grown' 'up' 'such' 'big' 'profile' 'whole' 'but'
 'these' 'should' 'be' 'praised' 'done' 'don' 'lovely' 'true' 'someone'
 'life' 'after' 'shared' 'us']
````

````python
print (cv.get_feature_names_out()[-10 :])
````

````
['Ã©tait' 'Ã©tat' 'Ã©tc' 'Ã©very' 'Ãªxtase' 'Ã­s' 'Ã­snt' 'Ã¸stbye' 'Ã¼ber'
 'Ã¼vegtigris']
````

````python
from sklearn.naive_bayes import MultinomialNB
mnb = MultinomialNB()
mnb.fit(x_traincv, y_train)
````

````python
from sklearn.metrics import accuracy_score, classification_report
x_testcv = cv.transform(x_test)
pred = mnb.predict(x_testcv)
acc = accuracy_score(y_test, pred)
print("Accuracy Score : ", acc)
````

````
Accuracy Score :  0.81932
````

````python
from sklearn.metrics import classification_report
print(classification_report(y_test, pred))
````

````
              precision    recall  f1-score   support

           0       0.79      0.87      0.83     12500
           1       0.85      0.77      0.81     12500

    accuracy                           0.82     25000
   macro avg       0.82      0.82      0.82     25000
weighted avg       0.82      0.82      0.82     25000
````

