---
layout: single
title:  'Planning by Dynamic Programming'
toc: true
categories: [Reinforcement Learning]
tags: [Dynamic Programming, Policy Evaluation]
---

ë³¸ ê²Œì‹œë¬¼ì€ David Silverì˜ [ê°•ì˜](https://www.youtube.com/watch?v=lfHX2hHRMVQ&list=PLhhVkSH_JBI8ofvmbrG7m86wmVXq_7dit&index=2)ì™€ íŒ¡ìš”ë© Pang-Yo Labì˜ [ìœ íŠœë¸Œ ê°•ì˜ 3ê°•](https://www.youtube.com/watch?v=rrTxOkbHj-M&list=PLpRS2w0xWHTcTZyyX8LMmtbcMXpd3s4TU&index=3) ì„ ë³´ê³  ì •ë¦¬í•˜ëŠ” ê¸€ì´ë‹¤.<br>ê°•ì˜ë…¸íŠ¸ëŠ” [ì´ê³³](https://www.davidsilver.uk/teaching/)ì—ì„œ ì°¸ê³ í•˜ì˜€ë‹¤. ğŸ˜—
{: .notice}

## 1. Introduction

### 1.1 What is Dynamic Programming

ğŸ‘€ **Definition**

````
Dynamic Programmingì€ í° ë¬¸ì œë¥¼ ì‘ì€ ë¬¸ì œë¡œ ë‚˜ëˆ„ì–´ í‘¸ëŠ” ë°©ë²•ë¡ ì´ë‹¤.
````

- ë³µì¡í•œ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ì‹
- ì‘ì€ ë¬¸ì œë¡œ ë¶„í• í•˜ëŠ” ë°©ì‹

### 1.2 Requirements for Dynamic Programming

Dynamic Programming(=DP)ì€ ì¼ë°˜ì ìœ¼ë¡œ ë‘ê°€ì§€ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë¬¸ì œì— ì ìš©ë  ìˆ˜ ìˆë‹¤.

1. Optimal substructure: optimal solutionì´ ì‘ì€ ë¬¸ì œë“¤ë¡œ ë‚˜ë‰  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.
2. Overlapping subproblems: subproblemì— ëŒ€í•œ solutionì„ ì €ì¥í•´ ë†“ì•˜ë‹¤ê°€ ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

â˜ MDPê°€ ìœ„ ì¡°ê±´ ë‘ê°œë¥¼ í¬í•¨í•˜ê¸°ì— DPë¡œ ì ‘ê·¼í•˜ê¸° ì í•©í•˜ë‹¤.

### 1.3 Planning by Dynamic Programming

DPëŠ” MDPì— ëŒ€í•œ ëª¨ë“  êµ¬ì„±ìš”ì†Œë¥¼ ì•Œê³  ìˆë‹¤ê³  ê°€ì •í•œë‹¤. <br>â˜ state, action, transition probability, reward, discount factor

ğŸ“**Step of Dynamic Programming**

DPëŠ” predictionê³¼ control ë‘ê°€ì§€ stepìœ¼ë¡œ ë‚˜ë‰˜ëŠ”ë°, í˜„ì¬ ì§„í–‰í•˜ëŠ” policyì— ë”°ë¼ valueë¥¼ êµ¬í•˜ê³ (=prediction) ì´ë¥¼ í† ëŒ€ë¡œ policyë¥¼ optimalí•˜ê²Œ ë°œì „(=control)ì‹œí‚¤ëŠ” íë¦„ìœ¼ë¡œ ì§„í–‰ëœë‹¤.

1. Predcition<br>- input: MDP and policy/ MRP<br>- solution: Bellman expectaion Eqn<br>- output: value function<br>- policy evaluation

2. Control<br>- input: MDP<br>- solution: Bellman optimality Eqn<br>- output: optimal value function, optimal policy<br>- policy improvement

## 2. Policy Evaluation

### 2.1 Iterative Policy Evaluation

- Problem: policyë¥¼ í‰ê°€í•˜ëŠ” ê²ƒ<br> = policyë¥¼ ë”°ëì„ ë•Œ returnì„ êµ¬í•¨<br>= value function
- Solution: ë°˜ë³µì ì¸ Bellman expectation Eqn











