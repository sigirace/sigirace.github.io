---
layout: single
title:  'Markov Decision Process(MDP)'
toc: true
categories: [Reinforcement Learning]
tags: [Markov Process, Markov Reward Process]
---

ë³¸ ê²Œì‹œë¬¼ì€ David Silverì˜ [ê°•ì˜](https://www.youtube.com/watch?v=lfHX2hHRMVQ&list=PLhhVkSH_JBI8ofvmbrG7m86wmVXq_7dit&index=2)ì™€ íŒ¡ìš”ë© Pang-Yo Labì˜ [ìœ íŠœë¸Œ ê°•ì˜ 2ê°•](https://www.youtube.com/watch?v=wYgyiCEkwC8&list=PLpRS2w0xWHTcTZyyX8LMmtbcMXpd3s4TU) ì„ ë³´ê³  ì •ë¦¬í•˜ëŠ” ê¸€ì´ë‹¤.<br>ê°•ì˜ë…¸íŠ¸ëŠ” [ì´ê³³](https://www.davidsilver.uk/teaching/)ì—ì„œ ì°¸ê³ í•˜ì˜€ë‹¤. ğŸ˜—
{: .notice}

## 1. Markov Proceses

### 1.1 Introduction to MDPs

- Markov decision process(markov processê°€ ì•„ë‹˜!)ëŠ” reinforcement learning(=RL)ì—ì„œì˜ í™˜ê²½ì„ ì„¤ëª…í•˜ëŠ” ê²ƒì´ë‹¤.
- MDPëŠ” environmentê°€ ëª¨ë‘ ë‹¤ ê´€ì¸¡ ê°€ëŠ¥í•œ ìƒí™©ì´ë‹¤.<br>ex) í˜„ì¬ stateê°€ processë¥¼ ì™„ì „íˆ í‘œí˜„í•˜ëŠ” ê²ƒ = markov property
- ê±°ì˜ ëª¨ë“  ê°•í™”í•™ìŠµ MDP í˜•íƒœë¡œ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

### 1.2 Markov Property

ğŸ‘€ **Definition**

```
ë¯¸ë˜ëŠ” í˜„ì¬ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê³¼ê±°ì— ë…ë¦½ì ì´ë‹¤.
```

$$
P[S_{t+1}|S_{t}] = P[S_{t+1}|S_1, ..., S_{t}]
$$

- State(=í˜„ì¬)ê°€ hitory(=ê³¼ê±°)ì— ìˆëŠ” ê´€ë ¨ëœ ëª¨ë“  ì •ë³´ë¥¼ ê°–ê³  ìˆê¸° ë•Œë¬¸ì—, í˜„ì¬ë¥¼ ì•Œë©´ ê³¼ê±°ëŠ” ëª¨ë‘ ë²„ë ¤ë„ ë¬´ë°©í•˜ë‹¤.

### 1.3 State Transition Matrix

- Markov state sì—ì„œ ë‹¤ìŒ state s`ë¡œ ì „ì´ë  í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.

$$
P_{ss^{`}} = [S_{t+1} = s^{`}|S_{t} = s]
$$

- ì–´ë– í•œ state sì—ì„œ ë‹¤ìŒ state s`ë¡œ ì „ì´ë  í™•ë¥ ì„ ëª¨ì•„ matrixë¡œ í‘œí˜„í•œ ê²ƒì´ state transition matrix ì´ë‹¤.
- ì´ë•Œ matrixì˜ ëª¨ë“  í•©ì€ 1ì´ë‹¤.

### 1.4 Markov Process

- Markov processëŠ” memoryless random processì´ë‹¤.
- memorylessëŠ” ë‚´ê°€ ì–´ë–¤ ê²½ë¡œë¥¼ í†µí•´ì„œ ì™”ë˜, í˜„ì¬ stateì— ì˜¨ ìˆœê°„ ë‹¤ìŒ ê²½ë¡œëŠ” matrixì˜ í™•ë¥ ì„ ë”°ë¥¸ë‹¤ëŠ” ê²ƒì´ë‹¤. 
- random processëŠ” ìƒ˜í”Œë§ì„ í•  ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ë¡œ, í•˜ë‚˜ì˜ stateë¡œ ì´ë™í•˜ê³  ëì´ ì•„ë‹ˆë¼ sequenceë¥¼ ìƒ˜í”Œë§ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.<br>(random processì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì¶”í›„ ì—…ë°ì´íŠ¸...)

ğŸ‘€ **Definition**

````
Markov Process (or Markov Chain)ì€ <S, P>ë¡œ ì´ë£¨ì–´ì§„ íŠœí”Œì´ë‹¤.
- S: ìœ í•œ state ì§‘í•©
- P: state transition probability matrix
````

ğŸ“ **Example**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/reinforcement/lec2/RL-2-2.png?raw=true" width="600" height="350"></p>

> S1 = C1ì—ì„œ ì‹œì‘í•˜ëŠ” Student Markov Chainì„ ìƒ˜í”Œë§ í•œê²ƒì´ê³ , Transition MatrixëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/reinforcement/lec2/RL-2-3.png?raw=true" width="400" height="250"></p>

<br><br>

## 2. Markov Reward Process

### 2.1 Markov Reward Process

ğŸ‘€ **Definition**

````
Markov Reward ProcessëŠ” <S, P, R, r>ë¡œ ì´ë£¨ì–´ì§„ íŠœí”Œì´ë‹¤.
- S: ìœ í•œ state ì§‘í•©
- P: state transition probability matrix
- R: reward function
- r: discount factor
````

$$
R_{s} = E[R_{t+1} |R_{t} ]
$$

$$
r \in [0, 1]
$$

ğŸ“ **Example**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/reinforcement/lec2/RL-2-4.png?raw=true" width="600" height="500"></p>

### 2.2 Retrun

ğŸ‘€ **Definition**

````
Return GtëŠ” ê° time-step tì—ì„œ ì–»ì€ discounted rewardì˜ í•©ì´ë‹¤.
````

$$
G_t = R_{t+1} + rR_{t+2} + ... = \sum\limits_{k=0}^{\infty} r^{k}R_{t+k+1}
$$

