---
layout: single
title:  'Introduction to Reinforcement Learning'
toc: true
categories: Reinforcement Learning
tags: [RL]

---

ë³¸ ê²Œì‹œë¬¼ì€ David Silverì˜ [ê°•ì˜](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLhhVkSH_JBI8ofvmbrG7m86wmVXq_7dit)ì™€ íŒ¡ìš”ë© Pang-Yo Labì˜ ìœ íŠœë¸Œ [ê°•ì˜](https://www.youtube.com/watch?v=wYgyiCEkwC8&list=PLpRS2w0xWHTcTZyyX8LMmtbcMXpd3s4TU) ë¥¼ ë³´ê³  ì •ë¦¬í•˜ëŠ” ê¸€ì´ë‹¤.<br>ê°•ì˜ë…¸íŠ¸ëŠ” [ì´ê³³](https://www.davidsilver.uk/wp-content/uploads/2020/03/intro_RL.pdf)ì—ì„œ ì°¸ê³ í•˜ì˜€ë‹¤.
{: .notice}

<div class="notice">
<li><a href="https://sigirace.github.io/autoencoder/autoencoder_2/">ê°•ì˜ ëª©ë¡ ì¶”í›„ ì—…ë¡œë“œ</a></li>
</div>

## 1. About Reinforcement Learning

### 1.1 Branches of Machine Learning

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/reinforcement/lec1/intro_RL-07.png?raw=true" width="650" height="400"></p>

### 1.2 Charateristics of Reinforcement Learning<br>

â˜€ï¸ **ê°•í™”í•™ìŠµê³¼ ë‹¤ë¥¸ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì **

- supervisorê°€ ì—†ê³  reward signal ë§Œì´ ìˆë‹¤.<br>â˜ ì •ë‹µì„ ì•Œë ¤ì£¼ëŠ” supervisorê°€ ì—†ì´ reward ì‹ í˜¸ë§Œì„ í†µí•´ ì •ë‹µì„ ì°¾ì•„ ë‚˜ê°„ë‹¤.
- í”¼ë“œë°±(reward)ì´ ì¦‰ê°ì ì´ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.
- ì‹œê°„(=ìˆœì„œ)ì´ ì¤‘ìš”í•˜ë‹¤.<br>â˜ ì§€ë„í•™ìŠµì—ì„œëŠ” i.i.d(independent and identical distribution) dataë¥¼ ê°€ì •í•œë‹¤.<br>â˜ ì¦‰, ê° ìƒ˜í”Œì´ ë…ë¦½ì ìœ¼ë¡œ ë½‘í˜€ ì„œë¡œê°„ì˜ ì˜í–¥ì´ ì—†ê³  ë™ì¼í•œ í™•ë¥ ë¶„í¬ì—ì„œ ë‚˜ì˜¨ ê²ƒ<br>â˜ ê°•í™”í•™ìŠµì€ iidë¥¼ ê°€ì •í•˜ì§€ ì•Šê³ , ìˆœì„œê°€ ìˆëŠ”(=ë…ë¦½ì ì´ì§€ ì•Šì€) ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤.
- Agentì˜ actionì´ ì´í›„ì— ë°›ê²Œë˜ëŠ” ë°ì´í„°ì— ì˜í–¥ì„ ì¤€ë‹¤<br>â˜ ì§€ë„í•™ìŠµê³¼ ë¹„ì§€ë„í•™ìŠµì€ ì •í•´ì§„ ë°ì´í„° ì…‹ ì•ˆì—ì„œ ì´ë£¨ì–´ì§„ë‹¤ë©´ ê°•í™”í•™ìŠµì€ ë§¤ë²ˆ ë°ì´í„°ê°€ ë‹¬ë¼ì§„ë‹¤.

## 2. The RL Problem

í•´ë‹¹ chapterì—ì„œëŠ” ê°•í™”í•™ìŠµ(RL)ì˜ ìš©ì–´ë¥¼ ì •ë¦¬í•œë‹¤.

### 2.1 Rewards

- Reward $R_t$ëŠ” scalar feedback signalì´ë‹¤.<br>â˜ të²ˆì§¸ time(=step)ì— scalarë¡œ ì£¼ì–´ì§€ëŠ” ê²ƒìœ¼ë¡œ í•´ë‹¹ stepì—ì„œ agentì˜ í‰ê°€ ì²™ë„ê°€ ëœë‹¤.
- Agentì˜ ëª©ì ì€ cumulative rewardë¥¼ maximiseí•˜ëŠ” ê²ƒì´ë‹¤.<br>â˜ ê°•í™”í•™ìŠµì€ <mark style='background-color: #f6f8fa'>reward hypothesis</mark>ë¥¼ ê·¼ê±°ë¡œ í•œë‹¤.

ğŸ‘€ **Definition of Reward Hypothesis**

```
ëª¨ë“  ëª©ì ì€ rewardì˜ ì´ í•©ì„ ê·¹ëŒ€í™” í•˜ëŠ” ê²ƒìœ¼ë¡œ ì„¤ëª… ë  ìˆ˜ ìˆë‹¤.
```

### 2.2 Sequential Decision Making

í•œë²ˆì˜ rewardë¥¼ ì˜ ë°›ëŠ”ê²ƒì´ ì•„ë‹Œ ìˆœì°¨ì ì¸ ì˜ì‚¬ê²°ì •ì„ í†µí•´ rewardì˜ ì´ í•©ì´ ê·¹ëŒ€í™” ë˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.

- Goal: ë¯¸ë˜ì— ë°›ì„ rewardì˜ ì´ í•©ì„ ê·¹ëŒ€í™”í•˜ëŠ” actionì„ ì„ íƒí•˜ëŠ” ê²ƒ
- Actionì˜ ê²°ê³¼(reward)ê°€ ë‚˜ì¤‘ì— í™•ì¸ë  ìˆ˜ ìˆë‹¤.
- ë‹¹ì¥ì€ ì†í•´ë¥¼ ë³´ë”ë¼ë„ ì¶”í›„ì— ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

ğŸ“ **ì˜ˆì‹œ**

> ì²´ìŠ¤ì˜ ê²½ìš° ë‹¹ì¥ í°ì„ í•˜ë‚˜ ë¨¹íˆëŠ” actionì„ í•˜ë”ë¼ë„ í›„ì— í€¸ì„ ë¨¹ì„ ìˆ˜ ìˆë‹¤ë©´ ì¢‹ì€ ì„ íƒìœ¼ë¡œ ê²°ì •í•˜ì—¬ì•¼ í•œë‹¤.

### 2.3 Agent and Environment

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/reinforcement/lec1/intro_RL-17.png?raw=true" width="400" height="400"></p>

- **Agent** : ì£¼ì¸ê³µ, í•™ìŠµí•˜ëŠ” ëŒ€ìƒìœ¼ë¡œ, í™˜ê²½ì†ì—ì„œ í–‰ë™í•˜ëŠ” ê°œì²´ë¥¼ ë§í•œë‹¤. (ex. ê°•ì•„ì§€, ë¡œë´‡, í”Œë ˆì´ì–´)
- **Environment** : Agentì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” í™˜ê²½. ê°•í™”í•™ìŠµì€ Agentì™€ Environmentê°„ì˜ ìƒí˜¸ì‘ìš©ê°„ì— ì¼ì–´ë‚˜ëŠ” ê³¼ì •ì´ë‹¤.
- **State** : Agent(ë˜ëŠ” Environment)ê°€ í•„ìš”í•œ êµ¬ì²´ì  ì •ë³´(ex. ìœ„ì¹˜, ì†ë„)ë¥¼ ë§í•œë‹¤. (ë’¤ì—ì„œ ì„¤ëª…)
- **Observation** : Agentê°€ stateì¤‘ ì¼ë¶€ë¥¼ ë°›ëŠ”ë° ì´ ì •ë³´ë¥¼ Observationì´ë¼ í•œë‹¤.<br>â˜ ëŒ€ê°œì˜ ê²½ìš° Agentê°€ state ì „ë¶€ë¥¼ ì•„ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ê°€ëŠ¥í•œ ê²½ìš°ë¥¼ fully observableì´ë¼ê³  í•œë‹¤. 

<center>ì¶œì²˜: https://namu.wiki/w/ê°•í™”í•™ìŠµ/ìš©ì–´</center><br>

â˜€ï¸ **Agentì™€ Environmentì˜ ìƒí˜¸ì‘ìš©**

- Agentê°€ ì–´ë– í•œ actionì„ ìˆ˜í–‰í•œë‹¤. â˜ $A_t$
- Environmentê°€ ë‘ê°€ì§€ ì‹ í˜¸ë¥¼ ì¤€ë‹¤.
  - Actionì— ëŒ€í•œ reward â˜ $R_{t+1}$
  - Actionìœ¼ë¡œ ì¸í•´ ë³€ê²½ëœ ìƒí™©(=observation) â˜ $O_{t+1}$

### 2.4 History and State

- HistoryëŠ” observations, actions, rewardsì˜ ìˆœì°¨ì ì¸ ê¸°ë¡ì´ë‹¤.

$$
H_t = O_1, R_1, A_1, ... , A_{t-1}, O_t, R_t
$$

- **State**ëŠ” Agent(ë˜ëŠ” Environment)ê°€ ë‹¤ìŒì— ì–´ë–¤ ê²°ì •ì„ í•  ì§€ ê²°ì •í•˜ê¸° ìœ„í•œ ì •ë³´ë“¤ì´ë‹¤.<br>â˜ StateëŠ” historyì˜ ì •ë³´ë¥¼ ê°€ê³µí•˜ì—¬ ë§Œë“œëŠ” ê²ƒ, ì¦‰ history í•¨ìˆ˜ì´ë‹¤.

### 2.5 Environment State

- Environmentì˜ state $S^{e}_{t}$ëŠ” envirionmentì˜ privateí•œ í‘œí˜„ì´ë‹¤.<br>â˜ ë³´í†µ agentì—ê²Œ ë³´ì´ì§€ ì•Šìœ¼ë©°, ë³´ì´ë”ë¼ë„ ê´€ê³„ì—†ëŠ” ì •ë³´ë“¤ì´ë‹¤.
- environmentê°€ ë‹¤ìŒ observation/ rewardë¥¼ ì£¼ê¸° ìœ„í•´ ì‚¬ìš©í•œ dataì´ë‹¤.

ğŸ“ **ì˜ˆì‹œ**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/reinforcement/lec1/intro_RL-38.png?raw=true" width="400" height="400"></p>

> ì˜ˆì‹œ environmentëŠ” ê²Œì„ê¸°ì´ê³ , ê·¸ ì•ˆì—ëŠ” ë‹¤ìŒ í™”ë©´(=observation)ê³¼ ì ìˆ˜(=reward)ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ í˜„ì¬ state, ë“¤ì–´ì˜¨ agentì˜ action ë“±ë“±ì´ ìˆì„ ê²ƒì´ë‹¤. ì´ëŠ” environmentê°€ ë‹¤ìŒ obserbationê³¼ rewardë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì •ë³´(=environment state)ì´ì§€ë§Œ ì‚¬ìš©ìì—ê²ŒëŠ” ì“¸ëª¨ê°€ ì—†ëŠ” ë°ì´í„°ì´ë‹¤.

### 2.6 Agent State

- Agent state $S^{a}_{t}$ëŠ” agentì˜ ë‚´ë¶€ì ì¸ í‘œí˜„ì´ë‹¤.
- Agentê°€ ë‹¤ìŒ actionì„ í•˜ê¸° ìœ„í•´ ì“°ì´ëŠ” dataì´ë‹¤.
- ì´ëŸ¬í•œ ì •ë³´ëŠ” ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì‚¬ìš©ëœë‹¤.

### 2.7 Markov State

Markov stateëŠ” historyì—ì„œ ë¶€í„° ì˜¨ ëª¨ë“  ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆë‹¤.

ğŸ‘€ **Definition of Reward Markov**

```
ë§Œì•½ ë¯¸ë˜ì˜ Stateê°€ í˜„ì¬ì—ë§Œ ì˜í–¥ì„ ë°›ìœ¼ë©´, í˜„ì¬ì˜ State StëŠ” Markovì´ë‹¤. (â˜ í•„ìš”ì¶©ë¶„ ì¡°ê±´)
```

$$
P[S_{t+1}|S_{t}] = P[S_{t+1}|S_1, ..., S_{t}]
$$

- ë¯¸ë˜ëŠ” í˜„ì¬ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê³¼ê±°ì— ë…ë¦½ì ì´ë‹¤.
- Environment stateëŠ” Markovì´ë‹¤.

### 2.8 Fully Observable Environments

- Full observability: agentê°€ environment stateë¥¼ í™•ì¸í•  ìˆ˜ ìˆì„ ë•Œ, Environmentì™€ Agentì˜ stateê°€ ê°™ë‹¤.<br>
- Markov Decision Process (MDP)ë¼ê³  í•¨

$$
O_t = S^a_t = S^e_t
$$

### 2.9 Partially Observable Environments

- Partial observility: environmentì™€ agentì˜ stateê°€ ê°™ì§€ ì•Šì€ ê²½ìš°
- AgentëŠ” ìì‹ ì˜ stateë¥¼ í‘œí˜„í•´ì•¼í•œë‹¤.
  - History ìì²´ë¥¼ ì“°ê±°ë‚˜
  - RNNê°™ì€ ìˆœì°¨ì ì¸ ì •ë³´ë¥¼ ì“°ê±°ë‚˜

## 3. Inside An RL Agent

í•´ë‹¹ chapterì—ì„œëŠ” agentì˜ êµ¬ì„±ìš”ì†Œë¥¼ ì‚´í´ë³¸ë‹¤.

### 3.1 Policy

- PolicyëŠ” agentì˜ í–‰ë™ì„ ê·œì •í•˜ëŠ” ê²ƒ
- stateê°€ ì£¼ì–´ì¡Œì„ ë•Œ í–‰ë™ì„ ê·œì •í•¨
- Deterministic policy: í•˜ë‚˜ì˜ actionì„ ë§¤í•‘í•¨

$$
a = \pi(s)
$$

- Stochastic policy: ì—¬ëŸ¬ actionì˜ í™•ë¥ ì„ ë§¤í•‘í•¨

$$
\pi(a|s) = P[A_t = a| S_t = s]
$$

### 3.2 Value Function

- Value Functionì€ ë¯¸ë˜ì˜ rewardë¥¼ ì˜ˆì¸¡í•˜ì—¬ ìƒí™©(=í˜„ì¬ state)ì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€/ë‚˜ìœì§€ë¥¼ ë‚˜íƒ€ëƒ„<br>â˜ ì´ë¥¼ í†µí•´ actionì„ ì„ íƒí•¨
- í˜„ì¬ stateì—ì„œ policyë¥¼ ë”°ë¼ ì§„í–‰í•˜ì˜€ì„ ë•Œ ë°›ê²Œë˜ëŠ” ì´ rewardì˜ ê¸°ëŒ€ê°’<br>â˜ Stochasticì¼ ê²½ìš° í™•ë¥ ì— ë”°ë¼ policyê°€ ë‹¬ë¼ì ¸ rewardê°€ ë‹¬ë¼ì§€ë‹ˆ ê¸°ëŒ€ê°’ì„ ì·¨í•¨<br>â˜ Detrministicì¼ ê²½ìš°ì—ë„ environmentë¡œ ë¶€í„° ì˜¤ëŠ” í™•ë¥ ì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— ê¸°ëŒ€ê°’ì„ ì·¨í•¨
- Discount factorë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜ì˜ rewardì— ë¶ˆí™•ì‹¤ì„±ì„ í¬í•¨ì‹œí‚´

$$
v_{\pi}(s)= E_{\pi}[R_{t+1}+\gamma R_{t+2}+ \gamma^2R_{t+3}+...|S_t=s]
$$

### 3.3 Model

- Modelì€ environmentì˜ ë‹¤ìŒ í–‰ë™ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ
-  state sì—ì„œ action aë¥¼ í–ˆì„ë•Œ, reward ì˜ˆì¸¡

$$
R^{a}_{s} = E[R_{t+1}|S_t=s, A_t=a]
$$

- state sì—ì„œ action aë¥¼ í–ˆì„ë•Œ, ë‹¤ìŒ state s`ê°€ ë‚˜ì˜¬ í™•ë¥  (= probability of state transition)

$$
P^{a}_{ss^\prime} = P[S_{t+1}=s^\prime | S_t = s, A_t = a]
$$

### 3.4 Maze Example

â˜€ï¸ **Definition**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/reinforcement/lec1/intro_RL-30.png?raw=true" width="650" height="400"></p>

- Cumulative rewardê°€ ìµœëŒ€ê°€ ë˜ê¸° ìœ„í•´ ê° time-stepì˜ actionì— ëŒ€í•œ rewardë¥¼ 1ë¡œ ì„¤ì •í•¨

â˜€ï¸ **Policy**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/reinforcement/lec1/intro_RL-31.png?raw=true" width="650" height="400"></p>

- ì—¬ëŸ¬ policyê°€ ìˆê² ì§€ë§Œ, ì˜ˆì‹œëŠ” ê° stateì—ì„œì˜ optimal policyë¥¼ ë‚˜íƒ€ëƒ„

â˜€ï¸ **Value Function**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/reinforcement/lec1/intro_RL-32.png?raw=true" width="650" height="400"></p>

- Policyë¥¼ ë”°ëì„ ë•Œ, ê° stateì—ì„œì˜ value function

â˜€ï¸ **Model**

<p align="center"><img src="https://github.com/sigirace/page-images/blob/main/reinforcement/lec1/intro_RL-33.png?raw=true" width="650" height="400"></p>

- Modelì€ 1. next rewardì™€ 2. next stateë¥¼ ì •í•´ì•¼í•¨
- Next rewardëŠ” ëª¨ë‘ -1
- Next stateëŠ”

1:02:10
